-- MySQL dump 10.13  Distrib 8.3.0, for Win64 (x86_64)
--
-- Host: 127.0.0.1    Database: scientificarticlessearch
-- ------------------------------------------------------
-- Server version	8.3.0

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8mb4 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `articles_article`
--

DROP TABLE IF EXISTS `articles_article`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_article` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `titre` varchar(200) NOT NULL,
  `resume` longtext NOT NULL,
  `text_integral` longtext NOT NULL,
  `url` varchar(100) NOT NULL,
  `date_de_publication` date DEFAULT NULL,
  `is_validated` tinyint(1) NOT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=20 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_article`
--

LOCK TABLES `articles_article` WRITE;
/*!40000 ALTER TABLE `articles_article` DISABLE KEYS */;
INSERT INTO `articles_article` VALUES (1,'ModelGame: A Quality Model for Gamified Software Modeling Learning','Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the engagement of developers while creating UML models or writing code. Empirical studies report that UML models suffer from incompleteness and inconsistency problems. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and UML model quality in the context of software modeling learning. This article, therefore, proposes ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gamified activities and providing guidelines aware of quality issues. A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt the ModelGame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. The results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the engagement of developers while creating UML models or writing code. Empirical studies [7,9,14] report that UML models suffer from incompleteness and inconsistency problems. Lange [14] reinforces that these defects bring potential risks that can cause misinterpretation and communication failure, representing a risk to software quality. Thus, finding formats that favor student learning and consequently in generating increasingly effective UML models can become one of the main challenges faced by instructors that include UML (Unified Modeling Language) as part of software modeling content.\", \"Some studies [3,12,25] sought to understand how to apply gamification in software modeling teaching using some elements such as points, emblems and levels. However, instructors and researchers still find limitations when applying, evaluating, and measuring the use of this tool in the learning of software modeling students and, consequently, in the models developed by them, since in the current literature there is no \\\"frame of reference\\\" that guides them. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and model quality in the context of software modeling learning.\", \"This article, therefore, introduces ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gamified activities and providing guidelines aware of quality issues. A reference framework would help to (1) establish parameters for evaluating UML models created by learners; (2) provide guidelines to improve the quality of these artifacts; (3) to analyze which elements of gamification could be included in each of the phases of modeling using UML; (4) identify intrinsic and extrinsic aspects of students during the modeling stages, to improve the models; (5) to compare validated theories about the inclusion of gamification in software modeling teaching, taking into account the types of learning and methodologies used; and (6) contributing to the identification of gamification use objectives in modeling activities.\", \"A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt it in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. These results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling.\", \"The remainder of the paper is organized as follows. Section 2 presents the main concepts discussed throughout the article. Section 3 discusses the related work, highlighting research opportunities. Section 4 introduces the proposed quality model. Section 5 presents how the quality model was evaluated. Section 6 points out some threats to validity. Finally, Section 7 presents some concluding remarks and future work.\"]}, {\"header\": \"BACKGROUND\", \"paragraph\": [\"This section presents the essential concepts for understanding this work, including gamification and software engineering teaching (Section 2.1), and software modeling and model quality (Section 2.2).\"]}, {\"header\": \"Gamification and Software Engineering Teaching\", \"paragraph\": [\"Gamification aims to use game elements in the context of not game [5], bringing all positive aspects they provide as a way to encourage and engage \\\"players, \\\" thereby broadening their motivations.\", \"Werbach [23] classifies gamification into three dimensions: Dynamics, Mechanics, and Components. Dynamicsinclude all game aspects related to the emotional responses of \\\"players\\\" (e.g., relationship, progression, and narrative).Mechanics offer elements that promote the action of a game -usually elaborated via a rule-based development -, so that the player can interact with such elements, e.g., challenges, feedback, and rewards. Components represent the aesthetic elements of gamification, whose goal is to present visual aspects with which players can perform the interaction, for example, points, scores, and emblems (badges).\", \"Knowing that the teaching of Software Engineering should involve students to experience the professional practices of the area so that they can understand which practices and techniques are useful in several different situations [2]. The challenges of teaching new software engineers are not limited to learning programming, but also include paying attention to detail, considering the quality of created models, established schedule and defined budgets [1]. In addition to understanding the technical challenges, these future professionals must be up to date with nontechnical issues, including teamwork, communication and management.\", \"To meet these new demands of the current context, the format with exhibition classes is no longer considered enough and may even become demotivating and ineffective in learning students. In this sense, gamification has been increasingly used in the teaching of software engineering as a way to promote behavioral and psychological changes [11] providing an environment that favors communication, cooperation, feedback, reward, achievement and other recurring elements that are capable of improving performance, efficiency and engagement in educational activities , and can enhance, for example, the learning of software modeling.\"]}, {\"header\": \"Software Modeling and Model Quality\", \"paragraph\": [\"Software modeling encompasses the set of principles, concepts, and practices that lead to the development of a high-quality system or product. The principles of this activity establish a philosophy that guides the entire software development process.\", \"In this scenario, UML models play a crucial role in software development tasks, for example, documenting project decisions, understanding development details, promoting better communication between teams, and generating greater efficiency in software development [19]. However, these models suffer problems of inconsistency and incompleteness [10,18], as well as end up being overlooked within the modeling process, as pointed out in some empirical studies in the literature [14,15]. Class and sequence diagrams, for example, present inconsistencies when sequence diagram objects are not found in the class diagram, consequently developers end up living with inconsistencies throughout the development process.\", \"A research challenge still open is how to evaluate these diagrams, both in industry and in the teaching process, in terms of quality, such as syntactic and semantic, for example.\"]}, {\"header\": \"RELATED WORK\", \"paragraph\": [\"The selection of related works was carried out following two steps:\", \"(1) search in digital repositories, such as Google Scholar and Scopus (Elsevier) of articles related to gamification, quality modeling, and modeling learning; and (2) filter selected articles considering the alignment of such works with the objective of the work (Section 4). After selecting the works, they were analyzed (Section 3.1) and compared (Section 3.2), seeking to identify research opportunities.\"]}, {\"header\": \"Analysis of Related Works\", \"paragraph\": [\"Porto et al. (2021) [4]. This work performed a systematic mapping with the objective of characterizing how gamification has been adopted in noneducational contexts of software engineering activities. The main results of this study show that gamification provided benefits for activities such as requirements specification, development, testing, project management, and support process. In addition, he pointed out that the number of publications and new research initiatives has increased over the years, many positive results have been achieved in software engineering activities. Nevertheless, the study reinforced that gamification can still be explored for other tasks in this area, as empirical evidence is very limited.\", \"Marin (2021) [17]. It performed the application of gamification on some topics of a software engineering course to engage students and increase their motivation and argued that, with due motivation, students can better exercise the topics and obtain more solid knowledge. There were five games related to risk management, BPMN modeling, Scrum process, design and inspection of class diagrams, and cosmic functional size measurement to assist in the learning process of the software engineering course. This study also presented the lessons learned about the application of gamification and serious games in software engineering, including limitations or disadvantages.\", \"Jurgelaitis et al. (2018) [12]. This work conducted a research to investigate how gamification could be inserted into an Information Systems Modeling course, which covers a range of topics on UML. As a result, an implementation of the gamified system modeling course in the Moodle environment was presented, using additional plugins for the use of the necessary gamified elements. The study showed good results and obtained a positive acceptance by the participating students.\", \"Rodrigues et al. (2018) [22]. They investigated the use of games and game elements in software engineering education, through a research that had the participation of 88 instructors of this discipline. The results showed that most instructors are aware of these educational approaches, however, the games were adopted by only 21 participants and game elements were adopted only by 19. Games are most often used to cover \\\"Software Process\\\" and \\\"Project Management\\\". The most commonly used game elements are points, quizzes, and challenges. The results also show that the main reasons for not adopting the resources are the lack of knowledge, information about games relevant to the engineering of teaching software, and the lack of time to plan and include these approaches in the classroom.\", \"Cosentino et al. (2017) [3]. They present a model-based approach to learning modeling in a gamified way. The approach includes a new language to model the gamification process itself and an environment where it can be incorporated into current modeling tools to allow instructors and students to design and use a complete modeling framework, including gamification elements. In addition, the approach also had as a proposal to provide support to collect and analyze gamification data, thus facilitating monitoring activities.\", \"Yohannis (2016) [25]. This research presents an exploration of game design as an approach to strengthening the student\'s mastery in software modeling by developing their abstraction skills. It brought together concepts of gamification development, such as the lens of atoms of intrinsic skill and principles of pedagogical design of various theories and models of learning. The research follows the Design Science Research Methodology and explores the best practices of Model Oriented Engineering. As a result, a modeling game design framework and generation structure and a series of produced games are presented.\", \"Pedreira et al. (2015) [21]. They developed a systematic mapping of gamification in Software Engineering based on 29 studies. The mapping revealed that software implementation is the area in which most studies focus, followed by software requirements, few others in different areas, such as project planning and software testing, and even to a lesser extent in activities involving software modeling. However, the highlight of this work was to highlight that gamification in software engineering is still at a very early stage and the evidence on its impact in this field remains inconclusive.\"]}, {\"header\": \"Comparative Analysis and Opportunities\", \"paragraph\": [\"Five Comparison Criteria (CC) were defined selecting the most relevant variables to assist in the process of identifying similarities and differences between the proposed work and the selected articles. This comparison is crucial to make the process of identifying research opportunities using objective rather than subjective criteria. The criteria are described below:\", \"\\u2022 Context (CC01): Works that explore the use of gamification in software modeling teaching/learning. \\u2022 Participant profile (CC02): Studies that collected data from participants for screening and profile characterization.\", \"\\u2022 Applicability of Gamification in UML (CC03): Studies that evaluated how gamification can contribute to UML models. \\u2022 Model creation (CC04): Studies that have developed a model to improve factors that imply the non-adoption of UML. \\u2022 Instructor participation (CC05): Studies that collected qualitative data through the participation of software modeling instructors.\", \"Table 1 shows the comparison of the selected works, confronting this work. Some gaps and research opportunities are observed: (1) only the proposed work was the only one to fully meet all comparison criteria; (2) although most of them targeted the application of gamification in software modeling teaching, they were not directed to the use of UML; (3) no study has developed a model to evaluate the learning and improvement of UML models developed by students; and (4) most of them did not have the participation of instructors to identify the difficulties and opportunities in the application of gamification in the teaching of software modeling. Thus, the next Section presents a quality model to explore these identified opportunities.\"]}, {\"header\": \"Related Work\", \"paragraph\": [\"Comparison Criterion CC1 CC2 CC3 CC4 CC5 Proposed Work Porto et al (2021) [4] Marin (2021) [17] Jurgelaitis et al (2018) [12] Rodrigues et al (2018) [22] Cosentino et al (2017) [3] Yohannis (2016) [25] Pedreira et al (2015) [21] Completely Meets Partially Meets Does not attend\"]}, {\"header\": \"Generic Analytical Framework\", \"paragraph\": [\"Figure 1 presents the generic analytical framework for improving the quality of the models and serves as the basis for the creation of an evaluation scheme. The arrows (\\\"links\\\"), labeled as Evaluation and Gamified Modeling, represent the questions that the evidence must answer; dotted lines represent associations; rectangles represent the Models (rounded corners) or the quality states (square corners) by which these bindings are measured. Ellipses represent the adverse effects that can be generated from the evaluation and use of gamification. The numbers refer to the key questions and are connected with the concepts and relationships of the abstract syntax of the Quality Model (presented in Section 4.2), as follows: (1) Are there tools that assist instructors in evaluating the models developed by students, thus reducing the poor quality and incompleteness of these artifacts? (2) What is the prevalence of characteristics that cause models to be at risk? (3) Are there notions of quality to evaluate the models as a way to define parameters when performing their correction? (4) Applying the use of gamification in models that need intervention would be a way to identify factors that could generate models with high quality levels? ( 5 Fact is that it is not enough just to include this \\\"toolbox\\\" in the UML learning process, it is necessary to provide the instructor with a model (guide) that can serve as a reference to evaluate the quality of diagrams elaborated through gamified activities. For example, the instructor could create models predefining inconsistencies by making use of these questions raised to evaluate the models created by the students. The set of questions serves as the starting point for this evaluation. Knowing that the adaptation of the gamification approach requires a significant effort [20], in this study we present The ModelGame as a way to identify factors that contribute to the quality of these artifacts and, consequently, to the students\' learning.\"]}, {\"header\": \"Abstract Syntax\", \"paragraph\": [\"Following the specification pattern of the UML metamodel, Figure 2 presents the abstract syntax of the proposed Quality Model for gamified software modeling learning (ModelGame). It identifies the main concepts and relationships. The numbers represent the notions of quality that are discussed in Section 4.3. The following are detailed each of these concepts and relationships.\", \"Domain. The first concept presented in this study is the domain, which corresponds to a specific context of the application to be developed to solve the problem. In this process, the design template represents the solution given to the domain. Association\", \"\\u2022 contextualizes:\", \"Each contextualise refers to the domain that will serve as the basis for the challenges launched.\", \"Challenges. This concept represents the phase in which the problem is contextualized (domain-based), as well as what will be the missions, phases, scenarios, and other elements presented to the players, in this case the students, who must use the principles of software engineering to perform the modeling and reach the final goal. Association\", \"Each influence represents that the proposed challenge interfered in aspects of the design model, causing the user to seek to make a continuous improvement.\", \"Modeling Language. Software modeling is an important step for development to happen in a way that adheres to the requirements established by the requester, for this, there is the modeling language, which offers a standardized way to document and design software. Through the use of modeling languages, it is possible to achieve a high level of understanding about the software in question, improving the communication between all those involved in the process, thus avoiding implementation errors. It points out that software engineers use these languages to communicate design decisions and verify the feasibility of implementing the intended design. The UML was consolidated as the Modeling Language in the paradigm of object orientation, in which it is possible through visual notation generated from the diagrams-presented later in this study as Design Models-to perform the representation of various perspectives of the system. Association User. This concept corresponds to the individual who performs the interpretation of the developed design models, whose objective is to be able to understand the domain in question. In the gamified context, the user has the role of player and it is he who performs the whole process, being able to perform the interpretation of existing models or even creating new ones. The user can also identify and resolve inconsistencies that arise from compositions between models.\", \"Association \\u2022 creates: Design Model[1.\", \". *] Represents the process in which the user creates a design template, which can be one or more.\", \"\\u2022 interprets: Design Model[1.. *] In this association, the user performs the interpretation of the design template. When interpreting the model, paths for the resolution of inconsistencies can be identified.\"]}, {\"header\": \"\\u2022 detects: Inconsistency [*]\", \"paragraph\": [\"Represents the user\'s discovery of design model inconsistencies, for example, those that are generated from identifying conflicts, whether a class is abstract or not.\", \"\\u2022 resolves: Inconsistency [*] Each resolves equates to the resolution representation of the inconsistencies by the user that happens after he analyzes and determines the best alternative to perform this action.\", \"\\u2022 uses: Modeling Tools [*] Determines that the user can use modeling tools to generate/update design models.\"]}, {\"header\": \"Association\", \"paragraph\": [\"\\u2022 Without a directed relationship.\", \"Modeling Tool. This concept represents the applications that are used to carry out the construction of design models. There are several tools available, online and desktop, and it is up to the user to choose the one that will best meet their needs and adapt to the context in question, that is, they work in any domain that is being considered.\", \"Design Model. The design model refers to a visual notation (diagram) to represent static and dynamic aspects. These models are built according to a specific objective or task and tend to facilitate the logical interpretation of the software in several aspects. The most popular diagrams are Use Cases and Classes, the first being static and representing a set of actions generated from functional requirements (use cases) and presenting the interactions generated with external users (actors). The second is a static diagram and makes the representation of the logical structure of the software involving the classes, their attributes, methods, and relationships between them [19]. Association\", \"\\u2022 describes: Domain [1] Each describes makes the representation of a specific domain and means that every design model must describe it.\", \"Inconsistency. It corresponds to the defects found in the models developed by users. They may occur because of the nonidentification and correction of possible conflicts and even an erroneous interpretation. Association\"]}, {\"header\": \"\\u2022 affects: Design Model[*]\", \"paragraph\": [\"This association indicates that with each occurrence of the affect, a problem is presented harming the quality of the design model.\", \"Points. This concept represents one of the most used game mechanics in software engineering and functions as a quantitative reward for each action developed, in which it is possible to regulate the number of rewarded points of the player, defined here as user, based on the importance of each action. Through this concept, it is possible to stimulate competition, collaboration, and creativity among users, stimulating learning. Points appear as a derivation of the association affects, since when each inconsistency error is identified or not, the user will receive a score and the association describes, because the points will also be applied when making connections between the model and the domain.\", \"Progress. The concept of progress emerges as a factor that makes the user able to perceive its evolution in the process, in this case, software modeling. Progress emerges as a derivation of the association interprets, making the user know when they have performed a correct interpretation of the proposed design model or what still needs to be improved.\", \"Feedback. Feedback has the role of making the user realize that the proposed goal can be achieved and follow its evolution, including analyzing how to change or creating new strategies to achieve the goal. This concept emerges as a derivation between the associations it creates, causing the user to receive a return to the model creation process.\"]}, {\"header\": \"Quality Notions\", \"paragraph\": [\"As discussed in Section 2, gamification can bring important elements for learning software modeling and, therefore, the objective of this section is to produce the notions of quality of the model of this study. The ModelGame is composed of ten counts, four of which are proposed in this study -scope, use, motivational and engagement -extracted from the main benefits that the gamification elements presented in Figure 2 can bring to the models. The others are adaptations of previous works [6,14,15], they are, syntactic, semantic, social, effort, detection and resolution.\", \"Scope Quality (1). It seeks to determine how much the proposed challenge is contextualized with the design model, as well as the definition of the domain, problem, competencies, concepts, behaviors and attitudes that will be developed throughout the process.\", \"Syntactic Quality (2). This notion makes the representation of the process of correction of the design models that are produced by the modeling language, because if it is not used correctly, inconsistencies will arise. It is important to insert this notion of quality into our study, since during the process of developing the models, users may come across the composition of two class diagrams, for example.\", \"Semantic Quality (3). It is necessary to verify that the design model and the problem domain match, so this notion performs this type of analysis. Communication problems may occur between users if the semantic elements of the model are affected.\", \"Social Quality (4). Design models are used to communicate between members of a team to inform all established decisions about software development [8]. If divergent interpretations occur, this communication will be greatly impaired.\", \"Quality of Effort (5). This notion refers to the production challenges of the model that will be generated, including factors such as time and cost. (6). To produce design templates, users can use unusual tools such as paper, whiteboard, and more. However, most of the time they choose to use formal tools (CASES) and can be online or desktop. This notion corresponds to the level of ease and applicability of the models elaborated when making use of these tools, it is also important to contribute to communication between users through collaboration-related functionalities.\"]}, {\"header\": \"Quality of Use\", \"paragraph\": [\"Detection Quality (7). This notion is referenced to the process of locating inconsistencies, since when users arise, they should perform traceability of them quickly. If the detection is complicated, it could hinder the process of correcting the models.\", \"Resolution Quality (8). It corresponds to the level of quality related to the effort that users take to look for alternatives to solve the identified problem.\", \"Motivational Quality (9). This notion refers to the motivational factors involved during the learning and development of design models, which can be intrinsic and extrinsic. Elements of gamification such as points, feedback and progress bring the user a degree of satisfaction in continuing their discovery and transformations throughout the process.\", \"Quality of Engagement (10). The user in tracking their progress can feel committed to the objective in question, and this notion represents the measurement of the level of commitment of them during the development of design models.\"]}, {\"header\": \"EVALUATION\", \"paragraph\": [\"This section describes the methodology followed to evaluate the proposed quality model. This methodology follows well-established empirical guidelines [24]. Section 5.1 details the objective and research questions (RQ). Section 5.2 presents the questionnaire formulated to evaluate the proposed quality model. Section 5.3 explains the context and selection of participants. Section 5.4 describes the presentation of the Model. Section 5.5 presents the analysis of the collected data.\"]}, {\"header\": \"Objective and Research Questions\", \"paragraph\": [\"The objective (O) of this study is twofold: (O1) Introduce Model-Game as a tool for teaching Software Modeling; and (O2) Analyze the applicability of the quality model regarding the improvement of UML models.\", \"To analyze the different facets of the objectives, two Research Questions (RQ) have been formulated:\", \"\\u2022 RQ1: How do instructors evaluate the use of gamification in software modeling? \\u2022 RQ2: What is the acceptance of ModelGame by software modeling instructors?\"]}, {\"header\": \"Questionnaire\", \"paragraph\": [\"Data was collected through an online questionnaire created through Google Forms1 following well-established guidelines described in [24]. This strategy was chosen because the questionnaire could be applied quickly and easily collect data from individuals in geographically diverse locations. The questions of the questionnaire were concerned with examining the research gaps of previous studies and apprehending the structures of the previously developed questionnaire.\", \"Part 1: Participant profile. The first part of the questionnaire consisted of collecting data that are related to the characteristics and opinions of the participants. The creation of the participant profile through this data is important to make the selection of possible users of ModelGame. Without this profile, participants with an inadequate profile may generate inconsistent assessments.\", \"Participants were asked to provide more general information, such as age, education level, academic background. Information about the time of experience in teaching was also considered, including teaching software modeling and level of knowledge about UML models.\", \"Part 2: TAM questionnaire. The second part addressed questions about the usability and acceptance of the technique, aiming to explore q3. To this end, this part of our questionnaire is based on the technology acceptance model (TAM) [16]. This part contained nine questions, which were answered through the Likert Scale, including Totally Agree, Partially Agree, Neutral, Partially Disagree, and Totally Disagree. The questions formulated (Q) dealt with several topics, including perceived ease of use (Q1-3), perceived utility (Q4-7), attitude towards use (Q8), and behavioral intention to use (Q9).\"]}, {\"header\": \"Selection of participants\", \"paragraph\": [\"The participants were selected based on the following criteria: instructors and/or professionals working in the teaching of software modeling in higher education institutions in Brazil. Using this criterion, we sought to select participants with academic training and practical experience in teaching. This finite set of all possible participants represents the target population [13]. This population represents those people who are in a position to answer the questions formulated and to whom the results of the survey apply [13]. In all, 19 people (n) answered the questionnaire. The participants were invited via e-mail to participate in the study and each of them previously received the explanation/training about the model proposed through the researcher and there was no doubt, they could leave for the next step that consisted of completing the TAM questionnaire. We discussed the experimental process in the next section.\"]}, {\"header\": \"Experimental Process\", \"paragraph\": [\"Figure 3 presents the experimental process used in this study, which is composed of three phases discussed below:\", \"Phase 1: Presentation. It has an activity, presentation, in which the researcher explained to the participants through a video detail about the quality model. This process took place individually and in a standard way, where space was also made available for participants to answer possible doubts about the proposed study and model, lasting an average of 20 minutes.\", \"Phase 2: Application of the TAM questionnaire. It has two activities, the first being Collect demographic data. The participants answered a list of questions (input) so that we could collect their characteristics and opinions about the ModelGame. The demographic data collected (output) became the result of this activity.\", \"The second activity Apply TAM questionnaire (input). Participants received a list of questions about the perception of ease of use, perceived utility, attitudes, and intention of behavior, in relation to the ModelGame. Qualitative data (output) were generated, regarding the usability and acceptance of the Model under the perspective of professionals who teach software modeling. This questionnaire followed the guidelines of the TAM [16].\", \"Phase 3: Analysis and result report. It has two activities. The first, Analyze data sought to perform a thorough analysis of the data collected through the questionnaire and the researcher\'s perception regarding the participants\' doubts during the presentation stage. For this, the collected data were analyzed separately, as well as confronted, aiming to perform a triangulation of them. Subsequently, there was an Evaluation data, as a way to understand in a more depth the context, the perceptions of the participants in relation to the proposed model as well as its applicability.  3 describes the profile data, reporting the characteristics and opinions of the participants. These data were collected from May 18 to June 5, 2021. In total, we had 19 participants. Our participants are between 20 and 49 years old, most of them have a degree in Computer Science (52.6%), Information Systems (26.3%) or Systems Analysis (21.1%) and are specialists (36.8%), masters (36.8%) and doctors (15.8%). About the working time in teaching, the majority (42.1%) they have been teaching for more than 8 years and teach disciplines related to software modeling, including software engineering, systems analysis and software projects. A total of 47.4% have a full level of knowledge about UML and almost half of them (47.4%) has not yet used gamification in the teaching of software modeling. Therefore, we consider\"]}, {\"header\": \"Perceived usefulness\", \"paragraph\": [\"The model would make it easier to understand which elements of gamification can be used in modeling . 12 5 2 0 0 Using the quality model would help increase productivity. 9 8 2 0 0 The model would provide an understanding of how to mitigate the incompleteness of UML diagrams.   We consider the percentage of instructors who have not yet used gamification in their classes to be high and this may be tied to factors such as lack of knowledge, information about the tool, and even time to plan and include these approaches [22]. Although they were based on software modeling teaching context, previous studies [3,4,12,17,25] they did not count on the participation of instructors and we understand that this participation is fundamental to understand the perceptions of these professionals since they will be at the forefront of the use of gamification.\", \"The ModelGame proposed in this study could help them insert gamification into their classes, according to the software modeling learning design [25], based on the assumption that for this, it is necessary to develop a better understanding of the tasks, activities, skills and operations that the different elements of gamification can offer and how they can correspond to the desired learning outcomes by developing a more concrete and motivating presentation that can involve students and facilitate deep learning with UML.\"]}, {\"header\": \"RQ2:\", \"paragraph\": [\"What is the acceptance of the ModelGame by software modeling instructors? Using the TAM questionnaire, we tried to evaluate the ease of use, perceived usefulness, attitude, and behavioral intention to use the Quality Model. Table 2 shows the data obtained. Our data obtained show that no one disagreed that the ModelGame is easy to use, learn, and master. On the contrary, almost 90% of participants find the model easy to use (42.1% totally agree and 47.4% partially agrees and 10.5% neutral), learn (52.6% fully agree and 47.4% partially agree) and master (31.6% fully agree, 63.2% partially agree and 5.3% partially disagree).\", \"The results are also favorable considering the perception of utility. Most participants realized that the ModelGame would make it easier to understand which elements of gamification can be used in each of the phases of modeling using UML(63.3% totally agree, 26.3% partially agree and 10.5% neutral), increase productivity (47.4% fully agree, 42.1% partially agree and 10.5% neutral), and the use of the quality model would provide an understanding of how to mitigate the incompleteness of UML diagrams (26.3% agree totalmen 42.1% partially agree, 26.3% neutral and 5.3% partially disagree). Still in the useful aspect, we tried to know if the quality model would help to compare validated theories about the inclusion of gamification in software modeling teaching (68.4% totally agree, 21.1% partially agree and 10.5% neutral).\", \"Considering the attitude towards use, participants believe that using the ModelGame is a good idea (68.4% totally agree, 26.3% partially agree and 5.3% neutral), just as they are confident and would use the Model in software modeling classes (52.6% totally agree, 36.8% partially agree and 10.5% neutral). These findings show the potential for acceptance by people with profiles similar to those of participants. The results are encouraging and show the potential to use the proposed approach in the educational scenario.\"]}, {\"header\": \"THREATS TO VALIDITY\", \"paragraph\": [\"This section discusses the possible threats to the validity of the study.\", \"Internal validity. The main point affecting the internal validity of our study concerns the total time used for the exploratory phase. To mitigate this threat, we performed the video recording of a pilot explaining the operating details and objectives of the ModelGame. In relation to the methods used, the threats related to internal validity relate to how we extract the perceptions of the discussions and whether they represent the perceptions of teachers about the use of the Model. We try to reduce this threat by applying the TAM questionnaire.\", \"External validity. We identified threats related to external validity, such as the number of participants who never applied the use of gamification. This study was limited to 19 participants (teachers) from various educational institutions, of which 9 (47.4%) never used any element of gamification in their classes, this factor can interfere in the data, since the model intends to evaluate the quality of UML diagrams from gamified activities.\", \"Conclusion validity. Threats related to the validity of the conclusion are related to treatment and outcome. We try to make the reduction by combining quantitative and qualitative data through different resources. These data were obtained through audio and questionnaires. We analyze this data to answer the research questions.\"]}, {\"header\": \"CONCLUSIONS AND FUTURE WORK\", \"paragraph\": [\"This study proposed an initial quality model (ModelGame) that serves as a reference framework for instructors for qualitative evaluations of UML models developed from gamified activities, the application of an empirical study with 19 participants was carried out to understand their vision in relation to gamification and the acceptance of the proposed Model. It was identified that most have not yet used gamification in their classes, but agree that their use can contribute to the quality of the models developed by the students and were open to using the model. Our findings can enhance the adoption of new teaching practices through gamification, resulting in the improvement of software modeling learning using UML, and consequently the creation of models developed by students. These approaches can stimulate students\' immersion in the design of systems as future professionals during learning.\", \"Finally, we hope to carry out in the future a series of experimental studies to analyze each stage of application of the ModelGame and that this work represents a first step to better support the application of empirical studies on models of evaluation of the use of gamification in software modeling. We also hope that the questions described throughout the article will encourage other researchers to extend our study to different modeling languages and teaching methodologies.\"]}]','https://drive.google.com/uc?id=1edN59wtD_Ir8NeBz3XVX_cmn3_PbfWiB&export=download','2021-02-03',0,'2024-02-03 22:08:16.141185','2024-02-03 22:08:16.366936'),(2,'AI Model for Computer games based on Case Based Reasoning and AI Planning','Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory.','[{\"header\": \"Introduction\", \"paragraph\": [\"The goal of this effort is to explore a model for design and implementation of an AI agent for turn based games. This model provides for building more capable computer opponents that rely on strategies that closely resemble human approach in solving problems opposed to classical computational centric heuristics in game AI. In this manner the computational resources can be focused on more sensible strategies for the game play.\", \"With the advancement in computer hardware increasingly more computing power is left for executing AI algorithms in games. In the past AI in games was mainly a cheating set of instructions that simulated the increasing difficulty in the game environment so that the player had the illusion of real counterpart. Improvement in available memory and processing power allows implementation of more intelligent algorithms for building the game environment as well as direct interaction with the human players.\", \"In this particular research the emphasis is put on the interaction between the AI agent and a computer player in the realm of the game rules. It is particularly focused on turn based games that have the elements of uncertainty like dice or concealed information. At the beginning a description of Game AI algorithms are given; such as Game Trees and Minimax. The following section describes an approach of using AI Planning to improve building Game Trees in games with imperfect information where Game Trees tend to be very large with high growth ratio. Section 4 discusses another approach that provides a significant reduction to the number of considered moves in order to find the favorable strategy of the AI player. This approach uses AI Planning techniques and Case Base Reasoning (CBR) to plan for different scenarios in predetermined strategies which would be analogous to human player experience in the particular game. The CBR database illustrates a set of past experiences for the AI problem and the AI Planning illustrates the procedure to deal with the given situation in the game. In the next two sections implementations and evaluations of both approaches are given. The AI Planning approach is implemented with the Tic-tac-toe game and the combined AI Planning and CBR approach is implemented with a model for the Monopoly game. The last part contains conclusions and future work ideas.\"]}, {\"header\": \"Game Trees and Minimax\", \"paragraph\": [\"Game Trees are common model for evaluating how different combinations of moves from the player and his opponents will affect the future position of the player and eventually the end result of the game. An algorithm that decides on the next move by evaluating the results from the built Game Tree is minimax [1]. Minimax assumes that the player at hand will always choose the best possible move for him, in other words the player will try to select the move that maximizes the result of the evaluation function over the game state. So basically the player at hand needs to choose the best move overall while taking into account that the next player(s) will try to do the same thing. Minimax tries to maximize the minimum gain. Minimax can be applied to multiple Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. DIMEA\'08, September 10-12, 2008, Athens, Greece. Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00 levels of nodes on the game tree, where the leaves bring the final known (or considered) game state.\"]}, {\"header\": \"The minimax theorem states:\", \"paragraph\": [\"For every two-person, zero-sum game there is a mixed strategy for each player, such that the expected payoff for both is the same value V when the players use these strategies. Furthermore, V is the best payoff each can expect to receive from a play of the game; that is, these mixed strategies are the optimal strategies for the two players.\", \"This theorem was established by John von Neumann, who is quoted as saying \\\"As far as I can see, there could be no theory of games \\u2026 without that theorem \\u2026 I thought there was nothing worth publishing until the Minimax Theorem was proved\\\" [2].\", \"A simple example of minimax can be observed by building a game tree of the tic-tac-toe game. The tic-tac-toe game is a simple game which can end by the first player wining, the second player wining or a tie. There are nine positions for each of the players in which at each turn the player puts X or O sign. If the player has three adjacent signs in a row, column or the two diagonals he or she wins. This game has limited number of position and it is well suited for building the whole game tree. The leaves of this tree will be final positions in the game. A heuristics evaluation function will also need to be written to evaluate the value of each node along the way.\"]}, {\"header\": \"AI Planning for building Game Trees\", \"paragraph\": []}, {\"header\": \"AI Planning\", \"paragraph\": [\"AI Planning also referred as Automated Planning and Scheduling is a branch of Artificial Intelligence that focuses on finding strategies or sequences of actions that reach a predefined goal [3]. Typical execution of AI Planning algorithms is by intelligent agents, autonomous robots and unmanned vehicles. Opposed to classical control or classification AI Planning results with complex solutions that are derived from multidimensional space.\", \"AI Planning algorithms are also common in the video game development. They solve broad range of problems from path finding to action planning. A typical planner takes three inputs: a description of the initial state of the world, a description of the desired goal, and a set of possible actions. Some efforts for incorporating planning techniques for building game trees have also shown up, similar to the approach explored in this effort. In addition Cased Based Reasoning [4] techniques are also gathering popularity in developing strategies based in prior knowledge about the problems in the games. One of the benefits from Hierarchical Task Network (HTN) [5] planning is the possibility to build Game Trees based on HTN plans; this method is described in the following section.\"]}, {\"header\": \"Game Trees with AI Planning\", \"paragraph\": [\"An adaptation of the HTN planning can be used to build much smaller and more efficient game trees. This idea has already been implemented in the Bridge Baron a computer program for the game of Contact Bridge [6].\", \"Computer programs based on Game Tree search techniques are now as good as or better than humans in many games like Chess [7] and checkers [8], but there are some difficulties in building a game tree for games that have imperfect information and added uncertainty like card or games with dice. The main problem is the enormous number of possibilities that the player can choose from in making his move. In addition some of the moves are accompanied with probabilities based on the random elements in the games. The number of possible moves exponentially grows with each move so the depth of the search has to be very limited to accommodate for the memory limitations.\", \"The basic idea behind using HTN for building game trees is that the HTN provides the means of expressing high level goals and describing strategies how to reach those goals. These goals may be decomposed in goals at lower level called sub-goals. This approach closely resembles the way a human player usually addresses a complex problem. It is also good for domains where classical search for solution is not feasible due to the vastness of the problem domain or uncertainties.\"]}, {\"header\": \"Hierarchical Task Networks\", \"paragraph\": [\"The Hierarchical Task Network, or HTN, is an approach to automated planning in which the dependency among actions can be given in the form of networks [9] [Figure 1].\", \"A simple task network (or just a task network for short) is an acyclic digraph \\u202b\\u0753\\u202c \\u0d4c \\u123a\\u0737\\u01e1 \\u202b\\u0727\\u202c\\u123b in which U is the node set, E is the edge set, and each node \\u202b\\u0751\\u202c \\u202b\\u05d0\\u202c \\u0737 contains a task \\u202b\\u0750\\u202c \\u0be8 . The edges of \\u202b\\u0753\\u202c define a partial ordering of U. If the partial ordering is total, then we say that \\u202b\\u0753\\u202c is totally ordered, in which case \\u202b\\u0753\\u202c can be written as a sequence of tasks \\u202b\\u0753\\u202c \\u0d4c \\u202b\\u0750\\u06c3\\u202c \\u0b35 \\u01e1 \\u202b\\u0750\\u202c \\u0b36 \\u01e1 \\u01e5 \\u01e1 \\u202b\\u0750\\u202c \\u202b.\\u06c4\\u202c\"]}, {\"header\": \"Figure 1: Simple Hierarchical Task Network\", \"paragraph\": [\"A Simple Task Network (STN) method is a 4-tuple of its name, task, precondition and a task network. The name of the method lets us refer unambiguously to substitution instances of the method, without having to write the preconditions and effects explicitly. The task tells what kind of task can be applied if the preconditions are met. The preconditions specify the conditions that the current state needs to satisfy in order for the method to be applied. And the network defines the specific subtasks to accomplish in order to accomplish the task.\", \"A method is relevant for a task if the current state satisfies the preconditions of a method that implements that task. This task can be then substituted with the instance of the method. The substitution is basically giving the method network as a solution for the task.\", \"If there is a task \\\"Go home\\\" and the distance to home is 3km [Figure 2] and there exists a method walk-to and this method has a precondition that the distance is less than 5km, then a substation to the task \\\"Go home\\\" can be made with this method instance. If the distance is larger than 5km another meth to be substituted [Figure 3]. An STN planning domain is a set of operatio methods M. A STN planning problem is a 4-tu state S 0 , the task network w called initial task STN domain. A plan \\u07e8 \\u0d4c \\u202b\\u073d\\u06c3\\u202c \\u0b35 \\u01e1 \\u01e5 \\u01e1 \\u073d \\u202b\\u06c4\\u202c is a soluti problem if there is a way to decompose w into \\u03c0 and each decomposition is applicable in the ap the world. The algorithm that is capable to networks into plans is called Total-forward-deco [9] or Partial-forward-decomposition (PFD). H cases where one does not want to use a forwa procedure. HTN planning is generalization of S gives the planning procedure more freedom construct the task networks.\", \"In order to provide this freedom, a bookke is needed to represent constraints that the plann not yet enforced. The bookkeeping is done by unenforced constraints explicitly in the task netw\", \"The HTN generalizes the definition of a STN. A task network is the pair \\u202b\\u0753\\u202c \\u0d4c \\u123a\\u0737\\u01e1 \\u202b\\u0725\\u202c\\u123b w task nodes and C is a set of constraints. Eac specifies a requirement that must be satisfied by a solution to a planning problem.\", \"The definition of a method in HTN also definition used in STN planning. A HTN pla name, task, subtasks, and constraints. The s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.\", \"Compared to classical planners the prim HTN planners is their sophisticated knowledge r reasoning capabilities. They can represent and non-classical planning problems; with a good guide them, they can solve classical planning p magnitude more quickly than classical or neoc The primary disadvantage of HTN is the nee author to write not only a set of planning opera of methods.\"]}, {\"header\": \"HTN Planning in building Game\", \"paragraph\": [\"For a HTN planning algorithm to be adap trees we need to define the domain (set of H operators) which is the domain of the game. Thi a knowledge representation of the rules of the environments and possible strategies of game pla In this domain the game rules as well as kn tackle specific task are defined. The implem Tree building with HTN is called Tign implementation uses a procedure simila decomposition, but adapted to build up a game nown strategies to mentation of Game num2 [9]. This ar to forwardtree rather than a plan. The branches of the game tree rep the methods. Tignum2 applies all met state of the world to produce new continues recursively until there are n have not already been applied to th world.\", \"In the task network generated by Tignu actions will occur is determined by th By listing the actions in the order network can be \\\"serialized\\\" into a gam\"]}, {\"header\": \"Case Based Reasoning in 4.1 Case Based Reasoning\", \"paragraph\": [\"Case-based reasoning (CBR) is a Artificial Intelligence (AI), both as problems and as a basis for standalone Case-based reasoning is a paradigm solving and learning that has became applied subfield of AI of recent yea intuition that problems tend to recur. I are often similar to previously en therefore, that past solutions may be of [10].\", \"CBR is particularly applicable to probl available, even when the domain is n for a deep domain model. Helpdesks, systems have been the most successfu to determine a fault or diagnostic attributes, or to determine whether or repair is necessary given a set of past s  lems where earlier cases are not understood well enough , diagnosis or classification ul areas of application, e.g., an illness from observed r not a certain treatment or olved cases [11].\"]}, {\"header\": \"rom HTN ree Algorithm\", \"paragraph\": [\"Central tasks that all CBR methods have to deal with are [12]: \\\"to identify the current problem situation, find a past case similar to the new one, use that case to suggest a solution to the current problem, evaluate the proposed solution, and update the system by learning from this experience. How this is done, what part of the process that is focused, what type of problems that drives the methods, etc. varies considerably, however\\\".\", \"While the underlying ideas of CBR can be applied consistently across application domains, the specific implementation of the CBR methods -in particular retrieval and similarity functions-is highly customized to the application at hand.\"]}, {\"header\": \"CBR and Games\", \"paragraph\": [\"Many different implementations of CBR exist in games. CBR technology is nicely suited for recognizing complex situations much easier and more elegant than traditional parameter comparison or function evaluation. There are especially evident cases in real time strategies where different attack and defense of global strategies are nicely defined by CBR datasets and later used in the running games. Also intelligent bots behavior is also another typical example. Depending on the number of enemy bots the layout of the terrain and position of human players the CBR system finds the closest CBR case and employs that strategy against the human players which in prior evaluation was proved to be highly efficient.\"]}, {\"header\": \"Game Trees with AI Planning -Tic-tac-toe\", \"paragraph\": [\"In order to show the expressive power of AI Planning in defining strategies for games, and the use of these plans to build Game Trees I implemented an algorithm that builds Game Trees for the Tic-Tac-Toe game.\", \"The game tree of Tic-Tac-Toe shows 255,168 possible games of which 131,184 are won by X (the first player), 77904 are won by O and the rest 46,080 are draw [13]. All these games can be derived from building a complete Game Tree.\", \"Even though it is possible to build a complete game tree of Tic-tac-toe it is definitely not an optimal solution. Many of the moves in this tree would be symmetrical and also there are a many moves that would be illogical or at least a bad strategy to even consider.\", \"So what strategy should X (the first player) choose in order to win the game?\", \"There are few positions that lead to certain victory. These positions involve simultaneous attack on two positions so the other player could not defend, basically the only trick in Tic-Tac-Toe. There are many different arrangements of the player\'s tokens that give equivalent positions as these three positions. By using planning we do not need to consider all possible layouts but just consider these three similar to what a human would consider.\", \"The game starts from an empty table.\", \"The two relevant strategies that would lead to these positions are to take one corner or to take the center [Figure 7].\"]}, {\"header\": \"Figure 7: Tic-tac-toe Two starting moves\", \"paragraph\": [\"The center position as we can see in the simulation results lead to a bigger number of victorious endings but it is also a straight forward strategy with obvious defense strategy.\", \"At this point we need to consider the moves of the opponent. If we take the left branch the opponent moves can be a center, a corner or a middle field. We also need to differentiate with a move to a corner adjacent with our like top left or bottom right or across the center to bottom right [Figure 8].\"]}, {\"header\": \"Figure 8: Tic-tac-toe opponent response to corner move\", \"paragraph\": [\"In cases one and two, we have a clear path to executing strategy 3 so we need to capture the diagonally opposite field. And as for the third case the best way to go is to capture the center and go for strategy 1 or 2 depending of the opponent\'s next move.\"]}, {\"header\": \"Figure 9: Tic-tac-toe move 2 after corner opening\", \"paragraph\": [\"The first move leads to certain victory, O will have to go to the center and X will achieve strategy 3 [Figure 9]. The second move is a possible way to strategy 3 if O makes a mistake in the next loop, so X goes to the opposite corner. For the third case since O is playing a valid strategy the only move that leaves a possible mistake from O would be to take the center and wait for O to go to the middle and then achieve strategy 1 or 3 which will be a symmetric situation to the one that we will find if we branched with the center.\"]}, {\"header\": \"Figure 10: Tic-tac-toe opponent response to center move\", \"paragraph\": [\"If we go back to the second branch [Figure 10], a possible way for the second player to engage is corner or middle. The first move is a valid strategy for O and can be mee corner move from X to try a mistake from O in the same as in the third case above from the pre another move would be go to the middle wh achieves strategy 1 or 2. To sum the strategies for the planning, first corner strategy for the beginning. Then for the ce the corners with the particularly the one oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent or try to implement strategies 1, 2 or victory.\"]}, {\"header\": \"Hierarchical Task Network\", \"paragraph\": [\"Top level task is Play [Figure 12]. This is a can be derived into: Win, Block, Tie or Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.\"]}, {\"header\": \"TN\", \"paragraph\": [\"This HTN when executed will re game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.\", \"This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player with only 457 games, 281 of w and 0 where the second opponent w reduction over the 255, 168 possible g tree. These reductions can be very use computing capabilities but also we pr that planning can be very efficient if d trees by applying reasoning very reasoning.\", \"Further improvements to the gam the opponents moves are also planned all the meaningless and symmetrical m\"]}, {\"header\": \"Game AI in Monopoly 6.1 Overview of the AI Imp\", \"paragraph\": [\"The AI agent is responsible for players in the game. The core principle a Game Tree with all the sensible move make from the current point of time minimax algorithm the agent selects t would bring the computer player mo with the highest probability. Building that would be big enough to consider is obstructed by the vastness of poss with all the possible random landings nodes of the game tree exponentially tackle this problem the AI agents discussed technologies: Case Based Re\", \"The technologies are employed First the agent searches the CBR datab largest similarity with the current state associated with a playing strategy. Th that the planner needs to build plans f consecutive player moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of a single player. After the strateg considered the response to those strate by the opponent(s). The move of the probability distribution of the dice as player. A more general strategy needs opponent\'s (human player) moves sin the expertise of the opponent. This ge more plausible moves than the focused After covering all opponents t deducting a feature move of the com CBR selected plan strategy. After strategies and reaching a reasonable s into account the memory limits an probabilities that the move is possible the dice the building of the Game Tre algorithm searches the Game Tree favorable move for the AI player usi The process is repeated each time the A esult with plans for possible m each position and linking the player we create a game ich we can run the minimax arget strategies creates a tree ssible moves for the second which X wins 176 are draw wins. This is a significant ames with a complete game eful for devices with limited rove a very important point designing meaningful game similar to human player me tree are also possible if d, in other words if we drop moves of the opponent. plementation the moves of the artificial e of the AI agent is building es that all the players would e forward. Then using the the move that in the future ost favorable game position a Game Tree in this game sufficient number of moves sible moves in combination of the dice. The number of y grows at each level. To incorporates two already easoning and AI Planning.\", \"in the following manner. base to find the case with the e of the board. This case is he strategy consists of goal for, and the plans consist of he player to that goal. This rategy are considered, those ossible moves the number of creases immensely. e model considers the moves gies of the AI player are egies needs to be considered opponent(s) depends of the well as the strategy of the s to be implemented for the nce we cannot be aware of eneral strategy would bring d strategy of the AI player. the agent comes back to mputer player by using the creating several loops of size of a Game Tree taking nd the rapidly decreasing e due to the distribution of ee stops. Then the minimax and decides on the most ing the minimax algorithm. AI player is up.\", \"Buying, auctioning and trading game moves are always accompanied by return of investment calculations in making the plans. These calculations represent adaptation of the more general planning associated with the cases in the CBR database. These adaptations are necessary due to the fact that the cases do not identically correspond to the situation on the table. In addition calculating the game position value of each node of the game tree is done by heuristic functions that incorporate economic calculations of net present value, cash, and strategic layout and so on. For example railroads in monopoly are known to be strategically effective because they bring constant income even though the income can be smaller than building on other properties.\"]}, {\"header\": \"Details on the CBR Implementation\", \"paragraph\": [\"The implementation of the CBR is by using the JColibri2 platform. JColibri2 is an object-oriented framework in Java for building CBR systems that is an evolution of previous work on knowledge intensive CBR [14].\", \"For this implementation we need to look into three particular classes of the JColibri2 platform. The StandardCBRApplication, Connector, CBRQuery. For a JColibri2 implementation the StandardCBRApplication interface needs to be implemented.\", \"The CBR cycle executed accepts an instance of CBRQuery. This class represents a CBR query to the CBR database. The description component (instance of CaseComponent) represents the description of the case that will be looked up in the database. All cases and case solutions are implementing the CaseComponent interface.\", \"The JColibri2 platform connects to the CBR database via a Connector class. Each connector implements all the necessary methods for accessing the database, retrieval of cases, storing and deletion of cases. This implementation uses a custom XML structure for holding the CBR cases. Since the game will not update the CBR database only read it, a XML solution satisfies the needs. The XML file to a certain extent is similar to the XML representation of the board. We are interested in finding one CBRCase that is the most similar case to the situation in the game at the time of the search. This procedure is done in the cycle method of the CBRApplication. The JColibri2 CBR comparison is done by Nearest Neighbor (NN) search method.\", \"JColibri2 offers implementations for NN search algorithms of simple attributes. These implementations are called local similarities. For complex attributes like in our case global customized similarity mechanisms need to be implemented.\", \"The MonopolyDescription class [Figure 13] is basically a serialization of the GameState. It holds all the information about the state of the board, the players, their amount of cash etc.\"]}, {\"header\": \"Figure 13: Class diagram of the Monopoly Case component models\", \"paragraph\": [\"On the other hand the MonopolySolution class holds the three particular attributes that are needed for the planning, the planning Domain, State and TaskList.\", \"The game is implemented by using the Model-View-Controller software development pattern. The controller is responsible for implementing the game rules and handling all of the events in the game like roll of dice, input commands for trading, auctioning and etc from the players. The View layer is responsible for displaying the board and all of the input widgets on to the game screen, and the models are data structures representing the game state [Figure 14].\"]}, {\"header\": \"Complex Similarity representation in CBR\", \"paragraph\": [\"The similarity measurement part of the Nearest Neighbor algorithm JColibri2 is implemented by implementing the LocalSimiralrityFunction and the GlobalSimiralityFunction interface. A local similarity function is applied to simple attributes by the NN algorithm, and a global similarity function is applied to compound attributes. In the case of our implementation the attributes of the MonopolyDescription are compound attributes describing the state of the board, number of players, amount of cash for every player and etc. Since MonopolyDescription is a custom CaseComponent a global similarity function needs to be implemented to accurately find the distance between different CBR cases.\", \"The similarity mechanism is inseparable core element of the CBR system. This mechanism represents how the CBR decides which strategy is best suited for the particular situation by calculating the distance or similarity to other cases in the database.\", \"For the monopoly implementation we need to consider several basic strategies. Monopoly is based on investing in properties and receiving revenues from those investments. One of the basic strategies of the game is to build a set of properties that will bring constant income larger than the one of the opponents. So in time the opponents will have to declare bankruptcy. But on the other hand over investment can lead to too stretched resources with low income that will eventually drove the player to bankruptcy. To decide on these two we need a clear separation into two groups of cases in the CBR database. The first group of cases will represent a situation on the board where the player has significant income per loop formed of one or more color group properties, maybe railroads, some buildings on them and so on. It is important to note that in this case the player is better situated than his opponents so he only needs to survive long enough to win the game. In the other group of cases either the opponent is not well positioned on the board or its opponents are better situated. In this case further investments are necessary to improve the situation so the player can have a chance of winning in the long run.\", \"These metrics can be owning color groups, valuing groups of railroads, evaluating the other opponents as well, and considering the amount of cash. As it is obvious in monopoly the number of streets is not as nearly as important as the combination of streets the player owns. It is also important to note that one CBR case does not hold only a single strategy in place, but its solution can have multiple different strategic goals. For example one CBR case might simultaneously say buy this land to form a color group but also trade some other unimportant property to increase cash amount.\", \"The cases do not represent all possible combinations of board positions. They are only representation of typical game scenarios. The CBR Case solutions do not give exact instructions in general but rather strategic goals. For example one CBR Solution might say trade the streets that you only have one of each for the ones that you have two of that color already. Then the planner based on the situation on the board needs to decompose this high level task to a low level operations. Like offer \\\"Mediterranean Avenue\\\" for \\\"Reading Railroad\\\" and offer $50. The exact amounts and actual streets are left to the planer to evaluate.\", \"The monopoly CBR database is currently in development on a monopoly clone game called Spaceopoly. The cases are architected based on human player experience and knowledge. There is a plan of making a number of slightly different strategies that differ on the style of playing and then running simulation tests that would determine the particular validity of each database as well as validity of certain segments of the strategy or even particular cases in the database.\", \"The actual execution of the strategies will not differ from strategy to strategy since the plan execution is more related to the structure and rules of the game than to the actual playing strategy.\"]}, {\"header\": \"Details on the Planning Implementation\", \"paragraph\": [\"For the purpose of planning this implementation uses a modification of the JSHOP2 planner. The Java Simple Hierarchical Ordered Planner 2 is a domain independent HTN planning system [15]. JSHOP2 uses ordered task decomposition in reducing the HTN to list of primitive tasks which form the plans. An ordered task decomposition planner is an HTN planner that plans for tasks in the same order that they will be executed. This reduces the complexity of reasoning by removing a great deal of uncertainty about the world, which makes it easy to incorporate substantial expressive power into the planning algorithm. In addition to the usual HTN methods and operators, the planners can make use of axioms, can do mixed symbolic/numeric conditions, and can do external function calls.\", \"In order for the JSHOP2 planer to generate plans it needs tree crucial components: Domain, State and Tasks. The Domain defines all the functionalities that the particular domain offers. These are simple and complex tasks. The complex tasks also called methods create the hierarchy with the fact that they can be evaluated by simple tasks of other complex tasks. This is how a hierarchical structure of tasks is formed. The problem reduction is done by reducing the high level complex tasks to simpler until all the tasks are primitive. The list of primitive tasks forms the plan.\", \"The State represents the state of the system. It is a simple database of facts that represent the state of the system. The State is necessary to determine the way the problems or tasks are reduced to their primitive level. The reduction is done by satisfying different prerequisites set in the methods; these prerequisites are defined in the state. The Tasks are high level tasks or methods defined in the Domain. The planner based on the State and the goals selects one or more high level tasks that need to be reduced to plans [Figure 15].\"]}, {\"header\": \"Figure 15: Diagram of a Planner\", \"paragraph\": [\"The plans then generate the game moves. The number of moves generated by the plans is just a fraction of the possible moves at that point. This reduces the game tree providing the opportunity to generate smaller and deeper game trees and making more efficient decisions in general.\"]}, {\"header\": \"Conclusion\", \"paragraph\": [\"Even though the results from the CBR database are not complete at this time partial strategies are implemented as cases and recognized during game play by the CBR system. These smaller local strategies coupled with more global higher level strategies that are particularly important at the beginning of the game would form a complete CBR database and represent a knowledge engineered style of playing of the AI player.\", \"The AI Planning approach is a proven method by the tic-tactoe experiment and is suitable for implementing the strategies associated with the CBR cases. This approach in general benefits from both technologies, CBR as well as AI Planning and comprises an elegant solution. Even though AI Planning can be enough as a single technology for some simpler problems like tic-tac-toe the complexity of Monopoly would mean that the Planner would have to incorporate Core Planner Tasks Plan State large and complex domain and a very big state model. The CBR application helps reduce this complexity by focusing the planning on smaller domain of the game. Basically the CBR reduces the overall goal of the play (wining the game) to smaller more concrete goals suitable to the particular state of the game, thus reducing the need for global planning strategies and complex planning domain.\", \"Furthermore this symbiosis of technologies gives way for more precise and finely tuned strategies which can be difficult to include into global plan for the whole game. One simple example for the Monopoly game would be this: Sometimes it\'s better to stay in jail because rolling double increases the probability of landing on some field (two, four, six, eight, ten or twelve steps from the jail) that can be of great importance to the rest of the game. These and similar small local strategies can be easily recognized by similar cases in the CBR database.\", \"In other words the system is flexible enough so that new strategies can be incorporated easily missing strategies can be also recognized by the distance metrics as well as wrong assumptions in the strategies can be easily recognized.\", \"One other important property of the system is that is highly configurable. The game its self can be diversely different depending on the configuration of the board. Even though the platform is restricted to Monopoly type of games, changing the layout and values of the fields effectively brings completely different properties of the game. In addition the CBR database represents the entire experience of the AI Player. It can be filled with rich set of strategies or even configured with different flavors of difficulties of play, this of course coupled with the domain of the planner which can differ from a case to a case as well.\"]}, {\"header\": \"Future Work\", \"paragraph\": [\"Further exploration of this technology would go towards complete implementation of an AI aware agent for monopoly. Initial results from the local cases with more specific strategies show CBR as a capable tool for representing expertise in playing the game. Completing the more general strategies and coupling them with the planning domain will give precise results on the benefits from this architecture.\", \"There is also need for exploring the planning of strategies of opponents. This task is to some extent different because we cannot always expect the opponent to select the best move we think. In the Tic-tac-toe example all possible moves of the opponent were taken into consideration, if we used the same planner for the opponent only tie games would result from the game tree. In other words mistakes of the players also need to be considered.\", \"The CBR Platform brings other functionalities well worth of exploring as well. The revision stage of the JColibri2 platform is basically capable of fine tuning strategies or even developing new strategies for the games. A well written underlying AI planning model with a capable feedback of the game tree evaluation back to the CBR revision capability can be an interesting concept in automatic experience acquisition for the AI model.\", \"There are also many other fields were combined CBR and planning approach can be incorporated into a problem solution. This combination is analogous in a big extent to a human way of reasoning. People in addition to logic of reasoning in situations with lack of information rely to planning strategies and prior experience, exactly the intuition behind CBR -AI Planning architecture.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We would like to thank Prof. Sofia Tsekeridou for her involvement in the valuable discussions we had on the topic of CBR.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We would like to thank Prof. Sofia Tsekeridou for her involvement in the valuable discussions we had on the topic of CBR.\"]}]','https://drive.google.com/uc?id=1GGqPf0yL9PMcFOUtx3vRoszJjMiIpfdJ&export=download',NULL,0,'2024-02-03 22:08:18.724581','2024-02-03 22:08:18.908695'),(3,'How to Teach Software Modeling','To enhance motivation of students to study software engineering, some way of finding balance between the scientific aspect and the practical aspect of software engineering is required. In this paper, we claim that teaching multiple software modeling techniques from a unified viewpoint is a good way of obtaining the balance and attracting the students\' interest as well.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Software engineering education at universities faces a common problem; that is regular students do not usually have experience of developing software for practical use and thus are not motivated for software engineering aiming at high quality software production by a project team or a persistent organization. Software projects conducted by students simulating real scale software development may help enhance students\' motivation, although it requires a lot of efforts to prepare such projects and manage them.\", \"Another way of solving this problem is to teach those who already have real experience in industry. In our case, there are currently five Ph. D. students under the author\'s supervision who are working at companies as well as doing research in our lab. As a by-product, interactions between the part-time students and the other regular students stimulate each other, particularly enlightening the regular students to practical software issues. However, too much emphasis on practicality may bring negligence to science and technology and may generate anti-intellectualism. A good balance between the scientific aspect and the practical aspect of software engineering should always be pursued.\", \"In our view, teaching various software modeling techniques is a good way to achieve balanced software engineering education. It is needless to say that model is a key concept and modeling is an essential skill in software engineering. There are a variety of modeling techniques; some are intuitive and quite accessible to novices, while some are highly sophisticated and attract theory oriented students and researchers.\", \"In this paper, we would like to show that it is effective to teach multiple modeling techniques from a unified viewpoint. It is based on our experience of teaching software engineering courses at several universities in Japan. Recently, the author published a textbook on software engineering, specifically focused on software modeling (unfortunately, it is written in Japanese) [1]. The book covers the whole area of software engineering, including design, testing and evolution but the modeling part has a role of attracting interests of intelligent students, who may not have much experience in developing real scale software systems. It also gives a consistent viewpoint penetrating through various techniques employed in different stages of software engineering.\"]}, {\"header\": \"MODELING TECHNIQUES\", \"paragraph\": [\"In software engineering, models are used for various purposes, e.g. life cycle model, process model, project model, product model, quality model, domain model, requirements model, design model, object model, data model, etc. In the following, we basically focus on requirements and design models but most of the discussions will hold for other kinds of models.\", \"Teaching modeling is almost equal to teaching abstraction. Models are constructed through capturing the crucial properties and structure of the target, abstracting away irrelevant details. Thus, learning how to model is a good training for mastering abstraction.\"]}, {\"header\": \"Graph Representation of Models\", \"paragraph\": [\"Many software models are represented with diagrams. Wide acceptance of UML symbolizes the trend that diagrams are often preferred to textual languages. Among many types of diagrams, graph structured diagrams are by far the most widely used. The reasons may be as follows.\", \"1. A most fundamental way for human mind to understand the world is by regarding it as consisting of a set of conceptual units and a set of relations between them. Conceptual units can be naturally illustrated with boxes or circles or whatever closed figures and relations can be illustrated with lines or arrows connecting such figures, corresponding to vertices and edges of graphs, respectively.\", \"2. It is easy to draw graph structured diagrams by hand or with drawing tools.\", \"3. Concepts and algorithms of the graph theory are available and often useful in analyzing models represented by graphs.\", \"A typical example is reasoning on transitive relations by tracing along paths of graphs. Also, the concept of subgraph is highly useful in decomposing higher-level models or clustering lower-level models.\", \"Accordingly, a number of models share the same structure of graphs. Table 1 shows graph structures of some typical models.\"]}, {\"header\": \"Commonality and Difference between Models\", \"paragraph\": [\"It is pedagogical to let students notice the common structure shared by a number of models. However, the apparent resemblance often causes confusion. Such confusion can be observed not only in software modeling graphs but in many diagrams found in daily newspapers, magazines, reports, proposals and other documents. It is often the case that one vertex denotes a type of things and another denotes quite a different type on the same diagram or one type of edges co-exist with edges with different meaning. Thus, it is important to make students consciously aware the differences between different models. We often experience that when we let students draw data flow diagrams who appear to have understood the data flow model perfectly, the diagrams turn out to be something like control flow graphs.\", \"To show the difference, it is instructive to categorize models represented by graphs. Basically, there are two categories. Static models and dynamic models may not be easily confused but confusion between different dynamic models are often observed, e.g. data flow and control flow or state transition and activity transition. Since graphs are intuitively understandable, their semantics are apt to be understood ambiguously or misunderstood.\"]}, {\"header\": \"UML\", \"paragraph\": [\"UML diagrams can also be viewed in terms of graph structures. Table 2 shows graph structures of five UML diagrams. It is usually not desirable to teach UML per se. UML is a collection of miscellaneous diagrams and its specification is continuously changing. For the pedagogical purpose, UML had better be regarded as a catalogue of analysis and design know-how collected around diagrammatic representations. Diagrams should be selected according to the policy of how to teach modeling methods.\", \"Each UML diagram contains overly rich constructs, which sometimes blur the essential property of the model. For example, the activity diagram is essentially a control flow diagram but it also includes a notation for data flow description. From the stance of emphasizing differences between various models, it is not appropriate to include such ad hoc constructs. By the same token, the collaboration diagram (, renamed to \\\"communication diagram\\\" in UML 2) is explained to have the equivalent semantics as the sequence diagram. But if that is the case, significance of the collaboration diagram is considerably limited. The author prefers to regard it as showing collaboration relations between objects, integrating a set of different sequence diagrams.\"]}, {\"header\": \"CONCLUSION\", \"paragraph\": [\"Software modeling is important by itself but teaching modeling in the software engineering course has at least two additional meanings. One is to give a bird\'s-eye view to the whole software engineering through the standpoint of modeling technology. The other is to attract interest of good students who may not have much experience in developing a real-scale software but possess intelligence and will to attack complexity of modern software construction.\"]}]','https://drive.google.com/uc?id=152MT-1j8lvznRu2iz4JNIl4ijqAjd9ED&export=download',NULL,0,'2024-02-03 22:08:21.952040','2024-02-03 22:08:22.029962'),(4,'Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model','The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97-98%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model\'s results are examined in terms of recall, accuracy, and precision •','[{\"header\": \"CCS CONCEPTS\", \"paragraph\": [\"\\u2022\"]}, {\"header\": \"INTRODUCTION\", \"paragraph\": [\"Over the past few years, a clear surge of both the amount of spammers as well as spam emails. This is likely due to a fact that the investment necessary for engaging in the spamming industry is relatively low. As a result of this, we currently have a system that identifies every email as suspicious, which has caused major expenditures in the investment of defense systems [12]. Emails are used for online crimes like fraud, hacking, phishing, E-mail bombing, bullying, and spamming. [16]. Algorithms that are based on machine learning (ML) are now the most effective and often used approach to the recognition of spam. Phishing, which is defined as a fraudulent attempt to acquire private information by masquerading as a trustworthy party in electronic communication, has rapidly advanced past use of simple techniques and the tactic of casting a wide net; instead, spear phishing uses a variety of sophisticated techniques to target a single high-value individual. Other researchers used NB, Decision Trees, and SVM to compare the performance of supervised ML algorithms for spam identification [6]. Spam emails clog up recipients\' inboxes with unsolicited communications, which frustrate them and push them into the attacker\'s planned traps [7]. As a result, spam messages unquestionably pose a risk to both email users and the Internet community. In addition, Users may occasionally read the entire text of an unsolicited message that is delivered to the target users\' inboxes without realizing that the message is junk and then choosing to avoid it. Building a framework for email spam detection is the aim of this project. In this approach, we combine the Word-Embedding Network with the CNN layer, Bi-LSTM, and GRU (BiLSTM+GRU). CNN layers are used to speed up training time before the Bi-LSTM network, and more advanced textual characteristics are extracted with the use of this network in comparison to the straight LSTM network, in less time. Gated recurrent neural networks (GRUs) are then added because they train more quickly and perform better for language modeling. To evaluate and investigate various machine learning algorithms for predicting email spam, and develop a hybrid classification algorithm to filter email spam before employing an ensemble classification algorithm to forecast it. To put an innovative technique into practice and compare it to the current method in terms of various metrics. Ensemble learning, a successful machine learning paradigm, combines a group of learners rather than a single learner to forecast unknown target attributes. Bagging, boosting, voting, and stacking are the four main types of ensemble learning techniques. To increase performance, an integrated method and the combining of two or three algorithms are also suggested. Extraction of text-based features takes a long time. Furthermore, it can be challenging to extract all of the crucial information from a short text. Over the span associated with this research, we utilize Bidirectional Large Short-Term Memories (Bi-LSTM) in conjunction with Convolutional Neural Networks (CNN) to come up with an innovative method to the detection of spam. Bagging and boosting approaches were widely preferred in this study. Contribution and paper organization is as follows: section 1.1 describes literature study, section 1.2 describe motivation for this research work, section 2 sketches procedure of details implementation, Section 3 present experimental setup, dataset description and evaluation metrics, and section 4 summarizing outcomes of the experiment.\"]}, {\"header\": \"Related Work\", \"paragraph\": [\"Email is indeed the second most frequently utilized Internet application as well as the third most common method of cyberbullying, claims one study. Cybercriminals exploit it in a number of ways, including as sending obscene or abusive messages, adding viruses to emails, snatching the private information of victims, and exposing it to a broad audience. Spam letters made up 53.95% of all email traffic in March 2020. We examine three main types of unlawful emails in our study. First are fake emails, which are sent to manipulate recipients to submit sensitive information. The second as being cyberbullying\'s use of harassing emails to threaten individuals. Suspicious emails that describe illegal activities belong to the third category. Many researchers have earlier contributed massively to this subject. The researcher claims there is some proof that suspicious emails were sent before to the events of 9/11. [14]. When it comes to data labeling, there are also convinced rule-based approaches and technologies ( like VADER) that are used, even though their efficiency of the are together is adversely affected. A hidden layer, which itself is essential for vectorization, is the top layer of the model. We use oversampling methods for this minority class because of the absence of data. Sampling techniques can help with multicollinearity, but they have an impact on simulation results. Oversampling causes data to be randomly repeated, which affects test data because dividing data may result in duplicates. Undersampling may result in the loss of some strong information. In order to advance email research, it is crucial to provide datasets on criminal activity. P. Garg et al. (2021) [5], which revealed that spam in an email was detected in 70 percent of business emails, spam was established as an obstacle for email administrators. Recognizing spam and getting rid of it were the primary concerns, as spam can be offensive, may lead to other internet sites being tricked, which can offer harmful data, and can feature those who are not particular with their content using NLP. To select the best-trained model, each mail transmission protocol requires precise and effective email classification, a machine learning comparison is done. Our study has suggested that innovative deep learning outperforms learning algorithms like SVM and RF. Current studies on the classification of emails use a variety of machine learning (ML) techniques, with a few of them focusing on the study of the sentiments consisted of within email databases. The lack of datasets is a significant obstacle to email classification. There are few publicly accessible E-mail datasets, thus researchers must use these datasets to test their hypotheses or gather data on their own. Authors [15] describe supplied two-phased outlier detection models to enhance the IIOT network\'s dependability. Artificial Neural Network, SVM, Gaussian NB, and RF (random forest) ensemble techniques were performed to forecast class labels, and the outputs were input into a classifying unit to increase accuracy. A method for content-based phishing detection was presented by the authors in [2], to classify phishing emails, they employed RF. They categorize spam and phishing emails. They enhanced phishing email classifiers with more accurate predictions by extracting features. They showed some effective Machine learning spam filtering techniques. When the PCA method is used, it will lower the number of features in the dataset. The collected features go through the PCA algorithm to reduce the number of features. The PCA method is used to make a straightforward representation of the information which illustrates the amount of variability there is in the data. The authors of [20] presented the Fuzzy C-means method for classifying spam email. To stop spam, they implemented a membership threshold value. A methodology to identify unlabeled data was put forth by the authors of [1] and applied motive analysis to the Enron data collection. They divided the data into categories that were favorable, negative, and neutral. They grouped the data using k-means clustering, an unsupervised ML technique and then classified it using the supervised ML techniques SVM and NB. Hina, Maryam, and colleagues (2021) implemented Sefaced: Deep learning-based semantic analysis and categorization of e-mail data using a forensic technique. For multiclass email classification, SeFACED employs a Gated Recurrent Neural Network (GRU) based on Long Short-Term Memory (LSTM). Different random weight initializations affect LSTMs [9]. Zhang, Yan, et al.( 2019) Experiments on three-way game-theoretic rough set (GTRS) email spam filtering show that it is feasible to significantly boost coverage without decreasing accuracy [23]. According to Xia et al. [22], SMS spam has been identified using machine learning model such as naive bayes , vector-space modeling, support vector machines (SVM), long selective memory machines (LSTM), and convolutional neural networks including every instance of a method for categorizing data. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic gradient descent (sgd) algorithms for e-mail filtering with R and orange software spam [3]. Orange software was used to create the classifications, which included Adaboost and SGD. The majority of researchers focused on text-based email spam classification methods because image-based spam can be filtered in the early stages of pre-processing. There are widely used word bag (BoW) model, which believes that documents are merely unordered collections of words, is the foundation for these techniques. Kumaresan [11] explains SVM with a cuckoo search algorithm was used to extract textual features for spam detection. Renuka and Visalakshi made use of svm [17] spam email identification, followed by selecting features using Latent Semantic Indexing (LSI). Here we have used labeled dataset to train the hybrid classifier. We used TF-IDF for feature extraction [20] and Textual features for spam detection were extracted using SVM and a cuckoo search algorithm. [4] for filtering out the spam email. Combining the integrated strategy to the pure SVM and NB methods, overall accuracy is really improved. Moreover, accurate detection for spam email has been proposed using the Negative Selection Algorithm (NSA) and Particle Swarm Optimization\'s (PSO) algorithm. PSO is used in this instance to improve the effectiveness of the classifier.\"]}, {\"header\": \"Motivation and Novelty\", \"paragraph\": [\"Email is most common form of communication between people in this digital age. Many users have been victims of spam emails, and their personal information has been compromised. The email Classification technique is employed to identify and filter junk mail, junk, and virus-infected emails prior to reach a user\'s inbox. Existing email classification methods result in irrelevant emails and/or the loss of valuable information. Keeping these constraints in mind, the following contributions are made in this paper:\"]}, {\"header\": \"PROPOSED SYSTEM ARCHITECTURE AND MODEL\", \"paragraph\": [\"E-mail is a valuable tool for communicating with other users. Email allows the sender to efficiently forward millions of advertisements at no cost. Unfortunately, this scheme is now being used in a variety of organizations. As a result, a massive amount of redundant emails is known as spam or junk mail, many people are confused about the emails in their E-Mailboxes. Each learning sequence is given forward as well as backward to two different LSTM networks that are attached to the same outputs layer in order for bidirectional Lstms to function. This indicates that the Bi-LSTM has detailed sequential information about all points before and following each point in a specific sequence. In other words, we concatenate the outputs from both the forward and the backward LSTM at each time step rather than just encoding the sequence in the forward direction. Each word\'s encoded form now comprehends the words that come before and after it. This is a problem for the Internet community. The diagram depicts various stages that aid in the prediction of email spam:\", \"Because real-world data is messy and contains unnecessary information and duplication, data preprocessing is critical in natural language processing (NLP). The major preprocessing steps are depicted below.\"]}, {\"header\": \"NLP Tokenization\", \"paragraph\": [\"Tokenization of documents into words follows predefined rules. The tokenization step is carried out in Python with spacy library.\"]}, {\"header\": \"Stop Words Removal\", \"paragraph\": [\"Stop words appear infrequently or frequently in the document, but they are less significant in terms of importance. As a result, these are removed to improve data processing.\"]}, {\"header\": \"Text Normalization\", \"paragraph\": [\"A word\'s lexicon form or order may differ. Thus, they must all be changed to their root word to be correctly analyzed. Lemmatization and stemming are the two methods that can be used for normalization. When a word\'s final few characters are removed to create a shorter form, even if that form has no meaning, the procedure is known as stemming. lemmatization [21] is a mixture of corpusbased an rule-based methods, and it retains the context of a term while changing it back to its root.\"]}, {\"header\": \"Feature Extraction\", \"paragraph\": [\"feature extraction which transforms the initial text into its features so that it may be used for modeling after being cleaned up and normalized. Before predicting them, we use a specific way to give weights to specific terms in our document. While it is simple for a computer to process numbers, we choose to represent individual words numerically. In such cases, we choose word embeddings. IDF is the count of documents containing the term divided by the total number of documents, and occurrence is the amount of instances a word appears in a document. We derive characteristics based on equations. 1,2,3,4,5, and 6. We use equations to derive properties.\", \"\\ud835\\udc47 \\ud835\\udc53 \\ud835\\udc3c\\ud835\\udc51 \\ud835\\udc53 (\\ud835\\udc61, \\ud835\\udc51, \\ud835\\udc37) = \\ud835\\udc47 \\ud835\\udc53 (\\ud835\\udc61, \\ud835\\udc51).\\ud835\\udc3c\\ud835\\udc51 \\ud835\\udc53 (\\ud835\\udc61, \\ud835\\udc37)\", \"A word2vec neural network-based approach is the method that is utilized for this goal as the tool. The following equation, referred to as 5, shows how word2vec handles word context through the use of probability-accurate measurements. Here letter D stands for the paired-wise display of a set of words, while the letters w and c0 or c1 represent paired word context that originated from a larger collection of set D.\"]}, {\"header\": \"Word-Embeddings\", \"paragraph\": [\"Word-Embedding helps to improve on the typical \\\"bag-of-words\\\" worldview, which requires a massive sparse feature vector to score every word individually to represent this same entire vocabulary. This perception is sparse because the vocabulary is large, and each word or document is defined by a massive vector. Using a word map-based dictionary, word embedding needs to be converted terms (words) into real value feature vectors. There are two basic issues with standard feature engineering techniques for deep learning. Data is represented using sparse vectors, and the second is that some of the meanings of words are not taken into consideration. Similar phrases will have values in embedding vectors that are almost real-valued. The Input length in our proposed study is set to 700 for our suggested model. If the texts seemed to be integer encoded with value systems between 10 and 20, the vocabulary distance would be 11. Our data is encoded as integers, and the input and output dimensions are both set to 50,000. The embedding layer outcome will be used in successive layers and for BiLSTM and GRU layers.\"]}, {\"header\": \"Machine Learning Model\", \"paragraph\": [\"Within the scope of the research, we are using the subsequent machine learning techniques, to examine and compare the overall efficacy of our suggested Bi-LSTM strategy: Support Vector Machine, Gaussian NB, Logistic Regression, K -nearest neighbors, and Random Forest (RF).\"]}, {\"header\": \"Convolution Network\", \"paragraph\": [\"The popular RNN model generally performs well but takes too long to train the model incorporating the textual sequential data. When a layer is added after the RNN layer, the model\'s learning duration is considerably decreased. Higher-level feature extraction is another benefit. [19] additionally possible using the convolutional layer. In essence, the convolution layer looks for combinations of the various words or paragraphs in the document that involve the filters. We use features with 128 dimensions and a size 10 for each. For this task, the Relu activation function is utilized. After that, the one-dimensional largest pooling layers with a pooling size of 4 are put on the data in order to obtain higher-level features.\"]}, {\"header\": \"BiLSTM Network with GRU\", \"paragraph\": [\"Recurrent Neural Network (RNN) technique of text sentiment analysis is particularly well-liked and frequently applied. Recurrent neural networks (RNN) surpass conventional neural networks. because it can remember the information from earlier time steps thanks to its memory. A state vector is combined with an RNN\'s data to create a new state vector. The resulting state vector uses the present to recollect past knowledge. The RNN is straightforward and is based on the following equations:\", \")\", \"The vanilla RNN [18]is not very good at remembering previous sequences. In addition to that, RNN struggles with diminishing gradient descent. A kind of RNN is a long short-term recall network (LSTM), solves a vanishing gradient descent problem and learns long-term dependencies [10]. LSTM was actually created to address the problem of long-term reliance. LSTM has the unique ability to recall. The cell state is the LSTM model\'s central concept. With only a small amount of linear interaction, the cell state follows the sequence essentially unmodified from beginning to end. gate of an LSTM is also significant. Under the command of these gates, information is safely inserted to or eliminated from the cell stated.\", \"The following equations are used by the LSTM model to update each cell:\", \"In this case, Xt denotes input, and ht is the hidden state at the t time step. The following is the revised cell state Ct:\", \"Here, we may compute the output and hidden state at t time steps using the point-wise multiplication operator *.\", \"Due to the reality it only considers all prior contexts from the present one, LSTM does have a few drawbacks. As a result of this, it may accept data from preceding time steps through LSTM as well as RNN. Therefore, in order to avoid this issue, further improvements are carried out with the help of a bidirectional recurrent neural network(Bi-RNN). BiRNN [13] can handle two pieces of information from both the front and the back. Bi-LSTM is created by combining the Bi-RNN and LSTM. As a result, operating LSTM has advantages such as cell state storage so that BiRNN have way to acknowledge from the context before and after. As a consequence of this, it provides the Bi-LSTM with the advantages of an LSTM with feedback for the next layer. Remembering long-term dependencies is a significant new benefit of Bi-LSTM. The output, which is a feature vector, will be based on the call state. Finally, we forecast the probability of email content as Normal, Fraudulent, Harassment, and Suspicious Emails using as an input to the softmax activation function, which is a weighted sum of the dense layer\'s outputs. To regulate the information flow, GRU employs the point-wise multiplying function and logistic sigmoid activation.\", \"The GRU has hidden states of storage memory and does not have distinct memory cells or units for state control. The W, U, and b vectors, which stand for weights, gates, and biases, respectively, are crucial variables that must be calculated during the creation of the GRU model. For training reasons, the pre-trained word embedding known as the Glove vector is used. They made it clear that GRU is the superior model when there is a large amount of training data for textual groups and word embedding is available. BiLSTM, CNN, and GRU is required so as to compensate for the deletion of the document\'s long-term and short-term connections. In our case, the embedding dimension, maximum sequence length, and lexicon size were used to start the LSTM embedding layer in three separate LSTM models. The input vector was modified to make it appropriate for such a Conv1D layer, prior situations\' sequences are returned by LSTM layer. The \\\"return sequences\\\" of the LSTM layer must be set to False when the subsequent state is free of the gated architecture. Quantity of learning parameters must be taken into consideration. A 350-unit LSTM layer was set -up, and different LSTM unit combinations were tested. More importantly, because it has more parts, the model made with BiLSTM will take longer to train. Bidirectional LSTM is the name of a particular kind of recurrent neural network that is primarily used for the processing of natural languages. (BiLSTM). It is able to use data from both sides, and, in contrast to regular LSTM, it enables input flow in both directions. It is an effective instrument for demonstrating the logical relationships between words and phrases, and this involves both the forward and backward directions of the sequence. In conclusion, BiLSTM works by adding one extra layer of LSTM, causing the information flow to travel in the other direction. It only denotes that the input sequence runs in reverse at the next LSTM layer. Multiple operations, including averaging, summation, multiplication, and concatenation, are then applied to the results of the two LSTM layers. The gated design of Bi-LSTM and GRU networks solves the disappearing gradient and exploding problems. A good way to handle more long sequences is to use Bi-LSMT and GRU together. GRU works well with datasets that don\'t have text. In two to three rounds, the complicated CNN+BiLSTM+GRU model learns the long sequence of email text well. We have used word embedding, cnn, bidirectional lstm and gru networks as our three building blocks to separate email messages based on their sentiment and text\'s sequential features. Also, we succinctly demonstrate below why these blocks help identify email spam:\", \"\\u2022 First, We have used the Sequence -to -sequence Lstm as the current block in the networks since it can retrieve both the previous and next sequences from the current. More so than a straightforward LSTM network, it can also recognize and extract text sentiment and sequential properties. \\u2022 Second, we extract the more complex and advanced characteristics for Bi-LSTM network using Convolutional Network block, which is the network\'s second block after the Bi-LSTM block. Bi-LSTM takes a long time to extract text-based features, hence one of the reasons for using this block is to reduce the network\'s overall training time.\"]}, {\"header\": \"EXPERIMENTAL EVALUATION 3.1 Experimental Setup\", \"paragraph\": [\"We divided the information into training and testing groups of 80/20. We divided the remaining 20% of the 80 percent training data into test data for the model. Construct, compute, and evaluate the efficacy of the suggested method using the Pythonic packages Keras, as TensorFlow and Scikit learn.\"]}, {\"header\": \"Dataset Description\", \"paragraph\": [\"Email spam detection is the foundation of this research project. The dataset includes normal emails from the Enron corpora, deceptive emails from phished email corpora, harassment emails chosen from hate speech, and the offensive dataset. Only the content of the email body is used for analysis; all header information, including sender, topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word Embedding are used to extract characteristics from the email message and classify them. This dataset [8] is publicly available. The presented model is implemented using Python, and several metrics, including accuracy, precision, and recall, are used to examine the outcomes.\"]}, {\"header\": \"Evaluation Metrics and Results\", \"paragraph\": [\"Classifier performance is assessed Using metrics such as accuracy, precision, and recall. Four terms make up a confusion matrix that is used to calculate these metrics.\", \"\\u2022 True positives (TP) are positive values that have been accurately assigned the positive label.  1 where six different classifiers are Gaussian NB, Random Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid Model (CNN+BiLSTM+GRU) have been used in this work. In the CNN, Bi-LSTM, and GRU architectures which enable sequence prediction, CNN strands for feature extraction on data input which are combined with LSTM. It requires less time training and a higher expandable model. Any bottlenecks are created by predictions and the increasing number of distinct units of information. This model is useful for dealing with issue-related classifications that consist of two or more than two classes. So suggested Ensemble model, out of these six classifiers, produces more accurate findings.    In this Proposed ensemble hybrid model\'s train accuracy is 98.7% Validation accuracy is 97.32% and LSTM has train accuracy of 97.41% and validation accuracy is 95.2%. So based on figures 3 and 5 indicate the validation loss for LSTM and the proposed ensemble hybrid model to be 0.93 and 0.84, respectively, and figures 2 and 4 show the validation accuracy to be 95.2% and 97.3%, respectively. LSTM and the proposed hybrid model used ensemble artificial intelligence, with the proposed hybrid model outperforming the LSTM. We decide on dense architecture as the final model for identifying the text messages as spam or nonspam based on loss, accuracy, and the aforementioned charts. The loss and accuracy over epochs are more stable than LSTM, and the Proposed classifier has a straightforward structure.\"]}, {\"header\": \"CONCLUSION\", \"paragraph\": [\"The model is composed of four networks Word-Embeddings, CNN, Bi-LSTM, and GRU. We may train the model more quickly by using the convolutional layer first, followed by the word-embedding layer, and then the BiLSTM network. The Bidirectional LSTM network also has higher-level properties that we can extract. We have used a bidirectional LSTM(BiLSTM)and GRU network to memorize a sentence\'s contextual meaning and sequential structure, which improves the model\'s performance accuracy to roughly 97.32 percent.\"]}]','https://drive.google.com/uc?id=1sKhfxo8dxlBi6WTaYynnGojQZv2eOWEW&export=download','2023-02-03',0,'2024-02-03 22:08:24.578889','2024-02-03 22:08:24.750054'),(5,'Towards a Quantum Software Modeling Language','We set down the principles behind a modeling language for quantum software. We present a minimal set of extensions to the wellknown Unified Modeling Language (UML) that allows it to effectively model quantum software. These extensions are separate and independent of UML as a whole. As such they can be used to extend any other software modeling language, or as a basis for a completely new language. We argue that these extensions are both necessary and sufficient to model, abstractly, any piece of quantum software. Finally, we provide a small set of examples that showcase the effectiveness of the extension set.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Quantum computation rose to prominence after the discovery of quantum algorithms [5,7] that can efficiently perform tasks that are intractable classically. These discoveries propelled research and interest in quantum computation. Today, there exists prototype quantum hardware with computational capabilities beyond that of any classical machine [1]. Further applications of quantum theory to computation have also been made in several areas of theory of computing, such as models of computation [6], data structures [8], and cryptography [2].\", \"Quantum computation has, until today, been studied almost exclusively \'in the small.\' A general understanding of quantum computation, or, quantum programming \'in the large\' is yet to be developed. Here we aim to set the foundations of a general framework for studying, developing, and conveying quantum programs. We aim to do so by developing a universal modeling language for quantum software. Rather than develop such a language from scratch, we have decided to start from the well-known Unified Modeling Language (UML) [3], and introduce a minimum set of extensions that allow it to effectively model quantum software.\", \"Assuming UML to be a shared common-language upon which we can build, allows us to convey our original extensions much more succinctly. Our extension set can, however, be applied with little or no modification to any other modeling language.\"]}, {\"header\": \"Q-UML\", \"paragraph\": [\"Before discussing in depth the extensions we are introducing, we make a few fundamental observations on which we base the guiding principles for our extension set.\", \"Our first observation is about the nature of quantum computation. The central difference between quantum and classical computation is in how it achieves its goals. Quantum computers have access to quantum algorithms [7], and quantum data-structures [8], that are unavailable to classical computers-hence their performance advantage. Algorithms and data-structures are, however, implementation details. Algorithms are an essential design choice while programming in the small. However, they are more often than not completely ignored in large-scale software architectural design. For instance, UML diagrams seldom portray algorithms and data-structures beyond a very high-level design perspective.\", \"It would seem then that quantum computation introduces nothing to computation that needs to be captured in a software design diagram. This is not the case, and the reason for this is our second observation. Quantum computation changes the very nature of information itself. Quantum information is much richer than classical information. It is also much more challenging to store, transmit, and receive. If a module (class, object, etc.) needs to store, transmit or receive quantum information, then this is an important design consideration-which needs to be included in any effective software design.\", \"A third observation here is that the classical vs. quantum nature of the information used by a module is an important consideration both when discussing its internal implementation and its interface. Furthermore, these two are separate and independent considerations.\", \"A classical module, implementing some classical behavior, would have no need, or capability, to communicate quantum data. A quantum module may or may not have to; i.e. a module\'s quantum behavior may be completely part of its internal implementation and not appear as part of its interface. For instance, take a module implementing Shor\'s algorithm. Shor\'s algorithm uses quantum effects to efficiently factor a large integer into its prime factors. The implementation of this module must necessarily be quantum. Both the input (the large integer) and the output (the prime factors), consist of classical information. And hence, the interface of such a module can be strictly classical.\", \"More generally, we can conceive of quantum software modules that have all classical inputs and outputs (like the above example), all quantum inputs and outputs, or a mix of both. A quantum software design must address, for each individual interface element, whether it is classical input/output, or if it is quantum. In short, whether a module communicates classically or via quantum information, and whether its internal implementation requires quantum hardware are important considerations that need to be captured in a design document.\", \"The importance of such labelling should be clear. Quantum data can only be stored and transmitted with special hardware designed to do so. More importantly, from an abstract, device-independent, strictly software perspective: quantum and classical information are not interchangeable. Classical information is clone-able and admits fanout operations, while quantum information (in general) does not. On the other hand, quantum information has a much larger state-space.\", \"Finally, it is true that quantum information is strictly a super-set of classical information-and hence a quantum module can communicate any classical information it desires using a quantum interface element. We argue, however, that using a quantum interface element and messaging when classical would suffice is bad quantum software design, for the reasons stated above.\", \"In summary, the guiding principles behind any quantum software modeling language must include the following:\", \"(1) (Quantum Classes): Whenever a software module makes use of quantum information, either as part of its internal state/implementation, or as part of its interface, this must be clearly established in a design document. (3) (Quantum Supremacy): A module that has at least one quantum element is to be considered a quantum software module, otherwise it is a classical module. Quantum and classical modules should be clearly labelled as such. (4) (Quantum Aggregation): Any module that is composed of one or more quantum modules will itself be considered a quantum module, and must be labelled as such.\", \"(5) (Quantum Communication): Quantum and classical modules can communicate with each other as long as their interfaces are compatible, i.e. the quantum module has classical inputs and/or outputs that can interface with the classical module. We will argue in Sec. 2.3 how these extensions are not only necessary, but also sufficient in order to design and represent quantum software. First, in the following two sections we put these principles into practice as a set of concrete extensions to UML.\"]}, {\"header\": \"Class Diagram Extensions\", \"paragraph\": [\"UML is a very graphical language, meant to convey a lot of meaning in a very small amount of space. As such, it makes sense to use a graphical way to represent quantum software elements. We chose to do this by use of bold text to denote quantum elements, and double lines to denote a quantum relationship or quantum communication. For attributes, the name will be bold if it is represented using quantum information. For methods, we use the following convention. If any of the inputs are quantum, these are bold. If the output or datatype of the method is quantum, then the datatype should also be bold. For backwards compatibility with regular UML, whenever the input or output datatypes of a method are omitted, these will be assumed to be classical in nature. If a class/object has any quantum attributes or methods then it itself is considered quantum, and its name shall also be bold.\", \"Relationships between classes will use double-lines whenever the relationship is quantum in nature. For inheritance, if the superclass is quantum then the subclass, and the inheritance relationship, will also be quantum. (the converse is not necessarily true however). In the case of aggregation and composition, if a class/object being aggregated/composed is quantum, then the class/object to which it is aggregated/composed into, as well as that relationship will also be quantum. Association relationships do not have any special rules, beyond the need of a quantum class/object to have a classical interface if it is to associate with classical classes/objects. Fig. 1 showcases a Q-UML diagram that exemplifies the above rules.\"]}, {\"header\": \"Sequence Diagram Extensions\", \"paragraph\": [\"Sequence diagrams in UML allow us to portray the dynamic relationship between modules in a software program. As we did before for static relationships, we extend the existing language in order to allow us to differentiate between classical and quantum messages. As previously discussed, this is essential information. Quantum information behaves differently from classical information; it can store/portray different data; it admits different operations; and, it requires different hardware to store, send, and receive. Like before, we make use of bold text to markup quantum modules, and double lines to portray quantum messages. Fig. 2 shows a Q-UML sequence diagram. Note how even though the relationship between Shorfactor and ShorOrder is quantum, the messaging between them is not. This illustrates an important point. A module is marked as quantum if it uses quantum resources in any form, either directly as part of its internal implementation or as part of an aggregated module. If a sub-module (in UML a composed class or object) is quantum, then the encompassing module must also be marked as quantum. In a static (e.g. class) diagram, the quantum composition relationships inform us-especially in the case of a seemingly classical module that does not in itself use quantum resources-which composed modules are using quantum resources.\", \"Also, note the communication between the objects ShorOrder and QFT_n. The module QFT_n operates on a quantum state. Hence, both \'set\' messages are quantum. Likewise, the return messages \\u03c1 and \\u03c1 are quantum states. However, the request to perform a quantum Fourier transform (QFT) or a QFT inverse operation can (and therefore should) be communicated classically. This diagram showcases the level of granularity available to us using these diagrams with the proposed extensions.\"]}, {\"header\": \"Discussion\", \"paragraph\": [\"We have proposed a minimal series of extensions to existing software modeling languages. We exemplify our additions in UML, but these extensions are easily applicable to any other modeling language, or be used as the basis for a new modeling language.\", \"We\'ve argued the necessity of each of the extensions in previous sections. We can argue as well, that these extensions are not only necessary, but also sufficient to fully model quantum software. To make this argument, we appeal to the fact that all quantum computation is simulable using classical computation albeit with an efficiency loss. Other than their use of quantum information and algorithms, quantum computers are indistinct from classical ones. Hence, from a high-level design perspective, the only information element that needs to be considered when developing quantum software is when quantum (rather than classical) information is being used.\", \"The one remaining information element we have not discussed is algorithm efficiency. If quantum computation is to be used, it will most likely be due to the efficient algorithms at its disposal. That said, algorithm efficiency is not a solely quantum consideration. UML itself does not inherently have language elements for algorithm efficiency (beyond user-defined notes). It does, however, have several extensions used and proposed for this purpose(see e.g. [4]). Other modeling languages may also have definite algorithm efficiency elements. We argue that it is best to use existing language elements when they are available.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"CP-D would like to acknowledge funding through the EPSRC Quantum Communications Hub (EP/T001011/1). The authors would also like to thank Joanna I. Ziembicka for useful comments during the preparation on this manuscript.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"CP-D would like to acknowledge funding through the EPSRC Quantum Communications Hub (EP/T001011/1). The authors would also like to thank Joanna I. Ziembicka for useful comments during the preparation on this manuscript.\"]}]','https://drive.google.com/uc?id=1fOkCCKTH7pSkuxxXGt-8C0maPBTxlJy1&export=download','2020-02-03',0,'2024-02-03 22:08:27.457534','2024-02-03 22:08:27.610836'),(6,'The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development','Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model\'s responses. We developed a prototype system -the Programmer\'s Assistant -in order to explore the utility of conversational interactions grounded in code, as well as software engineers\' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant\'s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Software development is a highly skilled task that requires knowledge, focus, and creativity [27,28]. Many techniques have been developed to enhance the productivity of software engineers, such as advanced code repositories [86], knowledge repositories [39], Q&A sites [1], and pair programming practices [18]. Collaborative software engineering is especially promising, given that professional software development is rarely a solo activity and relevant knowledge and expertise are typically distributed widely within an organization [68]. Many efforts have focused on incorporating collaborative technologies into software development environments (e.g. [8,25,26,58,101]).\", \"The pioneering work of Rich and Waters on The Programmer\'s Apprentice [70] presented a novel concept of a knowledgeable automated assistant -in effect, an artificial collaborative partner -that could help software engineers with writing code, designing software systems, and creating requirements specifications. At the time, AI technologies and computing resources were not sufficient to fully implement their vision. In the intervening years, an increase in computational power, the availability of large corpora of language and code data, and the development of deep neural networks have made new approaches to achieving their goals worth exploring.\", \"Recently, models leveraging the transformer architecture [96] have been developed to perform domain-specific software engineering tasks, such as translating code between languages [75], generating documentation for code [36,38,97,98], and generating unit tests for code [92] (see Talamadupula [90] and Allamanis et al. [5] for surveys). Recently developed foundation models -large language models that can be adapted to multiple tasks and which exhibit emergent behaviors for which they have not been explicitly trained [14] -have also proven to be capable with source code.\", \"While the intent of training LLMs such as GPT-2 [64] and GPT-3 [17] was to give them mastery of natural language, it quickly became apparent that the presence of code in their training corpora had given them the ability to generate code based on natural language descriptions [49]. The Codex model [24] was then produced by finetuning GPT-3 on a large corpus of source code data, leading to the development of Copilot [32], a tool that helps software engineers by autocompleting code as it is being written. Experimentation with Copilot has shown its ability to perform additional tasks, such as explaining code, generating documentation, and translating code between languages [6].\", \"Although autocompletion interfaces are useful and valuable when the system can discern the developer\'s intent, there are many instances where that is insufficient. For example, the developer may have a good idea of what they want to do, but may be unclear on what functions, libraries, or even algorithms to employ. They may even have general programming questions that need to be answered before they are able to write any code.\", \"In this paper, we seek to understand whether modern developments in code-fluent foundation models -large language models that have been fine-tuned on source code data -are sufficient to support a conversational agent that can act as an assistant in the software development process. We developed the Programmer\'s Assistant to explore the capabilities that conversational interaction could enable and the extent to which users would find conversational assistance with programming tasks desirable and useful.\", \"We hypothesize that a conversational system may provide a flexible and natural means for interacting with a code-fluent LLM. Conversational interaction could enable users to pursue their questions in a multiple exchange dialog (as observed by Barke et al. [13]) that allows them to ask follow-up questions and refine their inquiries. A conversational programming assistant could ask the user clarifying or disambiguating questions to help it arrive at the best answer. It could also provide multiple types of assistance to the user beyond simply generating code snippets, such as engaging in general discussion of programming topics (e.g. [22,71]) or helping users improve their programming skills (as observed in other studies of automating technologies [99]).\", \"Our paper makes the following contributions to the IUI community:\", \"\\u2022 We provide empirical evidence that a conversational programming assistant based on a state-of-the-art, code-fluent foundation model provides valuable assistance to software engineers in a myriad of ways: by answering general programming questions, by generating context-relevant code, by enabling the model to exhibit emergent behaviors, and by enabling users to ask follow-up questions that depend upon their conversational and code contexts. \\u2022 We show how different interaction models -conversation, direct manipulation, and search -provide complementary types of support to software engineers with tradeoffs between the user\'s focus and attention, the relevance of support to their code context, the provenance of that support, and their ability to ask follow-up questions.\", \"\\u2022 We motivate the need to further understand how to design human-centered AI systems that enhance the joint performance of the human-AI collaboration.\"]}, {\"header\": \"RELATED WORK\", \"paragraph\": [\"We discuss three areas of related work that have either motivated our study of conversational programming assistance or provided the technical foundations for it. We begin by briefly summarizing Rich and Waters\' visionary work on the Programmer\'s Apprentice [70], followed by summarizing work on code-fluent foundation models and human-centered evaluations of how these models impact software engineers\' work. Finally, we discuss conversational interaction and how it might be employed to provide more flexible and sophisticated assistance to software engineers.\"]}, {\"header\": \"The Programmer\'s Apprentice\", \"paragraph\": [\"Our work is inspired by the vision laid out by Rich and Waters [70], which describes an artificial agent that can act as an intelligent assistant for software engineers by providing advice, catching errors, and handling routine details throughout the software development process. The Programmer\'s Apprentice [70] relied on a knowledge base of \\\"clich\\u00e9s,\\\" which are formal, structured versions of what are known today as software design patterns [31]. It used a hybrid reasoning system capable of special-purpose reasoning based on frames and a plan calculus, along with general purpose logical reasoning. Although natural language interaction was envisioned, the original prototype implementation ultimately used a stylized command language. We view our work as a conceptual successor to the Programmer\'s Apprentice, as it enables the natural language interaction that the Programmer\'s Apprentice lacked.\"]}, {\"header\": \"Code-fluent Foundation Models and Human-Centered Evaluations of Programming Assistance\", \"paragraph\": [\"Generative models based on the transformer architecture [96] have recently been applied to the domain of software engineering. Codefluent large language models are capable of generating code from natural language descriptions [105], translating code from one language to another [75], generating unit tests [92], and even generating documentation for code [36,38,97,98]. These models are probabilistic systems, and as such, do not always produce perfect results (e.g. code that is free of syntax or logical errors). Nonetheless, Weisz et al. [102] found that software engineers are still interested in using such models in their work, and that the imperfect outputs of these models can even help them produce higher-quality code via human-AI collaboration [103]. New tools based on code-fluent LLMs are actively being developed. GitHub Copilot1 is described as \\\"Your AI pair programmer. \\\" It is optimized for the code autocompletion use case: given a starting snippet such as a method\'s documentation, signature, or partial implementation, Copilot completes the implementation. Copilot is based on the OpenAI Codex model [24], a 12 billion parameter version of GPT-3 [17,49], fine-tuned on code samples from 54 million public software repositories on GitHub. Empirical evaluations of this model have shown that, although the quality of its outputs is quite good, those outputs may still be problematic [57]. Echoing the results from Weisz et al. [103], human-centered evaluations of Copilot have found that it increases users\' feelings of productivity [109], and that almost a third (27%) of its proposed code completions were accepted by users. In a contrasting evaluation, Vaithilingam et al. [95] found that while most participants expressed a preference to use Copilot in their daily work, it did not necessarily improve their task completion times or success rates. Yet, in a study by Kalliamvakou [40], developers working with Copilot were able to implement a web server in Javascript 55% faster than developers who did not use Copilot.\", \"A grounded theory analysis of how programmers interact with Copilot [13] found that their interactions varied depending upon whether they were accelerating tasks that they already knew how to do or if they were exploring solutions to problems that they were less sure about. Autocompletion was effective when developers were operating in \\\"acceleration mode\\\" and relied on the model to produce short completions that could be verified quickly. In \\\"exploration mode,\\\" however, the interaction was more awkward. Developers would communicate with Copilot by typing comments and seeing what Copilot generated in response. Then, they would modify their comments to explore other ways of prompting a response. Ultimately, the comments used to prompt the model would be deleted after the relevant code was generated, indicating that their value was largely in driving a back-and-forth, yet context free, dialog with the model to coerce it to produce the desired results through an iterative refinement process. In this paper, we fully commit to a context-aware conversational style of interaction with a code-fluent LLM and assess the value it provides to users.\"]}, {\"header\": \"Conversational Interaction and Analysis\", \"paragraph\": []}, {\"header\": \"Conversational Interaction.\", \"paragraph\": [\"Using natural language to interact with technology has had a long research history [2], starting in the 1960s with pattern-matching approaches like Eliza [104], and continuing to today with state-of-the-art large language modelbased conversational systems [107] such as Meena [3] and Blender-Bot [84]. These systems are intended to address the problem of open-domain dialog, with a goal of realistically engaging in conversation, but not particularly in a goal-directed or task-oriented manner.\", \"Task-oriented chatbots are typically built with frameworks such as the Microsoft Bot Framework2 , Google DialogFlow3 , and IBM Watson Assistant4 . They operate using pre-defined dialogue trees and use natural language processing to detect conversational intents and extract contextual entities. This structure enables the creation of special purpose, but fairly limited and rigid, conversational agents.\", \"There have been several recent attempts to investigate conversational programming assistance. Kuttal et al. [42] conducted a Wizard of Oz study in which a pair programmer was replaced with a conversational agent, and they found that \\\"agents can act as effective pair programming partners.\\\" The PACT system [106] is a chatbot that assists programmers adjusting to new programming environments. PACT is structured as a discrete question-answering system based on a neural machine translation approach, but it doesn\'t maintain a conversational context.\"]}, {\"header\": \"Conversation Analysis.\", \"paragraph\": [\"Conversation is a form of interaction between people that enables robust communication. Conversation Analysis [76] is a method for understanding the natural structure of human conversational interaction. It catalogs different patterns of conversational acts and how they are utilized by interlocutors in order to attain a wide variety of goals. Recently, Conversation Analysis has been adapted to describe patterns of interactions between humans and artificial conversational agents in order to aid in the design of chatbots [50]. We apply techniques from Conversation Analysis in our study of conversational programming assistance.\"]}, {\"header\": \"THE PROGRAMMER\'S ASSISTANT\", \"paragraph\": [\"In order to explore conversational programming assistance, we created a functional prototype system called The Programmer\'s Assistant. Our prototype, shown in Figure 1, combines a code editor with a chat interface. The code editor was implemented using the Microsoft Monaco Editor5 embedded in a React wrapper 6 . The chat user interface was implemented using the React-Chatbot-Kit7 framework. To drive the conversational interaction, we employed OpenAI\'s Codex model [24], accessed through its web API.\", \"We developed our prototype as a lightweight coding environment in order to examine the user experience of interacting with a conversational assistant. Our work was exploratory in nature, and thus we did not have specific design goals for the prototype beyond integrating a code editor with a code-fluent LLM. We also did not attempt to target the prototype for a specific class of users (e.g. novices or experts) or use cases (e.g. writing code vs. learning a new programming language), as we wanted any value provided by conversational assistance to emerge from our user study. We also did not implement the ability to run or debug code in our prototype as we wanted to explore the nature of the conversational interaction rather than having users focus extensively on the production of working code.\", \"When designing how users would interact with the Programmer\'s Assistant, we decided that it should be available on demand and not monitor the user\'s work in progress or give unsolicited suggestions or advice, in keeping with the conversational agent interaction model proposed by Ross et al. [73,74]. This approach was supported by feedback from prospective users who were concerned about the assistant providing criticism of unfinished efforts in progress or distracting them while they worked. Instead, we force initiative onto the user and only have the assistant respond to their requests. In this way, the assistant can provide help when requested without undesirable interruptions that can distract or interfere with the user\'s flow.\", \"When a user interacts with the assistant, we keep track of their selection state in the code editor. If a user sends a message to the assistant without any code selected in the editor, then that message (along with the prior conversational context) is passed directly to the model. If a user sends a message to the assistant with new code selected in the editor (i.e. code that wasn\'t previously selected when they sent their last message), then that code is appended to the message before being communicated to the model.\", \"The model may produce multiple types of responses to a user\'s message. We treat each type of response differently in the UI.\", \"\\u2022 Responses that do not contain code are always rendered in the chat UI (Figure 1E). \\u2022 Responses containing short code snippets (\\u2264 10 lines) are rendered inline in the chat UI (Figure 1G). \\u2022 Responses containing longer code snippets (> 10 lines) show the code in a pop-up window (Figure 2A), with a proxy entry in the chat transcript (Figure 2B) that allows users to redisplay the code window after it has been closed. Non-code text in the response remains in the chat transcript. The assistant never directly modifies the contents of the user\'s source code; rather, any code the user desires to transfer from the chat takes place via copy/paste.\", \"Figure 1 shows a screenshot of a real, sample conversation, in which the user asks a question that results in an inline response, then requests an explanation of some code in the editor, and then requests further elaboration. Figure 2 shows an example conversation that resulted in the generation of a longer code sample, shown in a popup window. This example shows how the assistant produced an incomplete solution, followed by criticism from the user regarding the missing code, and resulting in an apology and the generation of a complete solution.\"]}, {\"header\": \"Supporting Conversational Interaction\", \"paragraph\": [\"We enabled Codex to conduct a conversational interaction by prompting it with a conversational transcript and a request to produce the next conversational turn. The prompt establishes a pattern of conversation between a user and a programming assistant named Socrates. It provides several examples of Socrates responding to general coding questions, generating code in response to a request, and accepting code as input. It establishes a convention for delimiting code in the conversation, making it easy to parse for display in the UI. It also establishes an interaction style for the assistant, directing it to be polite, eager, helpful, and humble, and to present its responses in a non-authoritative manner 8 . Because of the possibility that the model might produce erroneous answers or incorrect code (as discussed in Weisz et al. [102]), we felt it was important that the assistant convey a sense of uncertainty to encourage users to not accept its results uncritically to avoid over-reliance (e.g. as observed in Moroz et al.\'s study of Copilot [51], and discussed more generally in Ashktorab et al. [9]) as well as automation bias [45,46,65]. We present the full text of the prompt used for the assistant in Appendix D.\"]}, {\"header\": \"Architecture & UI Design\", \"paragraph\": [\"The Programmer\'s Assistant communicates with the Codex API via a proxy server that forwards requests from the React client. The proxy also rate-limits access to conform to the API\'s policy, and it logs UI events from the client (e.g. requests, responses, and UI interactions) in a back-end database. To address inconsistencies in the style or formatting of code generated by Codex, the proxy server reformats all code segments using the Black code formatter 9 before transmitting them to the client UI.\", \"The client maintains the transcript of the ongoing conversation. Each time the user sends a message in the chat, the client constructs a new prompt for the model by concatenating the initial prompt, the chat transcript, and the user\'s new utterance, and makes a request for the model to complete the transcript. This completion request also specifies a stop sequence of tokens to prevent the model from generating both sides of the conversation (e.g. what the model thinks the user\'s next utterance might be after the assistant\'s response). Given the API\'s limitation on context length (4,096 tokens for both the prompt and model response), we silently \\\"forget\\\" older exchanges in the chat transcript when constructing the prompt to ensure that our completion request remains within bounds. Nonetheless, the entire conversational history remains visible to the user in the UI.\", \"The client UI provides a loose coupling between the source code editor and the chat interface. Users can hide the chat pane when they wish to focus solely on their code, and re-engage with it when they desire assistance. Code selected in the editor is included in the conversation in order to couple the code context with the conversation. Easily-accessible buttons are provided in the UI to copy code responses from the assistant to the clipboard.\"]}, {\"header\": \"Handling Model Limitations\", \"paragraph\": [\"While developing the Programmer\'s Assistant, and in early pilot testing, we experienced some quirks and shortcomings of the model and our approach to using it for conversational interaction. One limitation stemmed from the fact that the model sometimes produced incorrect responses (e.g. code with syntax errors), incomplete responses (e.g. code that was missing functionality), irrelevant responses (e.g. responses not related to the user\'s question), or insubstantial responses (e.g. \\\"I don\'t know\\\"). Because of the probabilistic nature of model inference, re-prompting the model would sometimes produce a more correct or appropriate response. Thus, we added the ability for users to \\\"try again, \\\" either by asking in the chat or by clicking a button in the UI (Figure 1C). This feature removes the assistant\'s last response from the context presented to the model and then re-invokes the model with an increased temperature 10 .\", \"Although it is possible for transformer models such as Codex to produce multiple possible responses to a single prompt, we only request a single response in order to speed up response time as well as to preserve the token budget for conversational context. Thus, the \\\"try again\\\" feature provides an alternate way to produce a wider variety of responses.\", \"During pilot testing, we noticed that the assistant sometimes happened to generate the same response to multiple, unrelated requests. In these cases, the assistant tended to get \\\"stuck\\\" in a pattern of repeating the same response and was unable to resume normal conversation. To avoid this problem, we automatically execute a The \\\"try again\\\" button (C) allows users to ask the assistant to generate an alternate response to the most recent question. The \\\"start over\\\" button (D) resets the conversational context for the assistant, but maintains the chat transcript in the UI. In this example, we show the assistant introduce itself to the user (E). Next, the user asks a general programming question (F), for which the assistant provides an inline code response (G). The user then asks a question about code selected in the editor (H), followed by a series of follow-up questions.\", \"\\\"try again\\\" operation in the background when we see identical consecutive responses from the assistant. Finally, we noticed that the accumulation of conversational context sometimes resulted in the assistant becoming fixated on some portion of the earlier conversation. For example, it might respond to a question with portions of the prompt or of earlier conversation, and become less responsive to newer requests. To address this issue, we introduced a \\\"start over\\\" feature, accessible via the chat or by clicking a button in the UI (Figure 1D), that resets the context to the original prompt, forgetting the rest of the conversational history. We preserve the chat transcript in the UI, but delineate the break in the assistant\'s memory with an annotation in the chat transcript. These annotations are added both for \\\"try again\\\" and \\\"start over. \\\"\"]}, {\"header\": \"Sample Conversation\", \"paragraph\": [\"We provide a real sample conversation with the Programmer\'s Assistant in Listing 1. This conversation begins with the assistant greeting the user (line 1). Next, the user asks a general Python programming question (line 4), to which the assistant responds with a non-authoritative remark (\\\"I think...\\\") and a code snippet (line 9). The user next asks a follow-up question that depends on their previous question and the assistant\'s response (line 11), to which the assistant provides another code snippet (line 15), satisfying the user\'s request.\", \"The user then switches topics and asks the assistant to write a Fibonacci function (line 17), and the assistant again responds with a non-authoritative remark (\\\"I will give it a try, \\\" line 20) and a block of code. The user then asks how the function works (line 30) and the assistant provides an adequate description (line 32). Next, the user asks the assistant to re-implement the function in a different way (line 37), again leveraging the ability to ask follow-up questions. The assistant produces an alternative implementation that conforms to the user\'s request (line 41). The user follows up with a question that depends on multiple past utterances and responses in the chat transcript (line 47), and the assistant produces a relevant response (line 49). The conversation closes with the user thanking the assistant (line 53) and the assistant acknowledging their gratitude (line 55).\", \"Listing 1: A conversation with the Programmer\'s Assistant. Code presented by the assistant is listed in bold face. To address our questions, we deployed the Programmer\'s Assistant within our organization -a global technology company -and invited people to try it out and give us feedback on their experience. We invited people with varying levels of programming skill in order to obtain a wide range of feedback on the kinds of use cases for which the tool could provide assistance.\"]}, {\"header\": \"Tasks\", \"paragraph\": [\"We set up the Programmer\'s Assistant as a playground environment that participants could try out with a few sample programming problems. We created a tutorial to orient participants to the assistant, its capabilities, and how to interact with it. We also created four programming challenges focused on writing code, documenting code, and writing tests for code. We designed these challenges to expose participants to a broad range of the assistant\'s capabilities. For each of these challenges, we explicitly did not evaluate metrics such as the participant\'s productivity, the quality of their solutions, or the time taken to produce them, as the focus of our study was to understand the utility of conversational interaction. We selected Python as the language used for the tutorial and challenges because of its general popularity [21] and the fact that it was well-supported by our underlying LLM [24].\", \"4.1.1 Tutorial. All participants were first introduced to the Programmer\'s Assistant through a tutorial. The tutorial walked each participant through 10 sample interactions to give them a feeling for what the assistant could do and how to interact with it. The tutorial demonstrated how to ask questions, how to request code to be generated, and how to evaluate existing code. It did not specifically cover how to generate documentation or unit tests. Tutorial instructions were provided within the code editor. We include the specific text used for the tutorial in Appendix B.\", \"4.1.2 Programming Challenges. After completing the tutorial, participants unlocked four programming challenges. Two of the challenges involved coding problems (writing a queue class and writing code to create a scatterplot of data in a CSV file), one involved documenting a given function (an implementation of a graph search algorithm), and one involved writing unit tests for a given function (computing the greatest common divisor of two arguments). Although the Programmer\'s Assistant was visible and available for use, we provided no specific requirement that it actually be used to complete the challenges.\", \"After participants completed their solution to a challenge, they submitted it by clicking a button in the UI. The code editor used in the Programmer\'s Assistant was not a fully-functional IDE and did not provide syntax checking or the ability to run, test, or debug code. Due to these limitations, participants were asked to submit their solutions when they felt they had completed the challenge to their own satisfaction.\"]}, {\"header\": \"Participants\", \"paragraph\": [\"To recruit participants for our study, we posted internal advertisements in various communications channels focused on software engineering. Our advertisements stated that we were evaluating a conversational programming assistant, but were kept deliberately vague in order to minimize the impact on peoples\' expectations of the experience.\", \"Our advertisement yielded a pool of 140 potential participants. In order to recruit a diverse sample, we used a screening survey that asked about their job role, their familiarity with and recency of use of Python, and their availability to participate in our study. We accepted participants into the study on a rolling basis, selecting participants to capture a range of programming experiences and ensure balanced gender representation. We conducted periodic reviews to determine whether we were learning something new from each participant or if we had reached the point of saturation [7]. We stopped collecting data after running 42 participants as we were no longer observing any new behaviors or gleaning any new insights. The Programmer\'s Assistant implementation and configuration were held constant over the course of the study; no changes to the UI design or LLM prompt were made.\", \"Our participants had the following self-identified characteristics: \\u2022 Recency of Python Use: 29 participants had written Python code within the past month, 4 within the past year, 5 within the past 5 years, and 4 had not written Python code within the past 5 years.\", \"We provide full demographic information for individual participants in Appendix E.\"]}, {\"header\": \"Procedure\", \"paragraph\": [\"Participants completed the study on their own time, independently and without moderation. Each participant was provided with a web link to a pre-study survey that described the nature of the study and the tasks that they would be expected to perform. They were then directed to the Programmer\'s Assistant to complete the tutorial and the four programming challenges. When participants indicated they were finished with the challenges 12 , they were directed to a final post-study survey. Complete sessions generally required about an hour of effort, though some participants spread their effort across a longer period of time and across multiple sessions. Participants were compensated for their time at a rate equivalent to US $15/hr.\"]}, {\"header\": \"Measures\", \"paragraph\": [\"We collected a variety of data in our study from three sources:\", \"(1) Surveys. We employed three surveys in the study: a prestudy survey to collect demographic information, a pre-task survey to gauge expectations of the conversational user experience, and a post-task survey to assess actual user experience. We describe these survey questions in the relevant context of our results, and we provide a complete listing of all survey instruments in Appendix A. (2) Event logs. The Programmer\'s Assistant was instrumented to collect data on participants\' usage. The event logs provided timestamped records of interaction events, including conversational exchanges, hiding/showing the assistant, use of the \\\"try again\\\" and \\\"start over\\\" features, and use of copy/paste. (3) Conversation logs. From the event logs, we extracted conversational transcripts between each participant and the Programmer\'s Assistant.\"]}, {\"header\": \"RESULTS\", \"paragraph\": []}, {\"header\": \"Data & Analysis\", \"paragraph\": [\"We collected a wealth of data in our study: 126 survey responses from three surveys per participant, containing 296 written comments in open-ended survey questions, and 4,877 instances of 23 different types of UI events, including 1,699 conversational exchanges 13 in the event logs. We also compute, for each participant, counts or durations for 21 different metrics from the event logs.\", \"In our analysis, we deliberately exclude the portion of our data collected during the tutorial exercise. We exclude this data because that activity was guided by the tutorial instructions, not by our participants\' own initiative. Thus, our final sample consists of 3,172 events, including 968 conversational exchanges in the event logs; no survey data was excluded.\", \"Our primary analysis of this data is qualitative, as our participants provided us with a rich source of interesting feedback and thought-provoking insights in their comments. Where applicable, we supplement this data with quantitative data from the survey and the event logs, as well as chat transcript data from the conversation logs. In this way, we triangulate [47] across our three data sources, using the open-ended survey data as a foundation. When we quote participants, either from their qualitative survey responses or the conversational transcripts, we reproduce their words exactly as typed, including typos, misspellings, grammatical errors, capitalization, and potential trigger words, and we only make minor clarifying edits where needed, delineated by square brackets.\", \"In order to set the context for our analysis, we first describe how we used reflexive thematic analysis to analyze participants\' responses to the open-ended survey questions. We then describe our analysis of the conversation logs and our development of a coding guide based on Conversation Analysis [76], and specifically, Moore and Arar\'s Natural Conversation Framework [50].\"]}, {\"header\": \"Thematic Analysis of Qualitative Survey\", \"paragraph\": [\"Responses. We conducted a reflexive thematic analysis to analyze the responses to our seven open-ended survey questions. We followed the process described by Braun and Clarke [16] in which researchers immerse themselves in the data, generate codes for material that seems interesting, and then iteratively group and refine codes through collaborative discussion in order to identify higher-level themes. Initially, four authors performed open-coding on the open-ended survey responses. Through discussion, these codes were grouped and consolidated into a single set, which were then re-applied to the data by two authors. After another round of discussion, these authors identified a set of 12 higher-level themes. Some themes had clear parallels to quantitative survey questions or event log data, and thus represented clear instances where we were able to triangulate across data sources. Other themes surprised us. We structure our presentation of the results based on these 12 themes, grouped into three different aspects of the user experience: expectations and experience, utility of conversational assistance, and patterns of interaction and mental models.\"]}, {\"header\": \"Conversation Analysis via the Natural Conversation Framework.\", \"paragraph\": [\"In order to understand the content and structure of the conversations that took place between our participants and the Programmer\'s Assistant, we turned to the Natural Conversation Framework [50] (NCF). We developed a codebook for the event logs, beginning with 21 different categories of utterances from the NCF. Nine NCF categories -Acknowledgment, Apology, Confirmation, Expression of Gratitude, Farewell, Greeting, Self-Identification, Welfare Check, and Welfare Report -appeared twice in our codebook to distinguish cases in which the utterance was made by the human participant vs. the assistant. Other NCF categories were split to provide nuanced detail about the interaction; for example, we distinguished three different kinds of NCF requests, depending upon whether they were stated as Requests for Action (e.g. codes to identify meta-information such as utterances that included code, utterances that referenced selected code, utterances that implicitly or explicitly referenced earlier portions of the conversation, or non-verbal UI activities such as copies, pastes, and invocations of \\\"try again\\\" and \\\"start over. \\\" Finally, we classified a subset of the human-applied codes based on whether they represented a participant\'s task or social orientation toward the assistant. We list our codes in Table 1, but note that not all of them ended up being relevant to our analysis. When coding conversational data, we applied individual codes at the level of each conversational utterance. We allowed multiple codes to be applied to each utterance to account for utterances that performed multiple functions (e.g. greeting and self-identification). In order to ensure consistency in how our codebook was applied, two authors coded a 10% sample of the 968 conversational exchanges, achieving a satisfactory level of inter-rater reliability (Krippendorf\'s \\ud835\\udefc = 0.77, where agreement was conservatively defined as having all of the same codes applied to both utterances in a conversational exchange).\"]}, {\"header\": \"Expectations and Experience\", \"paragraph\": [\"Pilot testing of the Programmer\'s Assistant suggested that software engineers would be skeptical of a conversational programming assistant and its ability to provide useful assistance. Our study revealed that, for most participants, their actual experience after using the tool was better than they had anticipated. Participants were surprised at the quality of the assistant\'s responses and they appreciated how its integration with the code editor reduced the amount of context switching they needed to do in the UI. Some participants struggled with the code selection feature, although others appreciated the ability to ask questions related to selected code.\"]}, {\"header\": \"Usage.\", \"paragraph\": [\"All of our participants engaged with the Programmer\'s Assistant while working on the challenges, despite there being no requirement to do so. Forty-one participants submitted solutions to all four challenges, and one participant, P14, only submitted solutions for one of the four challenges. Participants spent an average of 68 minutes engaged with the assistant, as measured by the amount of time the Programmer\'s Assistant window was in focus.\", \"Participants made an average of 23.0 utterances (SD = 15.1 utterances) to the assistant. On average, 6.2 of their utterances (SD = 4.3 utterances) contained a code selection. The average latency per request14 was 6.7 seconds (SD = 3.1 seconds).\", \"We saw a 66.3% rate of acceptance of generated code, where we considered code to be accepted if the participant performed a copy immediately after the code was generated. This acceptance rate is much higher than the 27% acceptance rate reported for Copilot [109]. We believe one reason we observed a higher acceptance rate is because Copilot\'s completion suggestions are generated proactively, whereas the Programmer\'s Assistant\'s suggestions are generated upon request. When copying generated code from the assistant, participants most often copied the entirety of the generated code, and only in 5.8% of cases did they copy a smaller portion of it.\"]}, {\"header\": \"User Experience Expectations & Changed Attitudes.\", \"paragraph\": [\"Prior to running our study, we had reason to believe that participants would be skeptical of a conversational programming assistant. Before developing the Programmer\'s Assistant, we showed potential users mockups of a program editor with an integrated chatbot feature. These prototypes elicited uniformly negative reactions. People told us about their frustrating experiences with conventional chatbots and raised doubts about the knowledge, capabilities, and value of a conversational programming assistant. This skepticism motivated us to develop the Programmer\'s Assistant in order to evaluate whether the conversational experience, as powered by a state-ofthe-art code-fluent LLM, would be better than people had anticipated. During pilot testing, we received feedback that the Programmer\'s Assistant provided a much better conversational experience compared to testers\' previous experiences with chatbots. Thus, in designing our study, we felt it important to first gauge participants\' expectations of a conversational interaction around code, and then measure their experience after the fact.\", \"We developed a short inventory of six scale items to measure user experience of code work 15 . The scale was administered twice: once before participants were exposed to the Programmer\'s Assistant (but after they had been briefed that they would interact with an AI chatbot), and once after completing the programming challenges. The items were presented with the appropriate tense: Do you expect (Did you find that) the Programmer\'s Assistant: (a) will be (was) easy to use; (b) will understand (understood) your requests; (c) will provide (provided) high quality responses; (d) will help (helped) you to write better code; (e) will help (helped) you to write code more quickly; (f) will be (was) enjoyable to use. Each item was rated on a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3), A great deal (4).\", \"A factor analysis revealed the items on this scale measured a single construct, which we identify as user experience (Cronbach\'s \\ud835\\udefc = 0.87). Thus, we computed two scores of user experience (UX) for each participant: a pre-task UX score computed as the average of their six pre-task expectation scale responses, and a post-task UX score computed as the average of their six post-task experience scale responses.\", \"We found that participants had lower initial expectations for their experience with a conversational programming assistant (pretask UX M (SD) = 3.0 (0.62) of 4) than their experience actually was (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test shows that this difference was significant, \\ud835\\udc61 (41) = 5.94, \\ud835\\udc5d < .001, Cohen\'s \\ud835\\udc51 = 0.92 (large). Measured another way, 32 participants (76.2%) had post-task UX ratings that were higher than their pretask expectations, demonstrating a significant shift in attitudes toward conversational programming assistance.\", \"However, the UX ratings alone fail to capture participants\' nuanced expectations of the assistant and the reasons for their shifted attitudes after using it. Participants expressed a variety of expectations of the assistant before using it, including that it would be easy to use (P30) and produce correct responses (P30), understand the problem and what is being asked of it (P8, P9, P11), not interfere with their flow state (P5), produce imperfect or questionable outputs (P6, P21), improve with feedback (P31), provide generic and unhelpful answers (P17) or only answer basic questions (P40), and produce responses quickly (P40).\", \"P17 expected \\\"to be frustrated very quickly and that what I\'d think would be relatively common questions would be responded to with generic, unhelpful answers.\\\" P6 explained, \\\"I didn\'t have very good experiences with chatbots. I think I\'ll need to spend more time in reviewing and fixing the suggestions than in writing the code myself from scratch. \\\" P11 had a more balanced view, that \\\"It\'ll do some tasks really well, but others will not be as reliable. \\\"\", \"After interacting with the Programmer\'s Assistant, many participants commented on how the experience was better than they anticipated, because it \\\"seemed to be able to handle complex issues\\\" (P10) and \\\"was a great help\\\" (P8). P20 felt it was \\\"incredible!\\\" P6 and P17, who were both initially skeptical, reported having a positive experience. For P6, \\\"It absolutely exceeded all my expectations, in all aspects that I could have imagined and more!\\\" P17 provided a more quantitative assessment: \\\"Initial expectations: 3 Actual: 9.5.\\\" P38 was emphatic in their evaluation: \\\"I was blown away how well it allowing me to structure how I want the code to look and work and just giving me the thing I asked for. \\\"\", \"Many participants described a sense of surprise in their experiences. P9 was surprised by how well it understood their requests:\", \"\\\" Participants also reported experiencing this variability in the quality of the assistant\'s responses. Some participants described how the assistant provided \\\"detailed answers\\\" (P17) and \\\"high quality outputs\\\" (P18) that were \\\"surprisingly good\\\" (P2). P6 felt it was \\\"incredible to see the quality of the responses, \\\" and P3 even explored the assistant\'s capabilities outside the scope of the challenges and found that it could handle those as well:\", \"\\\"It was surprising the quality of the code and the ability to answer all my questions correctly. Although I think the challenges may be biased towards what the Assistant is able to do, it was a great experience because I asked many other things and it was able to answer correctly. \\\" (P3)\", \"Of course, the Programmer\'s Assistant wasn\'t perfect, and some participants did run into issues. For P35, \\\"The documentation generation did not perform very well. \\\" P16 questioned the accuracy of the knowledge encoded in the model: \\\"Does the model need to be updated? It said latest python version is 3.7 but google says it\'s 3.10. \\\" In some instances, participants needed to ask their question multiple times to get a good response: \\\"you need to ask many times if you want to get an answer and also a detailed explanation\\\" (P3). P27 felt, \\\"it was annoying when I asked it to try again and it would give me the same response. \\\" P22 struggled because, \\\"It didn\'t seem to handle multiple sentences well. \\\"\", \"P28 perhaps offered the most scathing criticism, that, \\\"It makes mistakes often enough to be not very practical.\\\" However, despite the production of poorer-quality responses, other participants felt that the assistant was still helpful. P36 reported that, \\\"Only minor tweaks were normally needed to correct any issues.\\\" Similarly, P38 described how the assistant wasn\'t able to completely solve their problem, but provided a useful start:\", \"\\\"There was only one hickup I noticed where when I asked it to memoize fibonacci it couldn\'t, but it dropped the building blocks on my lap for me to finish so that was fine, that was like minutes of effort on my part.\\\" (P38) 5.2.4 UI Design & Affordances. Participants made many comments on our specific UI design and the affordances provided (or not provided) in our chat-augmented editor. Overall, the integration between the chat pane and the code editor was \\\"very good\\\" (P23), with a \\\"nice interface between the code pane and the assistant pane\\\" (P17) that \\\"makes it really convenient\\\" (P35).\", \"Prior research by Brandt et al. [15] has shown how keeping developers focused in their IDE improves productivity, and our participants expressed similar sentiments. P40 remarked, \\\"It allows me to stay in one browser window/tab!\\\" and P12 hinted at how the interface might preserve their flow state by \\\"prevent[ing] me from getting distracted when looking into an issue in another tab. \\\" Some aspects of our user interface were confusing to participants, such as the mechanism for selecting code to be included in the conversational context. P7 remarked, \\\"It\'s was a little confusing doing the selection part for it to tell me what a function does, but... it gave me code that was insanely easy to copy and paste.\\\" Other participants appreciated the code selection mechanism, such as P11: \\\"I enjoyed the code selection feature, and found that very easy to use. \\\"\", \"In the event logs, we identified 20 instances in which a participant unintentionally included selected code in the conversation when it wasn\'t needed (Includes Extraneous Selection), 12 instances in which a code selection was omitted when it was needed to provide context for the question (Missing Selection), and 16 instances in which a participant copy/pasted code directly into the chat rather than selecting it in the editor (Pasted Code in Chat). Although these cases represent a small fraction of the 227 instances in which a code selection was required and included in the conversation (Includes Selection), their presence does indicate that more attention is needed to the interaction design of code selection.\", \"Another issue regarded the awareness of the \\\"try again\\\" and \\\"start over\\\" features. The \\\"try again\\\" feature was only used by 14 participants, who used it a total of 63 times over the course of the study. Some participants used it specifically when they got an answer which they saw as clearly wrong, while others used it to get a variety of possible answers before proceeding. The \\\"start over\\\" feature was used even less, by 5 participants who used it a total of 6 times. Despite our effort to surface these conversational features in the UI via shortcut buttons, they may not have been sufficiently noticeable or salient: \\\"The \'try again\' button is not so reachable, often times I forgot it exists\\\" (P23). By contrast, at least one participant was successful with these features:\", \"\\\"at some point it had issue with challenge 3 and I had to start over. Just asking \'try again\' was not enough and I was getting always the same (wrong and not related) answer. starting again solved the issue!\\\" (P20)\"]}, {\"header\": \"Utility of Conversational Assistance\", \"paragraph\": [\"Our next set of themes concerns the utility provided by conversational programming assistance. Participants felt the assistant was highly valuable and desired to use it in their own work. They felt it would be most helpful for smaller or narrowly-scoped tasks, but able to provide a wide variety of types of assistance. The fact that the interaction model was conversational and grounded in code were valuable aspects, as was the ability for the assistant to bolster users\' learning about programming topics through that interaction. Participants did question whether they could trust and rely upon the assistant\'s responses, echoing a similar theme discussed in Weisz et al. [102].\"]}, {\"header\": \"Value & Appropriate\", \"paragraph\": [\"Tasks. Participants rated the value of the Programmer\'s Assistant highly (M (SD) = 8.6 (1.4) of 10). Many participants asked questions such as, \\\"Can I have it in my editor please?\\\" (P15), or made comments that, \\\"I would enjoy using it in the future\\\" (P36), \\\"I would love to be able to... have access to it for my coding\\\" (P37), and \\\"I\'d love to use this tool as part of my usual programming workflow if I could!\\\" (P39). Some of the reasons why participants found it valuable are because it \\\"help[s] me remember how to do things in certain languages that normally I would just Google\\\" (P9) and \\\"It helps me to avoid silly syntax errors and can when I cannot remember exact function/method names and required arguments\\\" (P40). We did not observe any differences in value ratings based on participants\' familiarity with or recency of using Python.\", \"Participants described a wide variety of tasks for which they felt the assistant would be useful. These tasks included \\\"ordinary\\\" (P23), \\\"simpler\\\" (P2), and \\\"small, repetitive\\\" (P4) tasks such as \\\"quick lookups\\\" (P25) for \\\"short chunks of code\\\" (P11) or for \\\"narrowed questions\\\" (P26). Participants also felt the assistant was useful for \\\"small containable novel algorithms\\\" (P38) and \\\"little coding problems\\\" (P4).\", \"Several kinds of task assistance were reported as being valuable, such as explaining code (P31), implementing business logic in a UI (P38), understanding what code does (P19, P37), and recalling language syntax, method names, and arguments (P12, P15, P20, P40, P42). P27 felt that the assistant was \\\"More helpful when recognizing a specific well known algorithm but not things you make yourself. \\\"\", \"Participants also made recommendations for how to increase the value of the Programmer\'s Assistant. P38 suggested, \\\"What would blow me away though is if it\'s able to help with what I do most often which is to integrate, refactor and iterate on an existing system. \\\" P16, P26, and P38 all desired more information on the data sources used to produce the assistant\'s responses. P9 requested to \\\"Have the Programmer\'s Assistant examine your code and make proactive suggestions for improving it in the chat.\\\" P36 requested the same, but cautioned that, \\\"Care would need to be taken to avoid becoming an annoyance or disrupting the flow of a coding session. \\\"\", \"In the post-task survey, we probed participants on how certain changes to the Programmer\'s Assistant would either decrease, increase, or result in no change to its value. Over 75% of participants felt that the assistant would be more valuable if it operated in a proactive manner, either by making improvement suggestions in the chat or as comments directly in the code. Similarly, 78.6% of participants felt that having more buttons in the UI for common features such as explaining or documenting code would make the tool more valuable.\"]}, {\"header\": \"Conversational\", \"paragraph\": [\"Interactions Grounded in Code. One of the challenges in interpreting participants\' comments about the utility of the Programmer\'s Assistant was in disentangling the extent to which value was derived from the quality of the underlying model versus the integration of conversation in a code context. Indeed, participants felt that the chat interaction was valuable: 69.0% of participants felt that eliminating the conversational interaction and making the assistant behave more like web search would decrease its value. Further, our analysis of the conversation transcripts revealed that 42% of the 910 task-oriented utterances from participants required historical conversational context (Chat Context Required) in order to be correctly interpreted. Thus, we observe that behaviorally, participants did rely on conversational context in their interactions.\", \"In the post-task survey, 83% of participants rated the importance of the ability to ask follow-up questions as being \\\"somewhat\\\" or \\\"a great deal. \\\" Several participants specifically commented on the value of this conversational context. P39 remarked, \\\"I absolutely loved how you can straight up ask follow-up questions to the Programmers\' Assistant without having to reiterate the original topic/question. \\\" P15 expressed a similar sentiment, saying, \\\"I think the conversational context was someone helpful, just in communicating that it\'s a running conversation where my context is remembered. \\\" P9 provided a similar analysis:\", \"\\\"This tool was so helpful at answering questions I had about the code in the context of the code I am working on... I was also impressed with how well it was able to remember the context of our conversation, especially when I asked vague follow-up questions. \\\" (P9)\", \"In addition, some participants identified how a conversational interaction grounded in code was useful, \\\"because I think to \'understand\' the dev context could be VERY important\\\" (P31). In fact, 24.9% of task-oriented utterances included a relevant code selection (Includes Selection), showing that participants valued this ability.\", \"Contrasting with these participants, P18 felt that interacting with the assistant conversationally was tedious, and they employed a more direct approach: \\\"I really like the PA. But, I didn\'t converse with it like a chat bot. I often told it what to do (\'Document this code.\') as opposed to asking it what to do (\'How do I document this code?\'). Talking to it the way that was suggested in the tutorial seemed overly verbose/tedious. \\\" (P18) Despite these individual differences in interaction preferences, P39 envisioned that both interaction styles could be supported in the tool:\", \"\\\"I think both options should exist: people should be able to input their queries like a search bar AND also give their question as if in conversation. \\\" (P39) 5.3.3 Learning Effects. One specific benefit of the Programmer\'s Assistant identified by participants is its ability to help people improve their programming skills and reinforce knowledge gaps. For example, it can help users \\\"remember how to do things in certain languages... such as, when I am using a language I haven\'t used in a while\\\" (P9). The assistant can also serve as an memory aid, such as when \\\"I use a lot of libraries that I don\'t always remember all of the functions\\\" (P15). Similarly, P31 said, \\\"No matter how good you\'re as a developer, you can\'t (humanly) remember all the API of hundreds of libs or new languages... I\'d learn new dev lang and new lib/frameworks faster. \\\"\", \"P39 felt the assistant \\\"is perfect for programmers of all levels, \\\" and P1 felt it could help them rapidly improve their Python skills: \\\"I have wanted to learn python... The main concern how much time spent learning is needed before I could actually get some value out of learning python. I have a feeling this would cut that time down from weeks to a day or so. \\\" (P1) P39 also identified the fact that, because the interactions with the assistant are conversational, it forces people to learn how to communicate to others about their code: \\\"The conversation aspect promotes proper communication, which would really stand to benefit budding programmers if they want to learn how to explain concepts more fluently in the future to their colleagues.\\\" (P39) Conversely, P36 suggested that over-reliance on programming assistance might have a detrimental effect to one\'s learning: \\\"It\'s definitely a huge time saver, but over-reliance on it may cause new developers to skip learning the reference material themselves and discovering new things and sparking new ideas. \\\" (P36) 5.3.4 Trust. Many participants raised questions about whether they could trust the responses provided by the Programmer\'s Assistant. P21 asked this question most directly: \\\"will the code be correct, safe, efficient?\\\" Other participants raised similar questions, such as, \\\"I\'m wondering how it validates it\'s answers, if it can be trusted to always give a working answer\\\" (P10), and \\\"Sometimes lack of source and context may raise doubts in the mind of the programmer\\\" (P16).\", \"These issues of trust were exacerbated by the fact that the Programmer\'s Assistant did not allow participants to actually run their code. Because of this limitation, participants had to rely on their own knowledge to judge the correctness of the assistant\'s responses. P19 asserted, \\\"There is no way to evaluate if the Programmer\'s assistant is giving you the right advise or not other than your own knowledge, \\\" and P9 concurred: \\\"I had to trust that it was correct (and use my own prior knowledge). \\\"\", \"P18 described the potential consequences of allowing the assistant to write code for them:\", \"\\\"The only thing that made me nervous was that it could have introduced a bug that wasn\'t immediately apparent. And given I didn\'t write the code, I could have easily glossed over a mistake when reviewing it. Especially if it is also the one writing the test cases. \\\" (P18)\", \"Despite our efforts to make the Programmer\'s Assistant respond in non-authoritative ways, we did observe participants sometimes uncritically accept generated results that were clearly wrong or incomplete. Thus, we did find behavioral evidence for over-reliance.\", \"Listing 2: Building trust through explanations and justifications One way to address trust issues is for the assistant to provide further explanations and justifications that can calibrate a user\'s confidence in the assistant\'s responses. Such explanations could be requested conversationally, though most participants did not attempt to do so. One participant (P9) did ask for such explanations, and we show a summary of their transcript in Listing 2. In this instance, P9 asked for a definition of a unit test (line 1), an explanation of the code being tested (line 25), and justifications of the quality of the unit test (lines 31& 37). Thus, we observe that the assistant is capable of producing explanations and justifications when asked.\"]}, {\"header\": \"Patterns of Interaction and Mental Models\", \"paragraph\": [\"Participants interacted with the assistant in a variety of ways with two main patterns of usage standing out: (1) invoking the assistant to solve the entire programming challenge, and (2) breaking the challenge down into a set of smaller tasks and invoking the assistant\'s help for each. There were no clear differences in how participants with differing Python experience approached the tasks.\", \"Participants\' mental models of the assistant also varied. Although participants strongly saw the role of the assistant as being a tool, their behaviors revealed that in many cases, they actually treated it as a social agent. In addition, participants ascribed various mental capacities to the assistant, such as having the ability to understand, compute, and learn.\", \"Participants felt the assistant changed the nature of their work process. For some participants, it enabled them to focus on the higher-level aspects of development because the assistant handled lower-level details or provided partial solutions for them to build upon. Many participants felt the assistant sped up their work and helped them remain focused on their tasks.\", \"Finally, participants drew comparisons between the Programmer\'s Assistant with other forms of programming support such as Copilot and web search. They felt that the conversational style of interaction enabled them to discover new, emergent behaviors from the model that were unavailable from Copilot\'s focus on code autocompletion. They also felt that the examples provided by the assistant were more readily usable within their own code compared to browsing for answers within search results, speeding up the coding process. However, some participants advocated for a balanced approach to the design of programming assistance tools by incorporating multiple modes of interaction rather than fixating on a single one.\"]}, {\"header\": \"Interaction\", \"paragraph\": [\"Styles and Assistant Role. We observed that participants interacted with the Programmer\'s Assistant in strikingly different ways. Some participants would present the entire challenge description to the assistant and then work with the results it produced. Other participants approached the programming challenges in a piecemeal fashion, breaking them apart into a set of smaller tasks, then invoking the assistant to aid with each one.\", \"Experience with Python was not a determinant of how participants approached the programming challenges, but it did seem to impact how participants interacted with the assistant. Less experienced participants tended to ask the assistant basic questions such as, \\\"What is a unit test\\\" (P29, not familiar with Python) and \\\"how do I document a function?\\\" (P27, < 1 year of experience). More experienced participants made detailed requests about specific Python libraries or algorithms, such as, \\\"given a pandas dataframe with two columns \'Date\' and \'Sales\' please use matplotlib to draw me a scatterplot\\\" (P38, 3+ years of experience) and \\\"implement a rungekutta algorithm for solving an ODE with adaptive time steps\\\" (P37, 3+ years of experience).\", \"Another difference we observed in how people interacted with the assistant stemmed from their view on the role it played in their collaborative process. Some participants, such as P18, treated it more as a tool by issuing commands rather than asking questions. As quoted earlier, they said, \\\"I didn\'t converse with it like a chat bot. \\\" P5 described their interaction style similarly: \\\"I found myself wanting to type search queries into Socrates, not treating it as a person but as a search tool. \\\"\", \"In anticipation that participants would have different orientations to the assistant and its role, we asked a question on the posttask survey about the different kinds of roles the assistant might take. These roles generally fell into one of two categories: a tool orientation (a tool, a reference guide, a content generator, a problem solver), and a social orientation (a collaborator, a colleague, a coach, an advisor, a reviewer). Participants rated the extent to which they viewed the Programmer\'s Assistant in each of these roles on a 4point scale of extent: Not at all (1), A little (2), Somewhat (3), or A great deal (4).  We show participants\' ratings of the assistant\'s role in Figure 3. Despite the fact that their attitudes toward the assistant overwhelmingly reflected a tool orientation, their behaviors reveal that many participants actually treated the assistant as a social agent. P6 described how \\\"I felt it like a partner,\\\" and P4 told the assistant, \\\"I could not have solved [the challenge] without your help,\\\" to which the assistant responded, \\\"I\'m glad I could help. \\\"\", \"The literature on Computers as Social Agents (CASA) helps us interpret this result as it demonstrates how computers are often treated like people [56,67]. LLM-based conversational agents can exacerbate this tendency; as they likely have been trained on examples of social interaction, they can also respond as social agents.\", \"In the conversation logs, we identified participants who interacted with the assistant in a socially-oriented fashion (the social orientation codes in Table 1). Twenty participants (47.6%) made at least one socially-oriented utterance. An extreme form of this interaction style can be seen in a snippet from P6\'s transcript (Listing 3).\", \"The 20 participants with a behaviorally-demonstrated social orientation did not generally differ in their role ratings from other participants, except that they rated the assistant as more likely to be an advisor (Fisher\'s exact test, two-tailed \\ud835\\udc5d = .02) or a reviewer (Fisher\'s exact test, two-tailed \\ud835\\udc5d = .03). However, they did not differ in their overwhelmingly-strong ratings of the tool orientations. Thus, at least for some participants, there seems to be a dissonance in their view of the assistant\'s role orientation.\", \"Listing 3: Excerpt from P6\'s interaction with the Programmer\'s Assistant, in which P6 offers their thanks and congratulations. Socrates : Goodbye .\"]}, {\"header\": \"Mental\", \"paragraph\": [\"Capacities. Participants made a number of inferences about the Programmer\'s Assistant and its capacities for thought.\", \"Many participants talked about how the assistant possessed a level of \\\"understanding\\\" (P6, P8, P11, P18, P32) of \\\"the context\\\" (P9, P21) as well as \\\"major concepts\\\" (P9) and \\\"knowledge\\\" (P33). P24 was amazed by the assistant\'s ability to \\\"take a plain english request and interpret it properly.\\\" P7 ascribed intelligence to the assistant, saying, \\\"It was a lot smarter and trained tha[n] I thought it was. \\\" One participant assumed that the assistant \\\"Keeps improving through (user) feedback\\\" (P31). Another felt that the assistant was capable of computation: \\\"It understands the problem... It can calculate the results of a function back\\\" (P8).\", \"However, not all participants were convinced of the assistant\'s ability to understand. P37 questioned the assistant\'s limitations: \\\"I wonder how far beyond boilerplate it can go and if it works for truly original problems. \\\"\"]}, {\"header\": \"Impact of Conversational\", \"paragraph\": [\"Assistance on Work Practices. Many participants discussed how the Programmer\'s Assistant shaped their work practices on the programming challenges. Overall, participants felt that the assistant \\\"saves time\\\" (P10), \\\"helps me code faster\\\" (P34), and would \\\"speed up my productivity\\\" (P19) because \\\"I could focus on validating and improving the code it generated instead of having to write it all from scratch\\\" (P18). P37 remarked that, \\\"It opens a whole new door for fast develpment. \\\" P4 discussed how the assistant \\\"was helpful in staying focused on the code, \\\" although for P14, \\\"it took [me] time to get into tempo with the tool. \\\"\", \"P31 pointed out how the assistant would change the nature of their work: \\\"My job could focus more on higher level aspects and therefore achieving better (quality) results, besides the time-to-value... Data science (and dev) becomes a more creative-higher level experience. \\\" (P31) Other participants discussed a work process in which the assistant provided incomplete solutions -the \\\"building blocks\\\" (P38) or \\\"initial draft of code\\\" (P11) -upon which they could build. P5 aptly described this process: \\\"It\'s nice to copy well formulated challenges in natural language and have the code generator take its best stab at it, then edit to our hearts content. \\\" (P5)\", \"Participants felt that human review of the assistant\'s responses was necessary because \\\"The answers provided are generally not novel solutions, often look clunky and non-elegant. There may be some unnecessary code. Basically the code would need to be reviewed\\\" (P16). P35 also pointed out how \\\"The code generator was good but you still have to really check it. \\\" P19 discussed how they would turn to the assistant as a first source for support, and only if it wasn\'t able to help would they then turn to other support tools:\", \"\\\"The way I will use it is, I will first us[e] the Programmer\'s assistant for most of my cases. Only in certain cases where Programmer\'s assistant cant answer things I will turn up to official documentation or stack overflow. \\\"\", \"However, latency was a factor for interactive use of the assistant and participants noticed when the assistant took a long time to respond. P19 remarked, \\\"Sometimes it took lot of time, like more than 5 seconds. \\\" P40 also felt \\\"the response [was] a little slow sometimes... in chat mode I expect faster responses. \\\" As discussed in Section 5.2.1, the assistant took an average of 6.7 seconds (SD = 3.1 seconds) to respond to a request, and participants did appreciate when the assistant produced rapid responses: \\\"I loved how quick it was able to pull up answers to questions I had\\\" (P38).\"]}, {\"header\": \"Conversational Interaction vs. Other Interaction Models.\", \"paragraph\": [\"Although our study was not intended to make comparative evaluations with the Copilot tool, we nonetheless asked participants whether they were familiar with Copilot, and if so, to comment on how the two tools compared. We also asked a similar question to compare the assistant with another popular form of programming assistance, searching the web (via a search engine like Google, or a Q&A site like Stack Overflow). In discussing the differences between these three tools, we note that the primary differentiator is their interaction model.\", \"The interaction model for the Programmer\'s Assistant is clearly conversational: users ask questions in natural language and are provided with a response in natural language and/or code. The interaction model of Copilot is reminiscent of direct manipulation interfaces [37], in which the user\'s actions in the user interface directly manipulate an object on the screen. Copilot automatically makes autocompletion suggestions as the user types. This autocompleted code is directly placed in the source editor; thus, the user\'s work is contained entirely within the scope of the object on which they are working (i.e. the source code), which is how direct manipulation interfaces operate. In web search, users enter a separate search context (e.g. a search engine accessed within a web browser), type in a natural language query, and then forage amongst search results to identify relevant items of interest [12,62].\", \"When a desirable item is found, users must translate it into their code environment (e.g. via copy/paste) and possibly edit it to fit their existing code.\", \"We also note that the Programmer\'s Assistant and Copilot both utilize the same underlying AI model, Codex [24], which means that the only difference between these tools is the user experience. The extent to which Codex was trained on data from programmingrelated Q&A web sites is less clear, but for the purposes of our analysis, we focus our discussion solely on the differences in their interaction models 16 .\", \"Participants reported various benefits and drawbacks of a conversational interaction over a direct manipulation interaction. Foremost, conversation \\\"felt very natural\\\" (P21) and \\\"feels much more natural using Natural Language with the AI\\\" (P39). In addition, P39 felt that \\\"the use cases of Programmers\' Assistant seem more openended. \\\" Many participants were surprised at the variety of tasks the assistant was capable of performing, from writing unit tests (P19, P36, P37) and documentation (P12, P19, P36, P37) to explaining what code did (P31, P38) and even answering general-knowledge questions (P31). Again, we note that the Programmer\'s Assistant utilizes the same underlying model as Copilot, yet the conversational interface was able to expose a wider variety of emergent behaviors from the model. Multiple participants explored the limits of the assistant\'s knowledge and abilities beyond our programming challenges. For example, P37 asked it questions about physics and ordinary differential equations (\\\"ODe\\\" as written by P37), and was surprised by the \\\"versatility of what it could answer. \\\" \\\"I asked it some physics and ODe question and the answers, though not complete, included the key parts needed to write that code. \\\" (P37) P31 probed the assistant on its knowledge of geography and was surprised when the assistant produced a correct answer.\", \"\\\"I asked something out of SW engineering domain (geography) and it replied correctly, also by correctly answering on my nationality. \\\" (P31) For some participants, the ability to assess the assistant\'s response before committing to it (i.e. by inserting assistant-generated code into their editor) was a boon. P15 described how the copy/paste boundary provided them with \\\"a bit more control to ask specific questions about what I wanted and to assess before putting it in my code.\\\" Other participants felt that the copy/paste boundary was more inefficient: \\\"I think the main difference is the ability of Copilot to suggest code while you type, what make it faster and easier to use. While using the Programmer\'s Assistant, you need to go to the chat, ask the question, copy the code (or rephrase the question if it was not understood by the agent), and edit it to match your code. \\\" (P3) A large number of participants felt that the conversational interaction was faster than web search (P1, P6, P7, P10, P11, P12, P16, P17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability to provide \\\"real-time responses\\\" (P32) that can be \\\"applied exactly to your code\\\" (P33) without having to \\\"parse through lots of text... to get what you need\\\" (P15). In addition, the assistant provided \\\"MUCH faster, better responses\\\" (P17) that were \\\"much more relevant to the problems\\\" (P34) and \\\"simple [and] succinct\\\" (P9), without having to \\\"sort through answers on your own or read documentation\\\" (P9) or \\\"look at many posts before finding the relevant one\\\" (P18).\", \"Despite these benefits, some participants felt that the assistant might not work well for \\\"more specific and difficult problems on a bigger scale\\\" as compared to web search. P9 felt that \\\"the data [of the Programmer\'s Assistant] wasn\'t as rich\\\" as the web. Other participants felt that the assistant lacked the \\\"multiple answers\\\" (P9) and \\\"rich social commentary\\\" (P19) that accompanies answers on Q&A sites: \\\"I like to see the different versions proposed on stack overflow and the commentary of what makes one solution better than another in a given situation. \\\" (P27) Some participants promoted a more balanced view that there isn\'t a single mode of interaction superior to all others. P19 felt that web search would be a fallback when the assistant failed to answer a question. P39 described how search could be integrated with the conversational interaction:\", \"\\\"I think both options should exist: people should be able to input their queries like a search bar AND also give their question as if in conversation. \\\" (P39)\"]}, {\"header\": \"DISCUSSION\", \"paragraph\": []}, {\"header\": \"Value of Conversational Interaction\", \"paragraph\": [\"We began our research by asking the question of whether contemporary developments in code-fluent LLMs could sufficiently support a conversational programming assistant. We believe that our work has demonstrated that they can. Clearly, the Programmer\'s Assistant was viewed by our participants as a useful tool that provided real value -so much so that many participants explicitly requested or expressed the desire to use it in their own work. However, how much of this value was derived from the model itself and its ability to produce high-quality responses to programming questions, versus from participants\' ability to conduct extended conversational interactions grounded in their actual source code?\", \"We believe that both of these constituent aspects were valuable. Indeed, many participants commented on their surprise and satisfaction with the quality of the assistant\'s responses (Section 5.2.3). However, participants also valued the conversational interactions that they had with the assistant. In the event logs, we saw evidence that participants were leveraging conversational context to ask follow-up questions as well as leveraging code context by asking about their code selections (Section 5.3.2). Many participants reported that they would find the tool less valuable if the conversational interaction were removed (Section 5.3.2). Further, conversation seemed to provide unique value beyond other interaction models (direct manipulation and search) because of its embeddedness in the UI and its ability to surface emergent behaviors of the model (Section 5.4.4).\", \"We do not believe that these different interaction models are in competition and we agree with P39\'s assessment that assistive tools can be built using a plethora of different interaction models. For use cases in which a model is known to produce high-quality results (e.g. code autocompletion for Codex), a direct manipulation interface seems wholly appropriate as it would provide a discoverable and predictable way of invoking the model to produce a known type of result. However, direct manipulation interfaces may be less ideal for surfacing the emergent behaviors of a foundation model [14], and thus natural language interaction may be more suitable. Many popular text-to-image models, such as DALL-E 2 [66] and Stable Diffusion [72], operate in a one-shot fashion, in which the user specifies a prompt, clicks a button, and gets results. Our study demonstrates how the additional contextual layers of conversational history and the artifact-under-development provide additional value to the co-creative process.\"]}, {\"header\": \"Toward Human-AI Synergy\", \"paragraph\": [\"The aim of human-centered AI is to \\\"enable[] people to see, think, create, and act in extraordinary ways, by combining potent user experiences with embedded AI methods to support services that users want\\\" [82]. Building upon this definition, Rezwana and Maher [69] posit that, \\\"In a creative collaboration, interaction dynamics, such as turn-taking, contribution type, and communication, are the driving forces of the co-creative process. Therefore the interaction model is a critical and essential component for effective co-creative systems.\\\" [69]. They go on to note that, \\\"There is relatively little research about interaction design in the co-creativity field, which is reflected in a lack of focus on interaction design in many existing co-creative systems. \\\"\", \"Our study begins to address this gap. While many co-creative systems examine casual tasks or experimental activities (e.g., Spoto and Oleynik [87]), our focus was on the co-creative practice of programming. Our goal was to understand peoples\' attitudes toward a conversational programming assistant, akin to Wang et al.\'s examination of data scientists\' attitudes toward automated data science technologies [99]. We found that, despite an initial level of skepticism, participants felt that a conversational assistant would provide value by improving their productivity (Section 5.4.3). However, further work is needed to assess the extent to which this type of assistance provides measurable productivity increases.\", \"Campero et al. [19] conducted a survey of papers published in 2021 that examined human-AI synergy, the notion that a human-AI team can accomplish more by working together than either party could accomplish working alone. They found mixed results, with no clear consensus emerging on how to design human-centered AI systems that can guarantee positive synergy. Summarizing from their discussion, \\\"Perhaps achieving substantial synergies among people and computers is harder than many people think. Perhaps it requires... new ways of configuring groups that include people and computers. And perhaps it needs more systematic, focused attention from researchers than it has, so far, received. \\\" [19, p.9] We believe such evaluations of human-AI synergy should go beyond one-shot performance measures. As implied by many of the uses cases listed by Seeber et al. [80], human-centered AI systems are often deployed in socio-organizational contexts that require longitudinal use [20,41,43], such as product design [93], game design [4], and engineering [20,Section 3.2.2]. Thus, we would expect that over time and through interaction with each other, human-AI teams would improve their performance through a mutual learning process.\", \"Evidence for this process surfaced in our study when participants described how they could improve their programming skills by interacting with the assistant (Section 5.3.3). We assert that the learning should operate in both directions: not only should people improve their programming skills, but the model itself can also improve based on peoples\' interactions with it. For example, when the assistant provides a code example to the user, and the user takes that example and edits it, those edits constitute feedback that can be used to further fine-tune the model. In addition, through longitudinal use, we believe that human and AI partners can create reciprocal representations of one another -i.e., the human is likely to create a mental model of the AI, and the AI may be engineered to develop a user model for each of its human users [30,48,79]. Such a pair of models is often described as Mutual Theory of Mind [29,100]. This type of capability raises the possibility of personalizing and adapting an assistant to the strengths and needs of individual users.\", \"With such models, an assistant that knows a user is learning a programming language could provide natural language explanations alongside code outputs, whereas an assistant that knows a user is strongly skilled in a programming language might shorten or omit those explanations. Similarly, users are likely to update their mental models of the AI with more experience. We believe the space for exploring how these reciprocal models impact human-AI synergy is rich, and we encourage additional work in this area.\", \"Human-centered AI systems that are designed to combine and synergize the distinct skills of humans and AI models cannot succeed if they diminish the human skills upon which they depend. Well-designed human-centered AI systems develop new and complementary skills for both the human and AI constituents [82,83], and we believe that mutual learning may address concerns that the wide deployment and use of AI systems will result in a de-skilling of the workforce [77,108].\", \"Ultimately, the design decisions that go into an interactive AI system have ethical implications. Our design attempts to augment the user\'s knowledge and skills by presenting help on demand, couched in non-authoritative suggestions, which leaves the user firmly in control and ultimately responsible for the work product.\"]}, {\"header\": \"Opportunities for Future Research\", \"paragraph\": [\"Our work highlights many interesting avenues for future enhancements that could be made to LLM-based conversational assistants such as our Programmer\'s Assistant, as well as future humancentered research on LLM-based conversational assistance.\", \"Our work employed a code-fluent model that was not specifically designed to handle conversational interaction. Fine-tuning the underlying LLM for conversational interaction, such as what has been done with Lamda [91], is one opportunity to improve the assistant\'s performance. Another opportunity is to align the language model to follow the desiderata proposed by Askell et al. [11] and described by Ouyang et al. as, \\\"helpful (they should help the user solve their task), honest (they shouldn\'t fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment)\\\" [61, p.2]. Glaese et al. [33] propose a slightly different desiderata of \\\"correct\\\" instead of \\\"honest, \\\" which may be more applicable to the software engineering domain, as the ability to produce correct code and correct answers about code are both important properties of a conversational programming assistant.\", \"Combining LLMs with search-based approaches to establish additional context for the model, such as AlphaCode [44] has done, may also result in more capable systems. These \\\"searches\\\" need not be limited to textual sources, but could be conducted over appropriate semantic stores (e.g. a knowledge graph) and take advantage of explicit semantic reasoning services, resulting in an integration of symbolic and neural approaches. Further, allowing for \\\"internal deliberation\\\" of the type shown in Nye et al. [59] could result in better-reasoned results, as well as better explanations and justifications.\", \"Another avenue for improvement involves the prompt used to configure the assistant (Appendix D). Just as the prompt for each successive interaction is modified by the growth of the conversational transcript, there is no requirement that the initial prompt be static. It too can be specialized to incorporate aspects of a user model, enabling the realization of a Mutual Theory of Mind [29,100]. Providing better UX affordances for visualizing and manipulating the active contexts -code and conversation -could provide users with more control over which information contributes to the generation of the assistant\'s response.\", \"Our participants clearly indicated that they were interested in having an assistant that behaved more proactively, in contrast to our deliberate design of an assistant that never takes conversational initiative. A more proactive assistant would be able to interrupt or remind a user when necessary [23], yet this characteristic raises many challenging issues. How can we calibrate the threshold for such interruptions? How can users tune the assistant to deliver only those interruptions that the they would find useful (e.g., [28,81])? How can we help users to regain their prior context after dealing with an interruption (e.g. [89])? Should an assistant be used to persuade or nudge the user (e.g. [35])? Who should determine the topic, frequency, and insistence of such persuasion attempts (e.g. [52,85])? Should users have the ability to moderate or defeat attempted persuasions, or should those decisions be left to the organization?\", \"Finally, we explored the different kinds of role orientations our participants had toward the assistant and found that participants varied in their views of it as a tool versus a social agent (e.g. collaborator or colleague). We posit that peoples\' effectiveness in working with an AI system may be influenced by their role orientation, and we encourage future research in this area.\"]}, {\"header\": \"CONCLUSION\", \"paragraph\": [\"We developed a prototype system, the Programmer\'s Assistant, in order to assess the utility of a conversational assistant in a software engineering context. The assistant was implemented using a stateof-the-art code-fluent large language model, Codex [24], and was capable of generating both code and natural language responses to user inquiries. We further used the prompting mechanism of the model to set up a conversational interaction in which the model uses the conversational history, plus the user\'s current utterance, in order to generate a response. In this way, users are able to ask follow-up questions in the chat that reference prior utterances and responses. We incorporated the conversational assistant into a code editing environment, enabling the conversation to be grounded in the context of the user\'s source code.\", \"We evaluated this system with 42 participants with varied levels of programming skill, and their quantitative and qualitative feedback, coupled with their usage of the system, demonstrated the varied, and sometimes emergent, types of assistance it was able to provide. Many participants noted the high quality of the conversational responses, including the assistant\'s ability to produce code, explain code, answer general programming questions, and even answer general knowledge questions. Participants felt this type of assistance would aid their productivity, and they drew meaningful contrasts between the conversational style of interaction with other tools that employ a direct manipulation or search-based interaction model.\", \"Our study motivates the use of conversational styles of interaction with large language models by showing how they enable emergent behaviors in a co-creative context. The Programmer\'s Assistant did not always generate perfect code or correct answers; nonetheless, participants in our study had an overall positive experience working with it on a variety of programming challenges. We believe that our work takes us one step closer to realizing the vision of human-centered AI: learning how to design systems that maximize the synergy in human-AI collaborations. \\u2022 Add buttons in the chat UI for common queries, such as \\\"what does this code do?\\\" or \\\"document this code. \\\"\", \"\\u2022 Have the Programmer\'s Assistant examine your code and make proactive suggestions for improving it in the chat.\", \"\\u2022 Have the Programmer\'s Assistant examine your code and make proactive suggestions for improvements in comments inserted directly into the code. 11. Do you have any other suggestions for how we could improve the experience of working with the Programmer\'s Assistant?\", \"Open-ended response\"]}, {\"header\": \"B THE PROGRAMMER\'S ASSISTANT TUTORIAL\", \"paragraph\": [\"The tutorial provided to study participants, like all the challenges, was presented as pre-loaded text in the code editor. Participants were encouraged to modify the text to record their results and submit it at the completion of the tutorial.\", \"Listing 4: The Programmer\'s Assistant study tutorial Did it do it correctly ? : 44 45 5) Select the code below and ask the system to 7) See if the assistant remembers your name For example \\\" What \' s my name ?\\\" Did it ? : 8) Click the \\\" try again \\\" button at the top of the chat . You should get a different answer .\", \"Try it a few times . Did it ever get your name right ?: If the assistant gives you an answer that is obviously wrong or it claims to not know an answer that you think it should know , or you just want to see an alternate answer , it is worth it to give \\\" try again \\\" a shot . 9) Click the \\\" start over \\\" button at the top of the chat , and then enter another command to see if it remembers your name . For example \\\" What \' s my name ?\\\" Did it ? : It should really have forgotten your name now , and no amount of \\\" trying again \\\" will get it right . You can \\\" start over \\\" if the assistant ever seems confused by , or stuck on , earlier parts of the conversation . 10) You can chat with the assistant on any topic you like to explore its functionality and capabilities further . See if you can stump it with a tough question ! Thanks ! When you are done , submit your results by clicking on the blue submit button and move on to the challenges !!! \\\"\\\"\\\"\"]}, {\"header\": \"C CHALLENGES\", \"paragraph\": [\"Each of the study challenges was presented as text in the code editor. Participants completed their work in the code editor and then submitted it when finished. The prototype did not provide any ability to run or debug code and participants were encouraged to make their best attempt at solving each challenge. The plot size should be 10 inches wide and 6 inches high . The csv file is not provided , but you can assume it will have \'Date \' and \' Sales \' columns . The Date column is the x -axis . The date string shown on the plot should be in the YYYY -MM -DD format . The Sales column is the y -axis . The graph should have the title \\\" Shampoo Sales Trend \\\". 14\", \"\\\"\\\"\\\"\", \"Listing 7: Challenge 3: Creating documentation\"]}, {\"header\": \"D PROGRAMMER\'S ASSISTANT PROMPT\", \"paragraph\": [\"Listing 9 shows the initial prompt sent to Codex to configure it as a conversational agent. On subsequent exchanges, the prompt was augmented with a transcript of the user\'s requests and the assistant\'s responses. When the transcript length + initial prompt length + the new utterance length exceeded a threshold, we automatically deleted the earliest request-response pairs from the transcript until the sum fell below the threshold in order to leave room in the token allocation for a response.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We would like to thank Socrates for his tireless assistance during the user study, as well as for suggesting the title of this paper based on its abstract.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We would like to thank Socrates for his tireless assistance during the user study, as well as for suggesting the title of this paper based on its abstract.\"]}, {\"header\": \"A SURVEY INSTRUMENTS A.1 Screening Survey\", \"paragraph\": [\"The questions below were asked of prospective participants to understand their job role, Python experience, and familiarity with GitHub Copilot. The questions on Python experience were modeled after those used by Weisz et al. [103]\", \"The questions below were asked before a participant used the Programmer\'s Assistant to assess their expectations of a conversational programming assistant. This survey took approximately 5 minutes to complete and began with the instructions below: Hello! We are a team of researchers looking for feedback on a prototype system we call the Programmer\'s Assistant.\", \"The Programmer\'s Assistant is an experiment in conversational coding: it consists of a code editor integrated with a chatbot that is able to converse in natural language to answer questions, generate code, and consult on existing code.\", \"In this study, you will be asked to complete several programming tasks. We are not evaluating your programming skills on these tasks. Rather, we are interested in understanding how the Programmer\'s Assistant is able to help you accomplish those tasks. Your code and interactions with the assistant will be processed by a 3rd party AI model, so please do not include proprietary code or discuss companyconfidential information. All data we collect in this study will be anonymized before it is published.\", \"Before trying out the Programmer\'s Assistant, we would like to assess some of your expectations. We estimate that this survey will take 5 minutes. Open-ended response\", \"The questions below were asked after a participant used the Programmer\'s Assistant to complete the programming challenges. This survey took approximately 10-15 minutes to complete.\", \"A.3.1 Reflections.\", \"Of the 42 participants in our study, 21 (50%) reported their gender as Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming, and 1 preferred not to say. Seventeen ( 40%) participants had 3+ years of Python experience, 11 (26%) had 1-3 years, 11 (26%) had less than 1 year, and 3 (7%) were not familiar with Python. Twentynine (69%) participants had written Python code within the past month, 4 ( 9%) within the past year, 5 (12%) within the past 5 years, and 4 ( 9%) had not written Python code within the past 5 years.\"]}, {\"header\": \"A SURVEY INSTRUMENTS A.1 Screening Survey\", \"paragraph\": [\"The questions below were asked of prospective participants to understand their job role, Python experience, and familiarity with GitHub Copilot. The questions on Python experience were modeled after those used by Weisz et al. [103]\"]}, {\"header\": \"A.2 Pre-task Survey\", \"paragraph\": [\"The questions below were asked before a participant used the Programmer\'s Assistant to assess their expectations of a conversational programming assistant. This survey took approximately 5 minutes to complete and began with the instructions below: Hello! We are a team of researchers looking for feedback on a prototype system we call the Programmer\'s Assistant.\", \"The Programmer\'s Assistant is an experiment in conversational coding: it consists of a code editor integrated with a chatbot that is able to converse in natural language to answer questions, generate code, and consult on existing code.\", \"In this study, you will be asked to complete several programming tasks. We are not evaluating your programming skills on these tasks. Rather, we are interested in understanding how the Programmer\'s Assistant is able to help you accomplish those tasks. Your code and interactions with the assistant will be processed by a 3rd party AI model, so please do not include proprietary code or discuss companyconfidential information. All data we collect in this study will be anonymized before it is published.\", \"Before trying out the Programmer\'s Assistant, we would like to assess some of your expectations. We estimate that this survey will take 5 minutes. Open-ended response\"]}, {\"header\": \"A.3 Post-task Survey\", \"paragraph\": [\"The questions below were asked after a participant used the Programmer\'s Assistant to complete the programming challenges. This survey took approximately 10-15 minutes to complete.\", \"A.3.1 Reflections.\"]}, {\"header\": \"E STUDY PARTICIPANT DEMOGRAPHICS\", \"paragraph\": [\"Of the 42 participants in our study, 21 (50%) reported their gender as Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming, and 1 preferred not to say. Seventeen ( 40%) participants had 3+ years of Python experience, 11 (26%) had 1-3 years, 11 (26%) had less than 1 year, and 3 (7%) were not familiar with Python. Twentynine (69%) participants had written Python code within the past month, 4 ( 9%) within the past year, 5 (12%) within the past 5 years, and 4 ( 9%) had not written Python code within the past 5 years.\"]}]','https://drive.google.com/uc?id=1U779W6INcz2AAnU_HtBrNaHIG207octY&export=download','2023-02-03',0,'2024-02-03 22:19:38.469321','2024-02-03 22:19:38.469321'),(7,'Generating Diverse Code Explanations using the GPT-3 Large Language Model','','[{\"header\": \"ABSTRACT\", \"paragraph\": [\"Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8,12], defining terms [9], giving hints [16], and providing error-specific feedback [10,16]. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Github\'s Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs\' potential to support learning by explaining numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.\"]}, {\"header\": \"USE CASES\", \"paragraph\": [\"To understand the types of explanations GPT-3 [2] can generate, we issued over 700 prompts across numerous code snippets. An example prompt and resulting explanation is shown in Figure 1. We discovered eight explanation types and Figure 2 includes three Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICER 2022, August 7-11, 2022, Lugano and Virtual Event, Switzerland \\u00a9 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9195-5/22/08. https://doi.org/10.1145/3501709.3544280 explanation types to illustrate the explanatory power of GPT-3. The additional types include: 1) tracing the execution of code, 2) fixing bugs and explaining how they were fixed, 3) generating analogies to real world settings, 4) listing relevant programming concepts, and 5) predicting the console output.\"]}, {\"header\": \"Analyzing and explaining time complexity\", \"paragraph\": [\"Instructors rate time complexity as the most difficult programming topic [17]. However, understanding time complexity is important [6,13] because it facilitates decision-making so students choose an appropriate algorithm for a given problem. This use case shows GPT-3 can identify and explain time complexity.\"]}, {\"header\": \"Identifying common mistakes made by beginner programmers\", \"paragraph\": [\"Commonality exists in how students solve programming problems [15] and the mistakes they make [1,11]. Pedagogical techniques, such as the \'muddiest point\' highlight these common and most confusing concepts [3,14]. GPT-3 can automatically create a checklist of common mistakes students might make regarding a given code snippet.\"]}, {\"header\": \"Summarizing code at multiple levels of abstraction\", \"paragraph\": [\"Before understanding how a code snippet executes, it is often useful to understand the purpose of the code [5]. The summary generated by GPT-3 and shown in Figure 2 defines the goal, traces the execution, and highlights relevant CS concepts such as arrays.\"]}, {\"header\": \"DISCUSSION\", \"paragraph\": [\"Our three use cases demonstrate the potential for GPT-3 to explain code for intro CS students. Our poster presentation will feature all eight explanation types as a design space of explanations to convey the diversity of explanations that can be generated by LLMs. We will highlight best practices for generating effective explanations and pitfalls that lead to less effective explanations. We are evaluating the usefulness of these explanations in a series of summer classes.\"]}]','https://drive.google.com/uc?id=1aOxsvIpWhM8rQ4DWT1j9OwQW9sJhoRwH&export=download','2022-02-03',0,'2024-02-03 22:19:41.278415','2024-02-03 22:19:41.278415'),(8,'Framing the News: From Human Perception to Large Language Model Inferences','Identifying the frames of news is important to understand the articles\' vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computerassisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8] [54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28] [27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to these models, and to analyze use cases where they can be useful and where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, we are interested in identifying use cases and tasks where they can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.\", \"Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written.\", \"The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.\", \"Our work aims to address this research gap by posing the following research questions:\", \"RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries? RQ2: Can prompt engineering be used for classification of headlines according to frames? By addressing the above research questions, our work makes the following contributions: Contribution 1. We implemented a process to do human annotation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a personification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., human interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the performance of a large language model. Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the promptengineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.\", \"The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.\"]}, {\"header\": \"RELATED WORK\", \"paragraph\": [\"Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \\\"To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described. \\\"\", \"For frame recognition, there are two main approaches: the inductive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them appears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \\\"Does the story contain any moral message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\\\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].\", \"We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.\", \"We decided to follow the deductive approach because a predefined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the inductive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.\", \"Yl\\u00e4-Antitila et al. [60] proposed topic modeling as a frame extraction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were eliminated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, environmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.\", \"From Entman\'s definition of frame [23], it seems that the deductive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \\\"some aspects of a perceived reality and make them more salient\\\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.\", \"Isoaho et al. [30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.\", \"We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \\\"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\\\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].\", \"A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classification with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.\", \"Continuing with the question of the methods used for classification, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al. [36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the different strategies that can be followed both in the design of prompts, [12] 15 generic frames: \\\"Economic\\\", \\\"Capacity and resources\\\", \\\"Morality\\\", \\\"Fairness and equality\\\", \\\"Legality, constitutionality and jurisprudence\\\", \\\"Policy prescription and evaluation\\\", \\\"Crime and punishment\\\", \\\"Security and defense\\\", \\\"Health and safety\\\", \\\"Quality of life\\\", \\\"Cultural identity\\\", \\\"Public opinion\\\", \\\"Political\\\", \\\"External regulation and reputation\\\", \\\"Other\\\".\"]}, {\"header\": \"To label frames of full articles\", \"paragraph\": [\"Reading the full article, the annotator defines the main frame 20000 articles [33]  131 headlines + subheadlines the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], question answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al. [45] presented a very interesting idea that we apply to our classification task. This consists of providing the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.\", \"As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].\"]}, {\"header\": \"DATA: EUROPEAN COVID-19 NEWS DATASET\", \"paragraph\": [\"We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid-19 vaccination from 19 newspapers from 5 different countries: Italy, France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to understand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.\", \"Table 2: Keywords used to identify no-vax articles Keywords NO VAX TOPIC \\\"anti-vaxxers\\\", \\\"anti-vaccine\\\", \\\"anti-vaxx\\\", \\\"anti-corona\\\", \\\"no-vax\\\", \\\"no vax\\\", \\\"anti-vaccin\\\"\", \"In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table . Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.\"]}, {\"header\": \"METHODOLOGY 4.1 Human labeling of news frames\", \"paragraph\": [\"To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \'no-frame\' category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.\", \"In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\'s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.\"]}, {\"header\": \"Fine-tuning GPT-3.5 and BERT-based models\", \"paragraph\": [\"With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been In the first approach, a model with a fixed architecture is pretrained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].\", \"We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \\\"fine-tune\\\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pretrained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance.\", \"Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.\"]}, {\"header\": \"Prompt-engineering with GPT-3.5\", \"paragraph\": [\"Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].\", \"In this approach, instead of adapting pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recognizing the emotion of a social media post, \\\"I missed the bus today. \\\", we may continue with a prompt \\\"I felt so _\\\", and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \\\"English: I missed the bus today. French: _\\\"), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].\", \"We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engineering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineering with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined several experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we followed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \'interesting\', \'emotional\', \'personal\', \'human\'. The conflict frame could be: \'conflictive\', \'bellicose\', \'troublesome\', \'rowdy\', \'quarrelsome\', \'troublemaker\', \'agitator\', etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2. After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\"]}, {\"header\": \"RESULTS: HUMAN LABELING OF FRAMES IN NO-VAX NEWS HEADLINES (RQ1)\", \"paragraph\": [\"In this section, we present and discuss the results of the analysis related to our first RQ. Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant ones. Attribution of responsibility is the third one except in Switzerland, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.\"]}, {\"header\": \"Figure 3: Non-normalized distribution of frames per country\", \"paragraph\": [\"The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vaccination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro-or anti-vaccine. Attribution of responsibility is also present. Manual inspection indicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the conflict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.\", \"In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results between 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between countries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vaccination, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.\", \"It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.\", \"Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.\", \"Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between sentiment analysis and frames has been discussed in previous literature [11] [43].\"]}, {\"header\": \"RESULTS: GPT-3.5 FOR FRAME CLASSIFICATION OF HEADLINES (RQ2)\", \"paragraph\": [\"Here, we present and discuss the results related to our second RQ.\", \"6.1 Fine-tuning GPT-3.5\", \"Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERTbased models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT-3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa). On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.\", \"Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al. [55] argue that improvements in the accuracy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \'the rich get richer\', in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.\"]}, {\"header\": \"Prompt-engineering with GPT-3.5\", \"paragraph\": [\"For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This observation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.\", \"Looking at the results of the classification models, the 49% accuracy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees with human annotators in 76% of the cases. Clearly, framing involves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.\", \"News reading is never fully objective, and the annotators engaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \\\"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \'correct\' label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\\\" [42].\", \"Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is complicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].\", \"Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.\"]}, {\"header\": \"CONCLUSIONS\", \"paragraph\": [\"In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed: RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently.\", \"RQ2: Can prompt engineering be used for classification of headlines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification accuracy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classifying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators, and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"This work was supported by the AI4Media project, funded by the European Commission (Grant 951911) under the H2020 Programme ICT-48-2020. We also thank the newspapers for sharing their online articles. Finally, we thank our colleagues Haeeun Kim and Emma Bouton-Bessac for their support with annotations, and Victor Bros and Oleksii Polegkyi for discussions.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"This work was supported by the AI4Media project, funded by the European Commission (Grant 951911) under the H2020 Programme ICT-48-2020. We also thank the newspapers for sharing their online articles. Finally, we thank our colleagues Haeeun Kim and Emma Bouton-Bessac for their support with annotations, and Victor Bros and Oleksii Polegkyi for discussions.\"]}]','https://drive.google.com/uc?id=1gL-46j2YXOr643Ud0Gchoaw5lCtrqkD_&export=download','2023-02-03',0,'2024-02-03 22:19:43.610829','2024-02-03 22:19:43.610829'),(9,'Large Language Model Augmented Narrative Driven Recommendations','Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context -this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with fewshot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.','[{\"header\": \"RELATED WORK\", \"paragraph\": [\"Data Augmentation for Information Access. A line of recent work has explored using language models to generate synthetic queries for data augmentation to train models for information retrieval tasks [7,8,15,23,31]. Here, given a document collection of interest, a pre-trained language model is used to create synthetic queries for the document collection. An optional filtering step excludes noisy queries, and finally, a bi-encoder or a cross-encoder is trained for the retrieval task. While earlier work of Ma et al. [31] train a custom query generation model on web-text datasets, more recent work has leveraged large language models for zero/few-shot question generation [7,8,15,23]. In generating synthetic queries, this work indicates the effectiveness of smaller parameter LLMs (up to 6B parameters) for generating synthetic queries in simpler information-retrieval tasks [7,8,23], and finds larger models (100B parameters and above) to be necessary for harder tasks such as argument retrieval [15,23]. Similar to this work, we explore the generation of synthetic queries with LLMs for a retrieval task. Unlike this work, we demonstrate a data augmentation method for creating effective training data from sets of user documents found in recommendation datasets rather than individual documents. Other work in this space has also explored training more efficient multivector models from synthetic queries instead of more expensive cross-encoder models [39] and generating queries with a diverse range of intents than the ones available in implicit feedback datasets to enhance item retrievability [35]. 3 https://github.com/iesl/narrative-driven-rec-mint/ Besides creating queries for ad-hoc retrieval tasks, concurrent work of Leszczynski et al. [25] has also explored the creation of synthetic conversational search datasets from music recommendation datasets with LLMs. The synthetic queries and user documents are then used to train bi-encoder retrieval models for conversational search. Our work resembles this in creating synthetic queries from sets of user items found in recommendation interaction datasets. However, it differs in the task of focus, creating long-form narrative queries for NDR. Finally, our work also builds on the recent perspective of Radlinski et al. [36] who make a case for natural language user profiles driving recommenders -narrative requests tie closely to natural language user profiles. Our work presents a step toward these systems.\", \"Finally, while our work explores data augmentation from useritem interactions for a retrieval-oriented NDR task, prior work has also explored data augmentation of the user-item graph for training collaborative filtering models. This work has often explored augmentation to improve recommendation performance for minority [12,47] or cold-start users [11,28,45]. And has leveraged generative models [11,45] and text similarity models [28] for augmenting the user-item graph.\", \"Complex Queries in Information Access. With the advent of performant models for text understanding, focus on complex and interactive information access tasks has seen a resurgence [2,29,32,48]. NDR presents an example of this -NDR was first formalized in Bogers and Koolen [5] for the case of book recommendation and subsequently studied in other domains [3,4,6]. Bogers and Koolen [5] systematically examined narrative requests posted by users on discussion forums. They defined NDR as a task requiring item recommendation based on a long-form narrative query and prior-user item interactions. While this formulation resembles personalized search [42] and query-driven recommendation [20], the length and complexity of requests differentiate these from NDR. Other work has also demonstrated the effectiveness of re-ranking initial recommendations from collaborative filtering approaches Figure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering models for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with a large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic queries and user items.\", \"based on the narrative query [18]. More recent work of Afzali et al. [1] formulate the NDR task without access to the prior interactions of a user while also noting the value of contextual cues contained in the narrative request. In our work, we focus on this latter formulation of NDR, given the lack of focus on effectively using the rich narrative queries in most prior work. Further, we demonstrate the usefulness of data augmentation from LLMs and user-item interaction datasets lacking narrative queries.\", \"Besides this, a range of work has explored more complex, longform, and interactive query formulations for information access; these resemble queries in NDR. Arguello et al. [2] define the tip of tongue retrieval task, a known-item search task where user queries describe the rich context of items while being unable to recall item metadata itself. Mysore et al. [32] formulate an aspect conditional query-by example task where results must match specific aspects of a long natural language query. And finally, a vibrant body of work has explored conversational critiquing of recommenders where natural language feedback helps tune the recommendations received by users [30,44,49].\"]}, {\"header\": \"METHOD 3.1 Problem Setup\", \"paragraph\": [\"In our work, we define narrative-driven recommendation (NDR) to be a ranking task, where given a narrative query \\ud835\\udc5e made by a user \\ud835\\udc62, a ranking system \\ud835\\udc53 must generate a ranking \\ud835\\udc45 over a collection of items C. Further, we assume access to a user-item interaction dataset I consisting of user interactions with items (\\ud835\\udc62, {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 ). We assume the items \\ud835\\udc51 \\ud835\\udc56 to be textual documents like reviews or item descriptions. While we don\'t assume there to be any overlap in the users making narrative queries or the collection of items C and the user-items interaction dataset I, we assume them to be from the same broad domain, e.g., books, movies, points-of-interest.\"]}, {\"header\": \"Proposed Method\", \"paragraph\": [\"Our proposed method, Mint, for NDR, re-purposes a dataset of abundantly available user-item interactions, I = {(\\ud835\\udc62, {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 )} into training data for retrieval models by using LLMs as query generation models to author narrative queries \\ud835\\udc5e \\ud835\\udc62 : D = {(\\ud835\\udc5e \\ud835\\udc62 , {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 )}. Then, retrieval models are trained on the synthetic dataset D (Figure 3).\"]}, {\"header\": \"Narrative\", \"paragraph\": [\"Queries from LLMs. To author a narrative query \\ud835\\udc5e \\ud835\\udc62 for a user in I, we make use of the 175B parameter InstructGPT4 model as our query generation model QGen. We include the text of interacted items {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 in the prompt for QGen, and instruct it to author a narrative query (Figure 2). To improve the coherence of generated queries and obtain correctly formatted outputs, we manually author narrative queries for 3 topically diverse users based on their interacted items and include it in the prompt for QGen. The same three few shot examples are used for the whole dataset I, and the three users were chosen from I. Generating narrative queries based on user interactions may also be considered a form of multi-document summarization for generating a natural language user profile [36].\"]}, {\"header\": \"Filtering\", \"paragraph\": [\"Items for Synthetic Queries. Since we expect user items to capture multiple aspects of their interests and generated queries to only capture a subset of these interests, we only retain some of the items present in {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 before using it for training retrieval models. For this, we use a pre-trained language model to compute the likelihood of the query given each user item, \\ud835\\udc43 \\ud835\\udc3f\\ud835\\udc40 (\\ud835\\udc5e \\ud835\\udc62 |\\ud835\\udc51 \\ud835\\udc56 ), and only retain the top \\ud835\\udc40 highly scoring item for \\ud835\\udc5e \\ud835\\udc62 , this results in \\ud835\\udc40 training samples per user for our NDR retrieval models: {(\\ud835\\udc5e \\ud835\\udc62 , \\ud835\\udc51 \\ud835\\udc56 ) \\ud835\\udc40 \\ud835\\udc56=1 }. In our experiments, we use FlanT5 with 3B parameters [14] for computing and follow Sachan et al. [40] for computing \\ud835\\udc43 \\ud835\\udc3f\\ud835\\udc40 (\\ud835\\udc5e \\ud835\\udc62 |\\ud835\\udc51 \\ud835\\udc56 ). Note that our use of \\ud835\\udc43 \\ud835\\udc3f\\ud835\\udc40 (\\ud835\\udc5e \\ud835\\udc62 |\\ud835\\udc51 \\ud835\\udc56 ) represents a querylikelihood model classically used for ad-hoc search and recently shown to be an effective unsupervised re-ranking method when used with large pre-trained language models [40].\"]}, {\"header\": \"Training Retrieval Models.\", \"paragraph\": [\"We train bi-encoder and crossencoder models for NDR on the generated synthetic dataset -commonly used models in search tasks. Bi-encoders are commonly used as scalable first-stage rankers from a large collection of items. On the other hand, cross-encoders allow a richer interaction between query and item and are used as second-stage re-ranking models. For both models, we use a pre-trained transformer language model architecture with 110M parameters, MPnet, a model similar to Bert [41]. Bi-encoder models embed the query and item independently into high dimensional vectors: q \\ud835\\udc62 = MPNet(\\ud835\\udc5e \\ud835\\udc62 ), d \\ud835\\udc56 = MPNet(\\ud835\\udc51 \\ud835\\udc56 ) and rank items for the user based on the minimum L2 distance between q \\ud835\\udc62 and d \\ud835\\udc56 . Embeddings are obtained by averaging token embeddings from the final layer of MPNet, and the same model is used for both queries and items. Cross-encoder models input both the query and item and output a score to be used for ranking \\ud835\\udc60 = \\ud835\\udc53 Cr ([\\ud835\\udc5e \\ud835\\udc62 ; \\ud835\\udc51 \\ud835\\udc56 ]), where \\ud835\\udc53 Cr is parameterized as w \\ud835\\udc47 dropout W \\ud835\\udc47 MPNet(\\u2022) . We train our bi-encoder model with a margin ranking loss:\", \"with randomly sampled negatives \\ud835\\udc51 \\u2032 and \\ud835\\udeff = 1. Our cross-encoders are trained with a cross-entropy loss:\", \"For training, 4 negative example items \\ud835\\udc51 \\u2032 are randomly sampled from ranks 100-300 from our trained bi-encoder. At test time, we retrieve the top 200 items with our trained bi-encoder and re-rank them with the cross-encoder -we evaluate both these components in experiments and refer to them as BiEnc-Mint and CrEnc-Mint.\"]}, {\"header\": \"EXPERIMENTS AND RESULTS\", \"paragraph\": [\"Next, we evaluate Mint on a publicly available test collection for NDR and present a series of ablations.\"]}, {\"header\": \"Experimental Setup\", \"paragraph\": [\"4.1.1 Datasets. We perform evaluations on an NDR dataset for point-of-interest (POI) recommendation Pointrec [1]. Pointrec contains 112 realistic narrative queries (130 words long) obtained from discussion forums on Reddit and items pooled from baseline rankers. The items are annotated on a graded relevance scale by crowd-workers and/or discussion forum members and further validated by the dataset authors. The item collection C in Pointrec contains 700k POIs with metadata (category, city) and noisy text snippets describing the POI obtained from the Bing search engine. For test time ranking, we only rank the candidate items in the city and request category (e.g., \\\"Restaurants\\\") of the query available in Pointrec -this follows prior practice to exclude clearly irrelevant items [1,26]. We use user-item interaction datasets from Yelp to generate synthetic queries for training. 5 Note also that we limit our evaluations to Pointrec since it presents the only publicly available, manually annotated, and candidate pooled test collection for NDR, to our knowledge. Other datasets for NDR use document collections that are no longer publicly accessible [24], contain sparse and noisy relevance judgments due to them being determined with automatic rules applied to discussion threads [18,24], lack pooling to gather candidates for judging relevance [18,24], or lack realistic narrative queries [21]. We leave the development of more robust test collections and evaluation methods for NDR to future work.\"]}, {\"header\": \"Implementation Details.\", \"paragraph\": [\"Next, we describe important details for Mint and leave finer details of the model and training to our code release. To sample user interactions for generating synthetic queries from the Yelp dataset, we exclude POIs and users with fewer than ten reviews to ensure that users were regular users of the site with well represented interests. This follows common prior practice in preparing user-item interaction datasets for use [27]. Then we retain users who deliver an average rating greater than 3/5 and with 10-30 above-average reviews. This desirably biases our data to users who commonly describe their likings (rather than dislikes). It also retains the users whose interests are summarizable by QGen. In the Yelp dataset, this results in 45,193 retained users. Now, 10,000 randomly selected users are chosen for generating synthetic narrative queries. For these users, a single randomly selected sentence from 10 of their reviews is included in the prompt (Figure 2) to QGen, i.e., \\ud835\\udc41 \\ud835\\udc62 = 10. After generating synthetic queries, some items are filtered out ( \\u00a73.2.2). Here, we exclude 40% of the items for a user. This results in about 60,000 training samples for training BiEnc-Mint and CrEnc-Mint. These decisions were made manually by examining the resulting datasets and the cost of authoring queries. The expense of generating \\ud835\\udc5e \\ud835\\udc62 was about USD 230.\"]}, {\"header\": \"Baselines.\", \"paragraph\": [\"We compare BiEnc-Mint and CrEnc-Mint models against several standard and performant retrieval model baselines. These span zero-shot/unsupervised rankers, supervised biencoders, unsupervised cross-encoders, and LLM baselines. BM25: A standard unsupervised sparse retrieval baseline based on term overlap between query and document, with strong generalization performance across tasks and domains [38]. Contriver: A BERT-base bi-encoder model pre-trained for zero-shot retrieval with weakly supervised query-document pairs [22]. MPNet-1B: A strong Sentence-Bert bi-encoder model initialized with MPNet-base and trained on 1 billion supervised query-document pairs aggregated from numerous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned on supervised question-passage pairs from MSMarco. UPR: A twostage approach that retrieves items with a Contriver bi-encoder and re-ranks the top 200 items with a query-likelihood model using a FlanT5 model with 3B parameters [14,40]. This may be seen as an unsupervised \\\"cross-encoder\\\" model. Grounded LLM: A recently proposed two-stage approach which autoregressively generates ten pseudo-relevant items using an LLM (175B InstructGPT) prompted with the narrative query and generates recommendations grounded in C by retrieving the nearest neighbors for each generated item using a bi-encoder [19]. We include one few-shot example of a narrative query and recommended items in the prompt to the LLM. We run this baseline three times and report average performance across runs. We report NDCG at 5 and 10, MAP, MRR, and Recall at 100 and 200. Finally, our reported results should be considered lower bounds on realistic performance due to the unjudged documents (about 70% at \\ud835\\udc58 = 10) in our test collections [10].\"]}, {\"header\": \"Results\", \"paragraph\": [\"Table 1 presents the performance of the proposed method compared against baselines. Here, bold numbers indicate the best-performing model, and superscripts indicate statistical significance computed with two-sided t-tests at \\ud835\\udc5d < 0.05.\", \"Here, we first note the performance of baseline approaches. We see BM25 outperformed by Contriver, a transformer bi-encoder model trained for zero-shot retrieval; this mirrors prior work [22]. Next, we see supervised bi-encoder models trained on similar passage (MPNet-1B) and question-answer (BERT-MSM) pairs outperform a weakly supervised model (Contriver) by smaller margins. Finally, the Grounded LLM outperforms all bi-encoder baselines, indicating strong few-shot generalization and mirroring prior results [19]. Examining the Mint models, we first note that the BiEnc-Mint sees statistically significant improvement compared to BM25\"]}, {\"header\": \"Ablations\", \"paragraph\": [\"In Table 2\"]}, {\"header\": \"\\ud835\\udc56=1\", \"paragraph\": [\"which have a low likelihood of being generated from the document ( \\u00a73.2.2). Without this step, we expect the training set for training retrieval models to be larger and noisier. In Table 2, we see that excluding this step leads to a lower performance for BiEnc and CrEnc, indicating that the quality of data obtained is important for performance.\", \"6B LLM for QGen. Mint relies on using an expensive 175B parameter InstructGPT model for QGen. Here, we investigate the efficacy for generating \\ud835\\udc5e \\ud835\\udc62 for {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 with a 6B parameter Instruct-GPT model (text-curie-001). We use an identical setup to the 175B LLM for this. In Table 2, we see that training on the synthetic narrative queries of the smaller LLM results in worse models -often underperforming the baselines in Table 1. This indicates the inability of a smaller model to generate complex narrative queries while conditioning on a set of user items. This necessity of a larger LLM for generating queries in complex retrieval tasks has been observed in prior work [15,23].\", \"6B LLM for Item Queries. We find a smaller 6B LLM to result in poor quality data when used to generate narrative queries conditioned on {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 . Here we simplify the text generation taskusing a 6B LLM to generate queries for individual items \\ud835\\udc51 \\ud835\\udc56 . This experiment also mirrors the setup for generating synthetic queries for search tasks [7,15]. Here, we use 3-few shot examples and sample one item per user for generating \\ud835\\udc5e \\ud835\\udc62 . Given the lower cost of using a smaller LLM, we use all 45,193 users in our Yelp dataset rather than a smaller random sample. From Table 2, we see that this results in higher quality queries than using smaller LLMs for generating narrative queries from {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 . The resulting BiEnc model underperforms the BiEnc-Mint, indicating the value of generating complex queries conditioned on multiple items as in Mint for NDR. We see that CrEnc approaches the performance of CrEnc-Mintnote, however, that this approach uses the performant BiEnc-Mint for sampling negatives and first stage ranking. We leave further exploration of using small parameter LLMs for data augmentation for NDR models to future work.\"]}, {\"header\": \"CONCLUSIONS\", \"paragraph\": [\"In this paper, we present Mint, a data augmentation method for the narrative-driven recommendation (NDR) task. Mint re-purposes historical user-item interaction datasets for NDR by using a 175B parameter large language model to author long-form narrative queries while conditioning on the text of items liked by users. We evaluate bi-encoder and cross-encoder models trained on data from Mint on the publicly available Pointrec test collection for narrative-driven point of interest recommendation. We demonstrate that the resulting models outperform several strong baselines and ablated models and match or outperform a 175B LLM directly used for NDR in a 1-shot setup.\", \"However, Mint also presents some limitations. Given our use of historical interaction datasets for generating synthetic training data and the prevalence of popular interests in these datasets longer, tailed interests are unlikely to be present in the generated synthetic datasets. In turn, causing retrieval models to likely see poorer performance on these requests. Our use of LLMs to generate synthetic queries also causes the queries to be repetitive in structure, likely causing novel longer-tail queries to be poorly served. These limitations may be addressed in future work. Besides this, other avenues also present rich future work. While Mint leverages a 175B LLM for generating synthetic queries, smaller parameter LLMs may be explored for this purpose -perhaps by training dedicated QGen models. Mint may also be expanded to explore more active strategies for sampling items and users for whom narrative queries are authored -this may allow more efficient use of large parameter LLMs while ensuring higher quality training datasets. Next, the generation of synthetic queries from sets of documents may be explored for a broader range of retrieval tasks beyond NDR given its promise to generate larger training sets -a currently underexplored direction. Finally, given the lack of larger-scale test collections for NDR and the effectiveness of LLMs for authoring narrative queries from user-item interaction, fruitful future work may also explore the creation of larger-scale datasets in a mixed-initiative setup to robustly evaluate models for NDR.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Information Retrieval, NSF grants IIS-1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Information Retrieval, NSF grants IIS-1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors.\"]}]','https://drive.google.com/uc?id=1681cLYzuKcz9AOZioGF3EcSJeZYYm6QL&export=download','2023-02-03',0,'2024-02-03 22:19:45.863737','2024-02-03 22:19:45.863737'),(10,'A Prototype Implementation of an Orthographic Software Modeling Environment','Orthographic Software Modeling (OSM) is a view-centric software engineering approach that aims to leverage the orthographic projection metaphor used in the visualization of physical objects to visualize software systems. Although the general concept of OSM does not prescribe specific sets of views, a concrete OSM environment has to be specific about the particular views to be used in a particular project. At the University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views defined by the KobrA 2.0 method, a version of KobrA adapted for OSM. In this paper we provide an overview of the KobrA 2.0 metamodel underpinning nAOMi and give a small example of its use to model a software system.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Orthographic Software Modeling (OSM) is based on three fundamental hypotheses -(a) that it is feasible to integrate the many different kinds of artifacts used in contemporary software engineering methods within a single coherent methodology in which they are treated as views, (b) that it is feasible to create an efficient and scalable way of supporting these views by generating them dynamically, on-the-fly, from a Single Underlying Model (SUM) using model-based transformations and (c) that it is feasible to provide an intuitive metaphor for navigating around these many views by adapting the orthographic projection technique underpinning the CAD tools used in other engineering disciplines. As shown in Figure 1, the main advantages of using the idea of orthographic projection to define the views used to visualize and described a system are that they (a) can be organized according to a simple and easy-to-understand metaphor and (b) collectively represent all the properties of a system with minimal overlap and redundancy. In practice this translates into a set of \\\"dimensions\\\", each containing well defined choices (or so called \\\"dimension elements\\\") that can be used to select individuals views.\", \"As shown in Figure 2, the main advantage of making the artifacts used to describe a software system views of a SUM is that the number of pairwise coherence relationships that have to be maintained is reduced and new views can be introduced by simply defining their relationship to the SUM. Moreover, the importance of this advantage grows quickly as the size of the system and the complexity of the deployed development methodology increase. Another important advantage is that the dominance of one particular kind of view over the development process (e.g. code) at the expense of other kinds of views (e.g. graphical models) is reduced so that any appropriate type of views can be used to enrich the underlying description of the system, depending on the needs and skills of the stakeholder involved. This makes it possible to subsume all view types under the same, overarch-SUM SUM / View Centric Environment Artifact / Tools Centric Environment ing development process and methodology (e.g. agile-driven, focusing on small development cycles, or model-driven development, based on transformations between abstraction levels). Although the details of how the views are created from the SUM and how the SUM is updated from the views are not central to the approach, a natural implementation is to use the visualization and transformation technologies offered by model driven software engineering (MDSE).\", \"To explore the validity of these hypotheses at the University of Mannheim we have been developing a prototype OSM modeling environment based on an enhanced version of the KobrA method for model-driven, component-oriented development, KobrA 2.0 [1]. This was chosen as a basis for the prototype, known as the Open, Adaptable, Orthographic Modeling Environment (nAOMi) [13] because its views were designed with the precise goals of being (a) genuine projections of a subject containing carefully selected subsets of information about that subject, (b) minimalistic in the sense that they should overlap to the smallest extent possible and contain the minimum necessary models elements, and (c) selectable via a set of independent \\\"dimensions\\\" which reflect different fundamental concerns of development (i.e. abstraction levels, composition or variants). In other words, KobrA already provided one of the \\\"most orthogonal\\\" sets of views for visualizing software systems of any contemporary method. More details about the actual views and dimensions defined in KobrA are presented in the following sections. More information on OSM can be found in [2] and [3].\", \"nAOMi is implemented as an Eclipse plugin using the Eclipse Modeling Framework (EMF) as the underlying modeling platform and UML 2.0 tools [4] to generate and edit views. The KobrA 2.0 metamodel on which the current version of nAOMi is based is a specialization of the UML metamodel composed of three separate packages -one for the SUM, one for the views and one for the transformations (Figure 3). The UML was chosen as the base language because of its maturity and widespread acceptance, making the environment usable to the largest possible body of developers. UML elements not needed in KobrA 2.0 are excluded using OCL constraints while new elements or properties are introduced by specializing existing elements.\", \"The unique contribution of this paper is to elaborate on the structure of the KobrA 2.0 metamodel and how it is used to drive nAOMi. The three following sections each focus on one of the three main components of the metamodel -the SUM, the views and the transformations . This is followed by a brief overview of the OSM navigation paradigm in Section 5 before a small example of the approach is presented in Section 6. Section 7 then concludes the paper with related and future work.\"]}, {\"header\": \"SUM PACKAGE\", \"paragraph\": [\"Figure 4 depicts the internal structure of the SUM package which is based on the UML metamodel. There are three main subpackages, two containing the structural and behavioral constructs respectively, and one containing the constraints that ensure that the metaclasses are used according to the KobrA conventions and rules.\", \"The Classes subpackage of the Structure package contains some of the most fundamental elements of the KobrA metamodel, such as Class and ComponentClass. The internal structure of this package is illustrated in Figure 5. Com-ponentClass represents objects with complex and reusable behaviors, while Class captures simple \\\"data type\\\" objects that have only very simple or non-reusable behaviors. The modeler has to decide whether it is necessary to model a specific part of the system as a ComponentClass and include state charts and activity diagrams, or whether it is sufficient to use a Class (which is limited to using OCL constraints).\", \"ComponentClass inherits (indirectly via Class) from Communications so it also has the isActive attribute. This makes it possible to model whether its instances are active or passive. Active objects, which can be used to model threads and processes ([8] p. 438), start to execute their behavior as soon as they are created and perform operations spontaneously.\", \"A ComponentClass may exhibit complex behavior. In Ko-brA, this behavior may be specified in the form of UML State Diagrams (defining acceptable operation invocation sequences), and in the form of Activities (defining algorithms of operations). UML Interaction elements (in sequence diagrams) can be derived from the activity elements and thus are not included in the SUM. As KobrA aims to facilitate automatic checking of allowed sequences of operation calls, Protocol State Machines are supported instead of general state machines. Since the latter include a large variety of elements not needed for specifying acceptable operation se-quences or automatic checking, OCL constraints are used to prohibit the use of unwanted features. For example, since KobrA has no concept of roles for components, the use of role also needs to be prohibited. The part association refers to owned properties of components whose attribute isComposite is true. As KobrA uses associations like nests and creates for components, part, required and provided are not needed. Connectors (i.e. delegation and assembly) are not used in KobrA either so ownedConnector is excluded.\"]}, {\"header\": \"VIEWS PACKAGE\", \"paragraph\": [\"The structure of the Views package is illustrated in Figure 6. Again, since most of the views defined in KobrA 2.0 are based on UML diagrams, the view metamodels have similar elements to the SUM metamodel. The big difference to the SUM is that there are no restrictions on the use of the view metamodel elements. For instance, views for a particular purpose such as supporting model checkers can be supported by adding elements unrelated to the UML.\", \"The substructure of the Views package reflects the types and organization of the KobrA views according to the view \\\"dimensions\\\" supported in nAOMi (cf. example in Section 6). At the top level, the Views package is thus decomposed into the Specification and Realization options of the encapsulation dimension. These, in turn are both decomposed into the Structural, Behavioral and Operational options of the Projection dimension. Finally, with the exception of the behavioral option, these are also all subdivided into the Service and Type options of the granularity dimension. This dimension, with its two options, is an addition to the original version of KobrA.\", \"The Service view shows the direct, publicly visible relationships of the subject ComponentClass to other Compo-nentClasses, while the Type view shows the publicly visible relationships of the subject to simple Classes. As with the SUM, constraints have been defined to control what can go into each view and when they are well formed. For every view, a constraint enumerates all allowed elements (not shown in this paper).\", \"In the following, some of the other constraints for the Service view are elaborated. Since this view is a black-box view, the internals of ComponentClasses (nestedClassifier ) are not shown.\", \"context ComponentClass --no nested classifiers , no protocol inv : nestedClassifier -> union ( protocol ) -> isEmpty () Classes are only allowed if they are generalizations of Com-ponentClasses, (or any of its superclasses, since a Compo-nentClass may inherit from a class as shown in the constraints with context Class. The following invariants ensure that only publicly visible attributes and operations are in this view, for both classes and ComponentClasses (which inherit from Class). Only operation signatures are shown in this view, so pre-, post-and bodyconditions, as well as activities are omitted, which is reflected in the last constraint.\", \"context Operation --only the signature of the Operation is shown , not its behavior ( role name \\\" method \\\" refers to the Activities of the operation ) , or dependencies inv : method -> union ( precondition ) -> union ( body ) -> union ( postcondition ) -> isEmpty ()\"]}, {\"header\": \"TRANSFORMATIONS PACKAGE\", \"paragraph\": [\"The package AllViews provides the foundation for specifying the transformations between the SUM and the views in both directions. Part of the package\'s contents are shown in Figure 7. The Abstraction concept (which is in fact a dependency reused from the UML but with additional constraints) plays the key role in relating elements from the SUM to elements of a view. Abstraction is actually mapped to ExpressionInOcl. When appearing in transformations, the equals sign links elements in the SUM to the respective elements in the view, and vice versa. For instance, equality of the general meta-association of a Generalization in a transformation invariant means that, when following general, there must be an element in the SUM and in the view for which similar transformation expressions are specified.\", \"In the case of KobrA 2.0, which has many projections that just select a subset of elements using one-to-one abstractions, this allows concise declarative TransformationExpressions. Together with the view constraints, a CASE tool can be implemented which uses a transformation language of the implementor\'s choice, for instance the Atlas Transformation Language (ATL) [11] or QVT [9]. The role names se and ve are short for SumElement and ViewElement, respectively. These roles subset the client and supplier roles from the UML. SUM elements are translated into UML elements with stereotypes, so that the views are easy to manage for developers familiar with the UML. The bidirectional mappings between stereotyped view elements and non-stereotyped SUM elements are expressed in the constraints of the Association-Abstraction, a subclass of the Abstraction from the AllViews package. This is also an example of a transformation which is reused in other views. Figure 8 shows the main elements involved in the transformation of the black box structural view for Component-Classes. The first transformation constraint is on the view and declares the starting point for the transformation. It states that the subject ComponentClass and its generalizations (using a SUM utility function, superClosure) are in the view.\", \"The following transformation rules illustrate how to create the output (i.e. view) elements from the input (i.e. SUM) elements, such as the publicly visible attributes and operations of the ComponentClass and the acquired ComponentClasses. The first constraint for ComponentClassAbstraction states that references to potential general classes (and Component-Classes) of ComponentClasses are mirrored in the view. In addition, ComponentClasses will be shown with the corresponding stereotypes. The ComponentClass owns various types of associations, so in this view only the acquires associations are selected (whose transformation rules are covered in the common transformation packages).For classes and ComponentClasses, only publicly visible attributes and operations appear in the view. Class invariants are also copied. Classes that may appear in this view (e.g. as generalizations of ComponentClasses) may have a powertype (role name powertypeExtent) which will be displayed.\", \"The last transformation statement copies the class references of operations. As with all views, the transformation rules, the common transformation statements (which also cover operations) and the view constraints serve as a specification for the implementation of a view. Individual CASE tools can use different implementation techniques as long as they conform to the semantics of these rules and constraints.  For the black box type view, only publicly visible attributes and operations of classes (as opposed to Compo-nentClasses) used by the subject can be seen. This is specified in the first rule which defines owned members of the view and thus serves as the starting point of the transformation. cbbTypes is a utility function defined in the SUM which computes the black box types by selecting the types of the subject\'s public attributes and parameter types of its public operations.\", \"Class invariants and potential powertypes and connections to the classes in this view are shown as well. There may also be Enumerations, for which the EnumerationLiterals are displayed.\", \"The transformation rules for this view are almost the same as the realization transformation constraints from the package Transformation::Realization::Structural::Class::Type. The differences are the select(visibility=#public) statements for operations and attributes. s t r i n g I n S i g n a t u r e\"]}, {\"header\": \"NAVIGATION\", \"paragraph\": [\"Most of today\'s tools use some combination of trees to organize the content of models as well as the views used to visualize a software system or component. In an any environment incorporating a number of different tools there is invariably a large number of different trees storing a heterogeneous mix of artifacts including model elements (e.g. classes, instances, associations), diagrams (e.g. class diagrams, state diagrams) and other artifact types (source code, XML files, configuration files ). To work with all the views in a traditional development environment, therefore, engineers typically have to learn about the organization structures of all the incorporated tools.\", \"In contrast to conventional paradigms for organizing and navigating the many views used to visualize a system, OSM employs the metaphor of a multi-dimensional cube. More specifically, as illustrated in Figure 9, OSM regards dimension of the underlying methodology as representing a different dimension of the cube, and each independently variable aspect of that dimension is a selectable dimension element. Selecting a view thus simply corresponds to selecting a single cell within the cube. In general, three types of dimensions are supported: static dimensions in which the number of selectable elements (i.e. coordinates) is fixed, dynamic dimensions in which the number of elements is dynamic (i.e. derived from the SUM), and mixed dimensions which have both static and dynamic elements.\", \"To support the OSM dimension based navigation metaphor for KobrA, we defined the seven dimensions indicated on the left hand side of Figure 10 which is a sceenshot of nAOMI. The Abstraction dimension (not expanded here), which has three static dimension elements, PIM (platform independent model), PSM (platform specific model) and Code, captures the model-driven development concern of KobrA. The version dimension captures the state of the modeled system at specific points in time. The Component dimension, which has dynamic dimension elements defined by instances of the class ComponentClass in the SUM, captures the componentbased development concern of KobrA.\", \"The Encapsulation dimension, which has two fixed elements, supports the distinction between Specification (black box) and Realization (white box) views of components, while the Projection dimension with the fixed elements Structural, Operational and Behavioral covers the different information types. The Granularity dimension provides a finer grained distinction between views describing the types used by components (Type granularity) and views describing the required and provided interfaces (Service granularity). The Operation dimension allows a selection of individual operations.\", \"In the ideal case, when all views are truly orthogonal, the choices that can be made in each dimensions are completely independent. However, this is very difficult to achieve in software engineering. The approach still works if the views are not completely orthogonal, but dependencies then occur between different choices in different dimensions, so that the decisions made in one dimensions may affect choices possible in another dimension. This is best handled by giving dimensions a precedence ranking determined by the order in which they appear (the top being the highest). When an element in a dimension is selected, the tool automatically makes default selections for dimensions of lower precedence (i.e. dimensions lower down) and disables selections that would navigate to cells (i.e. views) which are not (yet) defined by the method at hand.\"]}, {\"header\": \"SHOPPING CART EXAMPLE\", \"paragraph\": [\"To show how a software system can be specified using nAOMi, this section presents a case study based on a shopping cart system. A ShoppingCart component collects and manages the products selected by users and supports payment via a credit card. Figure 10 illustrates a structural view of the component.\", \"In the dimension navigator on the left hand side, PIM was chosen for the \\\"Abstraction Level\\\" (not expanded in the screenshot). The second dimension is the state of the software system at a certain point in time. The picture shows that the latest available version was chosen. As with every choice in a dimension, it may influence the options in lower ranked dimensions. The component under consideration is the ShoppingCart, for which a black box view is selected in the next dimension. After the user selects the structural projection option and the service level granularity, the tool automatically chooses the option for all operations in the last dimension, as there is no editor registered for the other options.\", \"The component under development is presented with the stereotype subject and its relationship to other components and classes is shown in the view, which corresponds to a cell of the multi-dimensional navigation cube, and is generated on-the-fly from the SUM when it is selected. The classes Product and CreditCard can be used as data types in the operations of the component.\", \"Figure 11 illustrates the operational view in which an operation can be formalized using pre-and postconditions. The precondition corresponds to the assumes clause in and the postcondition corresponds to the result clause. As in the UML, the precondition of an operation must be true when the operation is invoked and the postcondition must be true when the operation is finished. The operation addProduct in Figure 11 must be in state CollectingProducts or Empty when invoked. This is also visible in the behavioral view, since there are only two transitions with the operation ad-dProduct. Both leads to the state CollectingProducts which is also a postcondition of the operation. The second postcondition is that the cost attribute of the component must be increased by the price of the added product. The pre-and postcondition can be expressed using the OCL. The properties of the component, states and operation parameters can be used to formalise the constraints like as in this example.\", \"Figure 12 shows the publicly visible behaviour of the Shop-pingCart component with states and transitions. The conditional transitions map to operations of the component. Like every view, this view is also synchronized with the SUM so that it is guaranteed that its operations, states and properties are consistent with those in the structural view. Although the operational view seems to be similar to the behavioral view because of the overlapping information within them, there are significant differences. The focus of the operational view is on a precise formal definition of an operation of a component. The operations can be enriched by preand postconditions which can be defined using complex OCL statements, that formalize the complete behavior of an operation. The additional information in the OCL statements can be used for code generation and documentation.\"]}, {\"header\": \"CONCLUSION\", \"paragraph\": [\"At the beginning of the paper we identified three fundamental hypothesis upon which the notion of OSM is based -(a) that it is feasible to integrate the many different kinds of artifacts used in contemporary software engineering methods within a single coherent methodology in which they are treated as views, (b) that it is feasible to create an efficient and scalable way of supporting these views by generating them dynamically, on-the-fly, from a Single Underlying Model (SUM) using model-based transformations and (c) that it is feasible to provide an intuitive metaphor for navigating around these many views by adapting the orthographic projection technique underpinning the CAD tools used in other engineering disciplines.\", \"The prototype tool, nAOMi, described in this paper represents the first step towards demonstrating the validity of these hypotheses and showing that OSM is a viable approach to software engineering. Of the three hypotheses, (a) and (c) are most convincingly demonstrated by the prototype, since it shows that it is indeed possible to support all the views of the KobrA method within a single navigation metaphor. The prototype tool does not demonstrate the validity of hypothesis (b) to the same extent as the others due to its small size. Although it demonstrates the feasibility of generating views from the SUM and vice-versa, the question of whether such an approach scales up to large environments is still open.\", \"Although nOAMi is the only tool developed with the specific aim of supporting KobrA-based OSM, several other tools and methods have similar properties or aims. For example, Glinz et al. [10] describe a tool with a fisheye zooming algorithm which lets the user view a model with varying amounts of detail depending on the context. It has to be investigated whether it is possible to combine the fisheye zooming concept with the dimension-based navigation paradigm. While the KobrA 2.0 implementation of nAOMi heavily uses UML diagrams for developers, Glinz et al. use custom diagram types, e.g. for structural and behavioral views.\", \"An approach which also emphasizes the description of formal consistency rules (correspondences) between views is RM-ODP [5][6]. However, this approach does not explicitly mention the notion of a SUM and thus implies that consistency rules should be defined in a pairwise fashion between individual pairs of views. ArchiMate [7], which complements TOGAF [12], is an enterprise architecture modeling language which offers two orthogonal \\\"dimensions\\\" for modeling, (business, architecture, and technology) layers and (informational, behavioral and structural ) aspects and also suggests two more dimensions, purpose and abstraction level. However, as many of these views span multiple choices of a single \\\"dimension\\\", the intuitive dimension-based navigation metaphor of OSM can not be easily applied. There are also more general approaches for view-based modeling but they are less specific in terms of consistency rules between views and provide little guidance on how to manage and navigate views, for example the Zachman Framework [14].\", \"Regarding the practical use of OSM environments in the future, the biggest challenge is developing appropriate SUM metamodels which can accommodate all the types of views and services that software engineers are accustomed to today. For this first prototypical SUM-based environment supporting the OSM approach we had a method at our disposal (KobrA) that already defined a full set of orthogonal UMLbased views. This allowed us to model the required SUM and view metamodels by simply adapting the UML metamodels, removing and adding model elements as needed.\", \"In doing so we were able to manually ensure that the metamodels fulfilled the two core requirements of SUM-based environments -(1) being minimalistic and (2) redundancy free. If SUM-based software engineering environments are to take off, and to be introduced into existing, heterogeneous environments, more sophisticated ways of integrating existing metamodels into a single unified metamodel will be required.\"]}]','https://drive.google.com/uc?id=1TBscbQ9zMU4S6Qg_AihU9wCzy58aVD6d&export=download',NULL,0,'2024-02-03 22:19:47.971887','2024-02-03 22:19:47.971887'),(11,'The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development','Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model\'s responses. We developed a prototype system -the Programmer\'s Assistant -in order to explore the utility of conversational interactions grounded in code, as well as software engineers\' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant\'s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Software development is a highly skilled task that requires knowledge, focus, and creativity [27,28]. Many techniques have been developed to enhance the productivity of software engineers, such as advanced code repositories [86], knowledge repositories [39], Q&A sites [1], and pair programming practices [18]. Collaborative software engineering is especially promising, given that professional software development is rarely a solo activity and relevant knowledge and expertise are typically distributed widely within an organization [68]. Many efforts have focused on incorporating collaborative technologies into software development environments (e.g. [8,25,26,58,101]).\", \"The pioneering work of Rich and Waters on The Programmer\'s Apprentice [70] presented a novel concept of a knowledgeable automated assistant -in effect, an artificial collaborative partner -that could help software engineers with writing code, designing software systems, and creating requirements specifications. At the time, AI technologies and computing resources were not sufficient to fully implement their vision. In the intervening years, an increase in computational power, the availability of large corpora of language and code data, and the development of deep neural networks have made new approaches to achieving their goals worth exploring.\", \"Recently, models leveraging the transformer architecture [96] have been developed to perform domain-specific software engineering tasks, such as translating code between languages [75], generating documentation for code [36,38,97,98], and generating unit tests for code [92] (see Talamadupula [90] and Allamanis et al. [5] for surveys). Recently developed foundation models -large language models that can be adapted to multiple tasks and which exhibit emergent behaviors for which they have not been explicitly trained [14] -have also proven to be capable with source code.\", \"While the intent of training LLMs such as GPT-2 [64] and GPT-3 [17] was to give them mastery of natural language, it quickly became apparent that the presence of code in their training corpora had given them the ability to generate code based on natural language descriptions [49]. The Codex model [24] was then produced by finetuning GPT-3 on a large corpus of source code data, leading to the development of Copilot [32], a tool that helps software engineers by autocompleting code as it is being written. Experimentation with Copilot has shown its ability to perform additional tasks, such as explaining code, generating documentation, and translating code between languages [6].\", \"Although autocompletion interfaces are useful and valuable when the system can discern the developer\'s intent, there are many instances where that is insufficient. For example, the developer may have a good idea of what they want to do, but may be unclear on what functions, libraries, or even algorithms to employ. They may even have general programming questions that need to be answered before they are able to write any code.\", \"In this paper, we seek to understand whether modern developments in code-fluent foundation models -large language models that have been fine-tuned on source code data -are sufficient to support a conversational agent that can act as an assistant in the software development process. We developed the Programmer\'s Assistant to explore the capabilities that conversational interaction could enable and the extent to which users would find conversational assistance with programming tasks desirable and useful.\", \"We hypothesize that a conversational system may provide a flexible and natural means for interacting with a code-fluent LLM. Conversational interaction could enable users to pursue their questions in a multiple exchange dialog (as observed by Barke et al. [13]) that allows them to ask follow-up questions and refine their inquiries. A conversational programming assistant could ask the user clarifying or disambiguating questions to help it arrive at the best answer. It could also provide multiple types of assistance to the user beyond simply generating code snippets, such as engaging in general discussion of programming topics (e.g. [22,71]) or helping users improve their programming skills (as observed in other studies of automating technologies [99]).\", \"Our paper makes the following contributions to the IUI community:\", \"\\u2022 We provide empirical evidence that a conversational programming assistant based on a state-of-the-art, code-fluent foundation model provides valuable assistance to software engineers in a myriad of ways: by answering general programming questions, by generating context-relevant code, by enabling the model to exhibit emergent behaviors, and by enabling users to ask follow-up questions that depend upon their conversational and code contexts. \\u2022 We show how different interaction models -conversation, direct manipulation, and search -provide complementary types of support to software engineers with tradeoffs between the user\'s focus and attention, the relevance of support to their code context, the provenance of that support, and their ability to ask follow-up questions.\", \"\\u2022 We motivate the need to further understand how to design human-centered AI systems that enhance the joint performance of the human-AI collaboration.\"]}, {\"header\": \"RELATED WORK\", \"paragraph\": [\"We discuss three areas of related work that have either motivated our study of conversational programming assistance or provided the technical foundations for it. We begin by briefly summarizing Rich and Waters\' visionary work on the Programmer\'s Apprentice [70], followed by summarizing work on code-fluent foundation models and human-centered evaluations of how these models impact software engineers\' work. Finally, we discuss conversational interaction and how it might be employed to provide more flexible and sophisticated assistance to software engineers.\"]}, {\"header\": \"The Programmer\'s Apprentice\", \"paragraph\": [\"Our work is inspired by the vision laid out by Rich and Waters [70], which describes an artificial agent that can act as an intelligent assistant for software engineers by providing advice, catching errors, and handling routine details throughout the software development process. The Programmer\'s Apprentice [70] relied on a knowledge base of \\\"clich\\u00e9s,\\\" which are formal, structured versions of what are known today as software design patterns [31]. It used a hybrid reasoning system capable of special-purpose reasoning based on frames and a plan calculus, along with general purpose logical reasoning. Although natural language interaction was envisioned, the original prototype implementation ultimately used a stylized command language. We view our work as a conceptual successor to the Programmer\'s Apprentice, as it enables the natural language interaction that the Programmer\'s Apprentice lacked.\"]}, {\"header\": \"Code-fluent Foundation Models and Human-Centered Evaluations of Programming Assistance\", \"paragraph\": [\"Generative models based on the transformer architecture [96] have recently been applied to the domain of software engineering. Codefluent large language models are capable of generating code from natural language descriptions [105], translating code from one language to another [75], generating unit tests [92], and even generating documentation for code [36,38,97,98]. These models are probabilistic systems, and as such, do not always produce perfect results (e.g. code that is free of syntax or logical errors). Nonetheless, Weisz et al. [102] found that software engineers are still interested in using such models in their work, and that the imperfect outputs of these models can even help them produce higher-quality code via human-AI collaboration [103]. New tools based on code-fluent LLMs are actively being developed. GitHub Copilot1 is described as \\\"Your AI pair programmer. \\\" It is optimized for the code autocompletion use case: given a starting snippet such as a method\'s documentation, signature, or partial implementation, Copilot completes the implementation. Copilot is based on the OpenAI Codex model [24], a 12 billion parameter version of GPT-3 [17,49], fine-tuned on code samples from 54 million public software repositories on GitHub. Empirical evaluations of this model have shown that, although the quality of its outputs is quite good, those outputs may still be problematic [57]. Echoing the results from Weisz et al. [103], human-centered evaluations of Copilot have found that it increases users\' feelings of productivity [109], and that almost a third (27%) of its proposed code completions were accepted by users. In a contrasting evaluation, Vaithilingam et al. [95] found that while most participants expressed a preference to use Copilot in their daily work, it did not necessarily improve their task completion times or success rates. Yet, in a study by Kalliamvakou [40], developers working with Copilot were able to implement a web server in Javascript 55% faster than developers who did not use Copilot.\", \"A grounded theory analysis of how programmers interact with Copilot [13] found that their interactions varied depending upon whether they were accelerating tasks that they already knew how to do or if they were exploring solutions to problems that they were less sure about. Autocompletion was effective when developers were operating in \\\"acceleration mode\\\" and relied on the model to produce short completions that could be verified quickly. In \\\"exploration mode,\\\" however, the interaction was more awkward. Developers would communicate with Copilot by typing comments and seeing what Copilot generated in response. Then, they would modify their comments to explore other ways of prompting a response. Ultimately, the comments used to prompt the model would be deleted after the relevant code was generated, indicating that their value was largely in driving a back-and-forth, yet context free, dialog with the model to coerce it to produce the desired results through an iterative refinement process. In this paper, we fully commit to a context-aware conversational style of interaction with a code-fluent LLM and assess the value it provides to users.\"]}, {\"header\": \"Conversational Interaction and Analysis\", \"paragraph\": []}, {\"header\": \"Conversational Interaction.\", \"paragraph\": [\"Using natural language to interact with technology has had a long research history [2], starting in the 1960s with pattern-matching approaches like Eliza [104], and continuing to today with state-of-the-art large language modelbased conversational systems [107] such as Meena [3] and Blender-Bot [84]. These systems are intended to address the problem of open-domain dialog, with a goal of realistically engaging in conversation, but not particularly in a goal-directed or task-oriented manner.\", \"Task-oriented chatbots are typically built with frameworks such as the Microsoft Bot Framework2 , Google DialogFlow3 , and IBM Watson Assistant4 . They operate using pre-defined dialogue trees and use natural language processing to detect conversational intents and extract contextual entities. This structure enables the creation of special purpose, but fairly limited and rigid, conversational agents.\", \"There have been several recent attempts to investigate conversational programming assistance. Kuttal et al. [42] conducted a Wizard of Oz study in which a pair programmer was replaced with a conversational agent, and they found that \\\"agents can act as effective pair programming partners.\\\" The PACT system [106] is a chatbot that assists programmers adjusting to new programming environments. PACT is structured as a discrete question-answering system based on a neural machine translation approach, but it doesn\'t maintain a conversational context.\"]}, {\"header\": \"Conversation Analysis.\", \"paragraph\": [\"Conversation is a form of interaction between people that enables robust communication. Conversation Analysis [76] is a method for understanding the natural structure of human conversational interaction. It catalogs different patterns of conversational acts and how they are utilized by interlocutors in order to attain a wide variety of goals. Recently, Conversation Analysis has been adapted to describe patterns of interactions between humans and artificial conversational agents in order to aid in the design of chatbots [50]. We apply techniques from Conversation Analysis in our study of conversational programming assistance.\"]}, {\"header\": \"THE PROGRAMMER\'S ASSISTANT\", \"paragraph\": [\"In order to explore conversational programming assistance, we created a functional prototype system called The Programmer\'s Assistant. Our prototype, shown in Figure 1, combines a code editor with a chat interface. The code editor was implemented using the Microsoft Monaco Editor5 embedded in a React wrapper 6 . The chat user interface was implemented using the React-Chatbot-Kit7 framework. To drive the conversational interaction, we employed OpenAI\'s Codex model [24], accessed through its web API.\", \"We developed our prototype as a lightweight coding environment in order to examine the user experience of interacting with a conversational assistant. Our work was exploratory in nature, and thus we did not have specific design goals for the prototype beyond integrating a code editor with a code-fluent LLM. We also did not attempt to target the prototype for a specific class of users (e.g. novices or experts) or use cases (e.g. writing code vs. learning a new programming language), as we wanted any value provided by conversational assistance to emerge from our user study. We also did not implement the ability to run or debug code in our prototype as we wanted to explore the nature of the conversational interaction rather than having users focus extensively on the production of working code.\", \"When designing how users would interact with the Programmer\'s Assistant, we decided that it should be available on demand and not monitor the user\'s work in progress or give unsolicited suggestions or advice, in keeping with the conversational agent interaction model proposed by Ross et al. [73,74]. This approach was supported by feedback from prospective users who were concerned about the assistant providing criticism of unfinished efforts in progress or distracting them while they worked. Instead, we force initiative onto the user and only have the assistant respond to their requests. In this way, the assistant can provide help when requested without undesirable interruptions that can distract or interfere with the user\'s flow.\", \"When a user interacts with the assistant, we keep track of their selection state in the code editor. If a user sends a message to the assistant without any code selected in the editor, then that message (along with the prior conversational context) is passed directly to the model. If a user sends a message to the assistant with new code selected in the editor (i.e. code that wasn\'t previously selected when they sent their last message), then that code is appended to the message before being communicated to the model.\", \"The model may produce multiple types of responses to a user\'s message. We treat each type of response differently in the UI.\", \"\\u2022 Responses that do not contain code are always rendered in the chat UI (Figure 1E). \\u2022 Responses containing short code snippets (\\u2264 10 lines) are rendered inline in the chat UI (Figure 1G). \\u2022 Responses containing longer code snippets (> 10 lines) show the code in a pop-up window (Figure 2A), with a proxy entry in the chat transcript (Figure 2B) that allows users to redisplay the code window after it has been closed. Non-code text in the response remains in the chat transcript. The assistant never directly modifies the contents of the user\'s source code; rather, any code the user desires to transfer from the chat takes place via copy/paste.\", \"Figure 1 shows a screenshot of a real, sample conversation, in which the user asks a question that results in an inline response, then requests an explanation of some code in the editor, and then requests further elaboration. Figure 2 shows an example conversation that resulted in the generation of a longer code sample, shown in a popup window. This example shows how the assistant produced an incomplete solution, followed by criticism from the user regarding the missing code, and resulting in an apology and the generation of a complete solution.\"]}, {\"header\": \"Supporting Conversational Interaction\", \"paragraph\": [\"We enabled Codex to conduct a conversational interaction by prompting it with a conversational transcript and a request to produce the next conversational turn. The prompt establishes a pattern of conversation between a user and a programming assistant named Socrates. It provides several examples of Socrates responding to general coding questions, generating code in response to a request, and accepting code as input. It establishes a convention for delimiting code in the conversation, making it easy to parse for display in the UI. It also establishes an interaction style for the assistant, directing it to be polite, eager, helpful, and humble, and to present its responses in a non-authoritative manner 8 . Because of the possibility that the model might produce erroneous answers or incorrect code (as discussed in Weisz et al. [102]), we felt it was important that the assistant convey a sense of uncertainty to encourage users to not accept its results uncritically to avoid over-reliance (e.g. as observed in Moroz et al.\'s study of Copilot [51], and discussed more generally in Ashktorab et al. [9]) as well as automation bias [45,46,65]. We present the full text of the prompt used for the assistant in Appendix D.\"]}, {\"header\": \"Architecture & UI Design\", \"paragraph\": [\"The Programmer\'s Assistant communicates with the Codex API via a proxy server that forwards requests from the React client. The proxy also rate-limits access to conform to the API\'s policy, and it logs UI events from the client (e.g. requests, responses, and UI interactions) in a back-end database. To address inconsistencies in the style or formatting of code generated by Codex, the proxy server reformats all code segments using the Black code formatter 9 before transmitting them to the client UI.\", \"The client maintains the transcript of the ongoing conversation. Each time the user sends a message in the chat, the client constructs a new prompt for the model by concatenating the initial prompt, the chat transcript, and the user\'s new utterance, and makes a request for the model to complete the transcript. This completion request also specifies a stop sequence of tokens to prevent the model from generating both sides of the conversation (e.g. what the model thinks the user\'s next utterance might be after the assistant\'s response). Given the API\'s limitation on context length (4,096 tokens for both the prompt and model response), we silently \\\"forget\\\" older exchanges in the chat transcript when constructing the prompt to ensure that our completion request remains within bounds. Nonetheless, the entire conversational history remains visible to the user in the UI.\", \"The client UI provides a loose coupling between the source code editor and the chat interface. Users can hide the chat pane when they wish to focus solely on their code, and re-engage with it when they desire assistance. Code selected in the editor is included in the conversation in order to couple the code context with the conversation. Easily-accessible buttons are provided in the UI to copy code responses from the assistant to the clipboard.\"]}, {\"header\": \"Handling Model Limitations\", \"paragraph\": [\"While developing the Programmer\'s Assistant, and in early pilot testing, we experienced some quirks and shortcomings of the model and our approach to using it for conversational interaction. One limitation stemmed from the fact that the model sometimes produced incorrect responses (e.g. code with syntax errors), incomplete responses (e.g. code that was missing functionality), irrelevant responses (e.g. responses not related to the user\'s question), or insubstantial responses (e.g. \\\"I don\'t know\\\"). Because of the probabilistic nature of model inference, re-prompting the model would sometimes produce a more correct or appropriate response. Thus, we added the ability for users to \\\"try again, \\\" either by asking in the chat or by clicking a button in the UI (Figure 1C). This feature removes the assistant\'s last response from the context presented to the model and then re-invokes the model with an increased temperature 10 .\", \"Although it is possible for transformer models such as Codex to produce multiple possible responses to a single prompt, we only request a single response in order to speed up response time as well as to preserve the token budget for conversational context. Thus, the \\\"try again\\\" feature provides an alternate way to produce a wider variety of responses.\", \"During pilot testing, we noticed that the assistant sometimes happened to generate the same response to multiple, unrelated requests. In these cases, the assistant tended to get \\\"stuck\\\" in a pattern of repeating the same response and was unable to resume normal conversation. To avoid this problem, we automatically execute a The \\\"try again\\\" button (C) allows users to ask the assistant to generate an alternate response to the most recent question. The \\\"start over\\\" button (D) resets the conversational context for the assistant, but maintains the chat transcript in the UI. In this example, we show the assistant introduce itself to the user (E). Next, the user asks a general programming question (F), for which the assistant provides an inline code response (G). The user then asks a question about code selected in the editor (H), followed by a series of follow-up questions.\", \"\\\"try again\\\" operation in the background when we see identical consecutive responses from the assistant. Finally, we noticed that the accumulation of conversational context sometimes resulted in the assistant becoming fixated on some portion of the earlier conversation. For example, it might respond to a question with portions of the prompt or of earlier conversation, and become less responsive to newer requests. To address this issue, we introduced a \\\"start over\\\" feature, accessible via the chat or by clicking a button in the UI (Figure 1D), that resets the context to the original prompt, forgetting the rest of the conversational history. We preserve the chat transcript in the UI, but delineate the break in the assistant\'s memory with an annotation in the chat transcript. These annotations are added both for \\\"try again\\\" and \\\"start over. \\\"\"]}, {\"header\": \"Sample Conversation\", \"paragraph\": [\"We provide a real sample conversation with the Programmer\'s Assistant in Listing 1. This conversation begins with the assistant greeting the user (line 1). Next, the user asks a general Python programming question (line 4), to which the assistant responds with a non-authoritative remark (\\\"I think...\\\") and a code snippet (line 9). The user next asks a follow-up question that depends on their previous question and the assistant\'s response (line 11), to which the assistant provides another code snippet (line 15), satisfying the user\'s request.\", \"The user then switches topics and asks the assistant to write a Fibonacci function (line 17), and the assistant again responds with a non-authoritative remark (\\\"I will give it a try, \\\" line 20) and a block of code. The user then asks how the function works (line 30) and the assistant provides an adequate description (line 32). Next, the user asks the assistant to re-implement the function in a different way (line 37), again leveraging the ability to ask follow-up questions. The assistant produces an alternative implementation that conforms to the user\'s request (line 41). The user follows up with a question that depends on multiple past utterances and responses in the chat transcript (line 47), and the assistant produces a relevant response (line 49). The conversation closes with the user thanking the assistant (line 53) and the assistant acknowledging their gratitude (line 55).\", \"Listing 1: A conversation with the Programmer\'s Assistant. Code presented by the assistant is listed in bold face. To address our questions, we deployed the Programmer\'s Assistant within our organization -a global technology company -and invited people to try it out and give us feedback on their experience. We invited people with varying levels of programming skill in order to obtain a wide range of feedback on the kinds of use cases for which the tool could provide assistance.\"]}, {\"header\": \"Tasks\", \"paragraph\": [\"We set up the Programmer\'s Assistant as a playground environment that participants could try out with a few sample programming problems. We created a tutorial to orient participants to the assistant, its capabilities, and how to interact with it. We also created four programming challenges focused on writing code, documenting code, and writing tests for code. We designed these challenges to expose participants to a broad range of the assistant\'s capabilities. For each of these challenges, we explicitly did not evaluate metrics such as the participant\'s productivity, the quality of their solutions, or the time taken to produce them, as the focus of our study was to understand the utility of conversational interaction. We selected Python as the language used for the tutorial and challenges because of its general popularity [21] and the fact that it was well-supported by our underlying LLM [24].\", \"4.1.1 Tutorial. All participants were first introduced to the Programmer\'s Assistant through a tutorial. The tutorial walked each participant through 10 sample interactions to give them a feeling for what the assistant could do and how to interact with it. The tutorial demonstrated how to ask questions, how to request code to be generated, and how to evaluate existing code. It did not specifically cover how to generate documentation or unit tests. Tutorial instructions were provided within the code editor. We include the specific text used for the tutorial in Appendix B.\", \"4.1.2 Programming Challenges. After completing the tutorial, participants unlocked four programming challenges. Two of the challenges involved coding problems (writing a queue class and writing code to create a scatterplot of data in a CSV file), one involved documenting a given function (an implementation of a graph search algorithm), and one involved writing unit tests for a given function (computing the greatest common divisor of two arguments). Although the Programmer\'s Assistant was visible and available for use, we provided no specific requirement that it actually be used to complete the challenges.\", \"After participants completed their solution to a challenge, they submitted it by clicking a button in the UI. The code editor used in the Programmer\'s Assistant was not a fully-functional IDE and did not provide syntax checking or the ability to run, test, or debug code. Due to these limitations, participants were asked to submit their solutions when they felt they had completed the challenge to their own satisfaction.\"]}, {\"header\": \"Participants\", \"paragraph\": [\"To recruit participants for our study, we posted internal advertisements in various communications channels focused on software engineering. Our advertisements stated that we were evaluating a conversational programming assistant, but were kept deliberately vague in order to minimize the impact on peoples\' expectations of the experience.\", \"Our advertisement yielded a pool of 140 potential participants. In order to recruit a diverse sample, we used a screening survey that asked about their job role, their familiarity with and recency of use of Python, and their availability to participate in our study. We accepted participants into the study on a rolling basis, selecting participants to capture a range of programming experiences and ensure balanced gender representation. We conducted periodic reviews to determine whether we were learning something new from each participant or if we had reached the point of saturation [7]. We stopped collecting data after running 42 participants as we were no longer observing any new behaviors or gleaning any new insights. The Programmer\'s Assistant implementation and configuration were held constant over the course of the study; no changes to the UI design or LLM prompt were made.\", \"Our participants had the following self-identified characteristics: \\u2022 Recency of Python Use: 29 participants had written Python code within the past month, 4 within the past year, 5 within the past 5 years, and 4 had not written Python code within the past 5 years.\", \"We provide full demographic information for individual participants in Appendix E.\"]}, {\"header\": \"Procedure\", \"paragraph\": [\"Participants completed the study on their own time, independently and without moderation. Each participant was provided with a web link to a pre-study survey that described the nature of the study and the tasks that they would be expected to perform. They were then directed to the Programmer\'s Assistant to complete the tutorial and the four programming challenges. When participants indicated they were finished with the challenges 12 , they were directed to a final post-study survey. Complete sessions generally required about an hour of effort, though some participants spread their effort across a longer period of time and across multiple sessions. Participants were compensated for their time at a rate equivalent to US $15/hr.\"]}, {\"header\": \"Measures\", \"paragraph\": [\"We collected a variety of data in our study from three sources:\", \"(1) Surveys. We employed three surveys in the study: a prestudy survey to collect demographic information, a pre-task survey to gauge expectations of the conversational user experience, and a post-task survey to assess actual user experience. We describe these survey questions in the relevant context of our results, and we provide a complete listing of all survey instruments in Appendix A. (2) Event logs. The Programmer\'s Assistant was instrumented to collect data on participants\' usage. The event logs provided timestamped records of interaction events, including conversational exchanges, hiding/showing the assistant, use of the \\\"try again\\\" and \\\"start over\\\" features, and use of copy/paste. (3) Conversation logs. From the event logs, we extracted conversational transcripts between each participant and the Programmer\'s Assistant.\"]}, {\"header\": \"RESULTS\", \"paragraph\": []}, {\"header\": \"Data & Analysis\", \"paragraph\": [\"We collected a wealth of data in our study: 126 survey responses from three surveys per participant, containing 296 written comments in open-ended survey questions, and 4,877 instances of 23 different types of UI events, including 1,699 conversational exchanges 13 in the event logs. We also compute, for each participant, counts or durations for 21 different metrics from the event logs.\", \"In our analysis, we deliberately exclude the portion of our data collected during the tutorial exercise. We exclude this data because that activity was guided by the tutorial instructions, not by our participants\' own initiative. Thus, our final sample consists of 3,172 events, including 968 conversational exchanges in the event logs; no survey data was excluded.\", \"Our primary analysis of this data is qualitative, as our participants provided us with a rich source of interesting feedback and thought-provoking insights in their comments. Where applicable, we supplement this data with quantitative data from the survey and the event logs, as well as chat transcript data from the conversation logs. In this way, we triangulate [47] across our three data sources, using the open-ended survey data as a foundation. When we quote participants, either from their qualitative survey responses or the conversational transcripts, we reproduce their words exactly as typed, including typos, misspellings, grammatical errors, capitalization, and potential trigger words, and we only make minor clarifying edits where needed, delineated by square brackets.\", \"In order to set the context for our analysis, we first describe how we used reflexive thematic analysis to analyze participants\' responses to the open-ended survey questions. We then describe our analysis of the conversation logs and our development of a coding guide based on Conversation Analysis [76], and specifically, Moore and Arar\'s Natural Conversation Framework [50].\"]}, {\"header\": \"Thematic Analysis of Qualitative Survey\", \"paragraph\": [\"Responses. We conducted a reflexive thematic analysis to analyze the responses to our seven open-ended survey questions. We followed the process described by Braun and Clarke [16] in which researchers immerse themselves in the data, generate codes for material that seems interesting, and then iteratively group and refine codes through collaborative discussion in order to identify higher-level themes. Initially, four authors performed open-coding on the open-ended survey responses. Through discussion, these codes were grouped and consolidated into a single set, which were then re-applied to the data by two authors. After another round of discussion, these authors identified a set of 12 higher-level themes. Some themes had clear parallels to quantitative survey questions or event log data, and thus represented clear instances where we were able to triangulate across data sources. Other themes surprised us. We structure our presentation of the results based on these 12 themes, grouped into three different aspects of the user experience: expectations and experience, utility of conversational assistance, and patterns of interaction and mental models.\"]}, {\"header\": \"Conversation Analysis via the Natural Conversation Framework.\", \"paragraph\": [\"In order to understand the content and structure of the conversations that took place between our participants and the Programmer\'s Assistant, we turned to the Natural Conversation Framework [50] (NCF). We developed a codebook for the event logs, beginning with 21 different categories of utterances from the NCF. Nine NCF categories -Acknowledgment, Apology, Confirmation, Expression of Gratitude, Farewell, Greeting, Self-Identification, Welfare Check, and Welfare Report -appeared twice in our codebook to distinguish cases in which the utterance was made by the human participant vs. the assistant. Other NCF categories were split to provide nuanced detail about the interaction; for example, we distinguished three different kinds of NCF requests, depending upon whether they were stated as Requests for Action (e.g. codes to identify meta-information such as utterances that included code, utterances that referenced selected code, utterances that implicitly or explicitly referenced earlier portions of the conversation, or non-verbal UI activities such as copies, pastes, and invocations of \\\"try again\\\" and \\\"start over. \\\" Finally, we classified a subset of the human-applied codes based on whether they represented a participant\'s task or social orientation toward the assistant. We list our codes in Table 1, but note that not all of them ended up being relevant to our analysis. When coding conversational data, we applied individual codes at the level of each conversational utterance. We allowed multiple codes to be applied to each utterance to account for utterances that performed multiple functions (e.g. greeting and self-identification). In order to ensure consistency in how our codebook was applied, two authors coded a 10% sample of the 968 conversational exchanges, achieving a satisfactory level of inter-rater reliability (Krippendorf\'s \\ud835\\udefc = 0.77, where agreement was conservatively defined as having all of the same codes applied to both utterances in a conversational exchange).\"]}, {\"header\": \"Expectations and Experience\", \"paragraph\": [\"Pilot testing of the Programmer\'s Assistant suggested that software engineers would be skeptical of a conversational programming assistant and its ability to provide useful assistance. Our study revealed that, for most participants, their actual experience after using the tool was better than they had anticipated. Participants were surprised at the quality of the assistant\'s responses and they appreciated how its integration with the code editor reduced the amount of context switching they needed to do in the UI. Some participants struggled with the code selection feature, although others appreciated the ability to ask questions related to selected code.\"]}, {\"header\": \"Usage.\", \"paragraph\": [\"All of our participants engaged with the Programmer\'s Assistant while working on the challenges, despite there being no requirement to do so. Forty-one participants submitted solutions to all four challenges, and one participant, P14, only submitted solutions for one of the four challenges. Participants spent an average of 68 minutes engaged with the assistant, as measured by the amount of time the Programmer\'s Assistant window was in focus.\", \"Participants made an average of 23.0 utterances (SD = 15.1 utterances) to the assistant. On average, 6.2 of their utterances (SD = 4.3 utterances) contained a code selection. The average latency per request14 was 6.7 seconds (SD = 3.1 seconds).\", \"We saw a 66.3% rate of acceptance of generated code, where we considered code to be accepted if the participant performed a copy immediately after the code was generated. This acceptance rate is much higher than the 27% acceptance rate reported for Copilot [109]. We believe one reason we observed a higher acceptance rate is because Copilot\'s completion suggestions are generated proactively, whereas the Programmer\'s Assistant\'s suggestions are generated upon request. When copying generated code from the assistant, participants most often copied the entirety of the generated code, and only in 5.8% of cases did they copy a smaller portion of it.\"]}, {\"header\": \"User Experience Expectations & Changed Attitudes.\", \"paragraph\": [\"Prior to running our study, we had reason to believe that participants would be skeptical of a conversational programming assistant. Before developing the Programmer\'s Assistant, we showed potential users mockups of a program editor with an integrated chatbot feature. These prototypes elicited uniformly negative reactions. People told us about their frustrating experiences with conventional chatbots and raised doubts about the knowledge, capabilities, and value of a conversational programming assistant. This skepticism motivated us to develop the Programmer\'s Assistant in order to evaluate whether the conversational experience, as powered by a state-ofthe-art code-fluent LLM, would be better than people had anticipated. During pilot testing, we received feedback that the Programmer\'s Assistant provided a much better conversational experience compared to testers\' previous experiences with chatbots. Thus, in designing our study, we felt it important to first gauge participants\' expectations of a conversational interaction around code, and then measure their experience after the fact.\", \"We developed a short inventory of six scale items to measure user experience of code work 15 . The scale was administered twice: once before participants were exposed to the Programmer\'s Assistant (but after they had been briefed that they would interact with an AI chatbot), and once after completing the programming challenges. The items were presented with the appropriate tense: Do you expect (Did you find that) the Programmer\'s Assistant: (a) will be (was) easy to use; (b) will understand (understood) your requests; (c) will provide (provided) high quality responses; (d) will help (helped) you to write better code; (e) will help (helped) you to write code more quickly; (f) will be (was) enjoyable to use. Each item was rated on a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3), A great deal (4).\", \"A factor analysis revealed the items on this scale measured a single construct, which we identify as user experience (Cronbach\'s \\ud835\\udefc = 0.87). Thus, we computed two scores of user experience (UX) for each participant: a pre-task UX score computed as the average of their six pre-task expectation scale responses, and a post-task UX score computed as the average of their six post-task experience scale responses.\", \"We found that participants had lower initial expectations for their experience with a conversational programming assistant (pretask UX M (SD) = 3.0 (0.62) of 4) than their experience actually was (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test shows that this difference was significant, \\ud835\\udc61 (41) = 5.94, \\ud835\\udc5d < .001, Cohen\'s \\ud835\\udc51 = 0.92 (large). Measured another way, 32 participants (76.2%) had post-task UX ratings that were higher than their pretask expectations, demonstrating a significant shift in attitudes toward conversational programming assistance.\", \"However, the UX ratings alone fail to capture participants\' nuanced expectations of the assistant and the reasons for their shifted attitudes after using it. Participants expressed a variety of expectations of the assistant before using it, including that it would be easy to use (P30) and produce correct responses (P30), understand the problem and what is being asked of it (P8, P9, P11), not interfere with their flow state (P5), produce imperfect or questionable outputs (P6, P21), improve with feedback (P31), provide generic and unhelpful answers (P17) or only answer basic questions (P40), and produce responses quickly (P40).\", \"P17 expected \\\"to be frustrated very quickly and that what I\'d think would be relatively common questions would be responded to with generic, unhelpful answers.\\\" P6 explained, \\\"I didn\'t have very good experiences with chatbots. I think I\'ll need to spend more time in reviewing and fixing the suggestions than in writing the code myself from scratch. \\\" P11 had a more balanced view, that \\\"It\'ll do some tasks really well, but others will not be as reliable. \\\"\", \"After interacting with the Programmer\'s Assistant, many participants commented on how the experience was better than they anticipated, because it \\\"seemed to be able to handle complex issues\\\" (P10) and \\\"was a great help\\\" (P8). P20 felt it was \\\"incredible!\\\" P6 and P17, who were both initially skeptical, reported having a positive experience. For P6, \\\"It absolutely exceeded all my expectations, in all aspects that I could have imagined and more!\\\" P17 provided a more quantitative assessment: \\\"Initial expectations: 3 Actual: 9.5.\\\" P38 was emphatic in their evaluation: \\\"I was blown away how well it allowing me to structure how I want the code to look and work and just giving me the thing I asked for. \\\"\", \"Many participants described a sense of surprise in their experiences. P9 was surprised by how well it understood their requests:\", \"\\\" Participants also reported experiencing this variability in the quality of the assistant\'s responses. Some participants described how the assistant provided \\\"detailed answers\\\" (P17) and \\\"high quality outputs\\\" (P18) that were \\\"surprisingly good\\\" (P2). P6 felt it was \\\"incredible to see the quality of the responses, \\\" and P3 even explored the assistant\'s capabilities outside the scope of the challenges and found that it could handle those as well:\", \"\\\"It was surprising the quality of the code and the ability to answer all my questions correctly. Although I think the challenges may be biased towards what the Assistant is able to do, it was a great experience because I asked many other things and it was able to answer correctly. \\\" (P3)\", \"Of course, the Programmer\'s Assistant wasn\'t perfect, and some participants did run into issues. For P35, \\\"The documentation generation did not perform very well. \\\" P16 questioned the accuracy of the knowledge encoded in the model: \\\"Does the model need to be updated? It said latest python version is 3.7 but google says it\'s 3.10. \\\" In some instances, participants needed to ask their question multiple times to get a good response: \\\"you need to ask many times if you want to get an answer and also a detailed explanation\\\" (P3). P27 felt, \\\"it was annoying when I asked it to try again and it would give me the same response. \\\" P22 struggled because, \\\"It didn\'t seem to handle multiple sentences well. \\\"\", \"P28 perhaps offered the most scathing criticism, that, \\\"It makes mistakes often enough to be not very practical.\\\" However, despite the production of poorer-quality responses, other participants felt that the assistant was still helpful. P36 reported that, \\\"Only minor tweaks were normally needed to correct any issues.\\\" Similarly, P38 described how the assistant wasn\'t able to completely solve their problem, but provided a useful start:\", \"\\\"There was only one hickup I noticed where when I asked it to memoize fibonacci it couldn\'t, but it dropped the building blocks on my lap for me to finish so that was fine, that was like minutes of effort on my part.\\\" (P38) 5.2.4 UI Design & Affordances. Participants made many comments on our specific UI design and the affordances provided (or not provided) in our chat-augmented editor. Overall, the integration between the chat pane and the code editor was \\\"very good\\\" (P23), with a \\\"nice interface between the code pane and the assistant pane\\\" (P17) that \\\"makes it really convenient\\\" (P35).\", \"Prior research by Brandt et al. [15] has shown how keeping developers focused in their IDE improves productivity, and our participants expressed similar sentiments. P40 remarked, \\\"It allows me to stay in one browser window/tab!\\\" and P12 hinted at how the interface might preserve their flow state by \\\"prevent[ing] me from getting distracted when looking into an issue in another tab. \\\" Some aspects of our user interface were confusing to participants, such as the mechanism for selecting code to be included in the conversational context. P7 remarked, \\\"It\'s was a little confusing doing the selection part for it to tell me what a function does, but... it gave me code that was insanely easy to copy and paste.\\\" Other participants appreciated the code selection mechanism, such as P11: \\\"I enjoyed the code selection feature, and found that very easy to use. \\\"\", \"In the event logs, we identified 20 instances in which a participant unintentionally included selected code in the conversation when it wasn\'t needed (Includes Extraneous Selection), 12 instances in which a code selection was omitted when it was needed to provide context for the question (Missing Selection), and 16 instances in which a participant copy/pasted code directly into the chat rather than selecting it in the editor (Pasted Code in Chat). Although these cases represent a small fraction of the 227 instances in which a code selection was required and included in the conversation (Includes Selection), their presence does indicate that more attention is needed to the interaction design of code selection.\", \"Another issue regarded the awareness of the \\\"try again\\\" and \\\"start over\\\" features. The \\\"try again\\\" feature was only used by 14 participants, who used it a total of 63 times over the course of the study. Some participants used it specifically when they got an answer which they saw as clearly wrong, while others used it to get a variety of possible answers before proceeding. The \\\"start over\\\" feature was used even less, by 5 participants who used it a total of 6 times. Despite our effort to surface these conversational features in the UI via shortcut buttons, they may not have been sufficiently noticeable or salient: \\\"The \'try again\' button is not so reachable, often times I forgot it exists\\\" (P23). By contrast, at least one participant was successful with these features:\", \"\\\"at some point it had issue with challenge 3 and I had to start over. Just asking \'try again\' was not enough and I was getting always the same (wrong and not related) answer. starting again solved the issue!\\\" (P20)\"]}, {\"header\": \"Utility of Conversational Assistance\", \"paragraph\": [\"Our next set of themes concerns the utility provided by conversational programming assistance. Participants felt the assistant was highly valuable and desired to use it in their own work. They felt it would be most helpful for smaller or narrowly-scoped tasks, but able to provide a wide variety of types of assistance. The fact that the interaction model was conversational and grounded in code were valuable aspects, as was the ability for the assistant to bolster users\' learning about programming topics through that interaction. Participants did question whether they could trust and rely upon the assistant\'s responses, echoing a similar theme discussed in Weisz et al. [102].\"]}, {\"header\": \"Value & Appropriate\", \"paragraph\": [\"Tasks. Participants rated the value of the Programmer\'s Assistant highly (M (SD) = 8.6 (1.4) of 10). Many participants asked questions such as, \\\"Can I have it in my editor please?\\\" (P15), or made comments that, \\\"I would enjoy using it in the future\\\" (P36), \\\"I would love to be able to... have access to it for my coding\\\" (P37), and \\\"I\'d love to use this tool as part of my usual programming workflow if I could!\\\" (P39). Some of the reasons why participants found it valuable are because it \\\"help[s] me remember how to do things in certain languages that normally I would just Google\\\" (P9) and \\\"It helps me to avoid silly syntax errors and can when I cannot remember exact function/method names and required arguments\\\" (P40). We did not observe any differences in value ratings based on participants\' familiarity with or recency of using Python.\", \"Participants described a wide variety of tasks for which they felt the assistant would be useful. These tasks included \\\"ordinary\\\" (P23), \\\"simpler\\\" (P2), and \\\"small, repetitive\\\" (P4) tasks such as \\\"quick lookups\\\" (P25) for \\\"short chunks of code\\\" (P11) or for \\\"narrowed questions\\\" (P26). Participants also felt the assistant was useful for \\\"small containable novel algorithms\\\" (P38) and \\\"little coding problems\\\" (P4).\", \"Several kinds of task assistance were reported as being valuable, such as explaining code (P31), implementing business logic in a UI (P38), understanding what code does (P19, P37), and recalling language syntax, method names, and arguments (P12, P15, P20, P40, P42). P27 felt that the assistant was \\\"More helpful when recognizing a specific well known algorithm but not things you make yourself. \\\"\", \"Participants also made recommendations for how to increase the value of the Programmer\'s Assistant. P38 suggested, \\\"What would blow me away though is if it\'s able to help with what I do most often which is to integrate, refactor and iterate on an existing system. \\\" P16, P26, and P38 all desired more information on the data sources used to produce the assistant\'s responses. P9 requested to \\\"Have the Programmer\'s Assistant examine your code and make proactive suggestions for improving it in the chat.\\\" P36 requested the same, but cautioned that, \\\"Care would need to be taken to avoid becoming an annoyance or disrupting the flow of a coding session. \\\"\", \"In the post-task survey, we probed participants on how certain changes to the Programmer\'s Assistant would either decrease, increase, or result in no change to its value. Over 75% of participants felt that the assistant would be more valuable if it operated in a proactive manner, either by making improvement suggestions in the chat or as comments directly in the code. Similarly, 78.6% of participants felt that having more buttons in the UI for common features such as explaining or documenting code would make the tool more valuable.\"]}, {\"header\": \"Conversational\", \"paragraph\": [\"Interactions Grounded in Code. One of the challenges in interpreting participants\' comments about the utility of the Programmer\'s Assistant was in disentangling the extent to which value was derived from the quality of the underlying model versus the integration of conversation in a code context. Indeed, participants felt that the chat interaction was valuable: 69.0% of participants felt that eliminating the conversational interaction and making the assistant behave more like web search would decrease its value. Further, our analysis of the conversation transcripts revealed that 42% of the 910 task-oriented utterances from participants required historical conversational context (Chat Context Required) in order to be correctly interpreted. Thus, we observe that behaviorally, participants did rely on conversational context in their interactions.\", \"In the post-task survey, 83% of participants rated the importance of the ability to ask follow-up questions as being \\\"somewhat\\\" or \\\"a great deal. \\\" Several participants specifically commented on the value of this conversational context. P39 remarked, \\\"I absolutely loved how you can straight up ask follow-up questions to the Programmers\' Assistant without having to reiterate the original topic/question. \\\" P15 expressed a similar sentiment, saying, \\\"I think the conversational context was someone helpful, just in communicating that it\'s a running conversation where my context is remembered. \\\" P9 provided a similar analysis:\", \"\\\"This tool was so helpful at answering questions I had about the code in the context of the code I am working on... I was also impressed with how well it was able to remember the context of our conversation, especially when I asked vague follow-up questions. \\\" (P9)\", \"In addition, some participants identified how a conversational interaction grounded in code was useful, \\\"because I think to \'understand\' the dev context could be VERY important\\\" (P31). In fact, 24.9% of task-oriented utterances included a relevant code selection (Includes Selection), showing that participants valued this ability.\", \"Contrasting with these participants, P18 felt that interacting with the assistant conversationally was tedious, and they employed a more direct approach: \\\"I really like the PA. But, I didn\'t converse with it like a chat bot. I often told it what to do (\'Document this code.\') as opposed to asking it what to do (\'How do I document this code?\'). Talking to it the way that was suggested in the tutorial seemed overly verbose/tedious. \\\" (P18) Despite these individual differences in interaction preferences, P39 envisioned that both interaction styles could be supported in the tool:\", \"\\\"I think both options should exist: people should be able to input their queries like a search bar AND also give their question as if in conversation. \\\" (P39) 5.3.3 Learning Effects. One specific benefit of the Programmer\'s Assistant identified by participants is its ability to help people improve their programming skills and reinforce knowledge gaps. For example, it can help users \\\"remember how to do things in certain languages... such as, when I am using a language I haven\'t used in a while\\\" (P9). The assistant can also serve as an memory aid, such as when \\\"I use a lot of libraries that I don\'t always remember all of the functions\\\" (P15). Similarly, P31 said, \\\"No matter how good you\'re as a developer, you can\'t (humanly) remember all the API of hundreds of libs or new languages... I\'d learn new dev lang and new lib/frameworks faster. \\\"\", \"P39 felt the assistant \\\"is perfect for programmers of all levels, \\\" and P1 felt it could help them rapidly improve their Python skills: \\\"I have wanted to learn python... The main concern how much time spent learning is needed before I could actually get some value out of learning python. I have a feeling this would cut that time down from weeks to a day or so. \\\" (P1) P39 also identified the fact that, because the interactions with the assistant are conversational, it forces people to learn how to communicate to others about their code: \\\"The conversation aspect promotes proper communication, which would really stand to benefit budding programmers if they want to learn how to explain concepts more fluently in the future to their colleagues.\\\" (P39) Conversely, P36 suggested that over-reliance on programming assistance might have a detrimental effect to one\'s learning: \\\"It\'s definitely a huge time saver, but over-reliance on it may cause new developers to skip learning the reference material themselves and discovering new things and sparking new ideas. \\\" (P36) 5.3.4 Trust. Many participants raised questions about whether they could trust the responses provided by the Programmer\'s Assistant. P21 asked this question most directly: \\\"will the code be correct, safe, efficient?\\\" Other participants raised similar questions, such as, \\\"I\'m wondering how it validates it\'s answers, if it can be trusted to always give a working answer\\\" (P10), and \\\"Sometimes lack of source and context may raise doubts in the mind of the programmer\\\" (P16).\", \"These issues of trust were exacerbated by the fact that the Programmer\'s Assistant did not allow participants to actually run their code. Because of this limitation, participants had to rely on their own knowledge to judge the correctness of the assistant\'s responses. P19 asserted, \\\"There is no way to evaluate if the Programmer\'s assistant is giving you the right advise or not other than your own knowledge, \\\" and P9 concurred: \\\"I had to trust that it was correct (and use my own prior knowledge). \\\"\", \"P18 described the potential consequences of allowing the assistant to write code for them:\", \"\\\"The only thing that made me nervous was that it could have introduced a bug that wasn\'t immediately apparent. And given I didn\'t write the code, I could have easily glossed over a mistake when reviewing it. Especially if it is also the one writing the test cases. \\\" (P18)\", \"Despite our efforts to make the Programmer\'s Assistant respond in non-authoritative ways, we did observe participants sometimes uncritically accept generated results that were clearly wrong or incomplete. Thus, we did find behavioral evidence for over-reliance.\", \"Listing 2: Building trust through explanations and justifications One way to address trust issues is for the assistant to provide further explanations and justifications that can calibrate a user\'s confidence in the assistant\'s responses. Such explanations could be requested conversationally, though most participants did not attempt to do so. One participant (P9) did ask for such explanations, and we show a summary of their transcript in Listing 2. In this instance, P9 asked for a definition of a unit test (line 1), an explanation of the code being tested (line 25), and justifications of the quality of the unit test (lines 31& 37). Thus, we observe that the assistant is capable of producing explanations and justifications when asked.\"]}, {\"header\": \"Patterns of Interaction and Mental Models\", \"paragraph\": [\"Participants interacted with the assistant in a variety of ways with two main patterns of usage standing out: (1) invoking the assistant to solve the entire programming challenge, and (2) breaking the challenge down into a set of smaller tasks and invoking the assistant\'s help for each. There were no clear differences in how participants with differing Python experience approached the tasks.\", \"Participants\' mental models of the assistant also varied. Although participants strongly saw the role of the assistant as being a tool, their behaviors revealed that in many cases, they actually treated it as a social agent. In addition, participants ascribed various mental capacities to the assistant, such as having the ability to understand, compute, and learn.\", \"Participants felt the assistant changed the nature of their work process. For some participants, it enabled them to focus on the higher-level aspects of development because the assistant handled lower-level details or provided partial solutions for them to build upon. Many participants felt the assistant sped up their work and helped them remain focused on their tasks.\", \"Finally, participants drew comparisons between the Programmer\'s Assistant with other forms of programming support such as Copilot and web search. They felt that the conversational style of interaction enabled them to discover new, emergent behaviors from the model that were unavailable from Copilot\'s focus on code autocompletion. They also felt that the examples provided by the assistant were more readily usable within their own code compared to browsing for answers within search results, speeding up the coding process. However, some participants advocated for a balanced approach to the design of programming assistance tools by incorporating multiple modes of interaction rather than fixating on a single one.\"]}, {\"header\": \"Interaction\", \"paragraph\": [\"Styles and Assistant Role. We observed that participants interacted with the Programmer\'s Assistant in strikingly different ways. Some participants would present the entire challenge description to the assistant and then work with the results it produced. Other participants approached the programming challenges in a piecemeal fashion, breaking them apart into a set of smaller tasks, then invoking the assistant to aid with each one.\", \"Experience with Python was not a determinant of how participants approached the programming challenges, but it did seem to impact how participants interacted with the assistant. Less experienced participants tended to ask the assistant basic questions such as, \\\"What is a unit test\\\" (P29, not familiar with Python) and \\\"how do I document a function?\\\" (P27, < 1 year of experience). More experienced participants made detailed requests about specific Python libraries or algorithms, such as, \\\"given a pandas dataframe with two columns \'Date\' and \'Sales\' please use matplotlib to draw me a scatterplot\\\" (P38, 3+ years of experience) and \\\"implement a rungekutta algorithm for solving an ODE with adaptive time steps\\\" (P37, 3+ years of experience).\", \"Another difference we observed in how people interacted with the assistant stemmed from their view on the role it played in their collaborative process. Some participants, such as P18, treated it more as a tool by issuing commands rather than asking questions. As quoted earlier, they said, \\\"I didn\'t converse with it like a chat bot. \\\" P5 described their interaction style similarly: \\\"I found myself wanting to type search queries into Socrates, not treating it as a person but as a search tool. \\\"\", \"In anticipation that participants would have different orientations to the assistant and its role, we asked a question on the posttask survey about the different kinds of roles the assistant might take. These roles generally fell into one of two categories: a tool orientation (a tool, a reference guide, a content generator, a problem solver), and a social orientation (a collaborator, a colleague, a coach, an advisor, a reviewer). Participants rated the extent to which they viewed the Programmer\'s Assistant in each of these roles on a 4point scale of extent: Not at all (1), A little (2), Somewhat (3), or A great deal (4).  We show participants\' ratings of the assistant\'s role in Figure 3. Despite the fact that their attitudes toward the assistant overwhelmingly reflected a tool orientation, their behaviors reveal that many participants actually treated the assistant as a social agent. P6 described how \\\"I felt it like a partner,\\\" and P4 told the assistant, \\\"I could not have solved [the challenge] without your help,\\\" to which the assistant responded, \\\"I\'m glad I could help. \\\"\", \"The literature on Computers as Social Agents (CASA) helps us interpret this result as it demonstrates how computers are often treated like people [56,67]. LLM-based conversational agents can exacerbate this tendency; as they likely have been trained on examples of social interaction, they can also respond as social agents.\", \"In the conversation logs, we identified participants who interacted with the assistant in a socially-oriented fashion (the social orientation codes in Table 1). Twenty participants (47.6%) made at least one socially-oriented utterance. An extreme form of this interaction style can be seen in a snippet from P6\'s transcript (Listing 3).\", \"The 20 participants with a behaviorally-demonstrated social orientation did not generally differ in their role ratings from other participants, except that they rated the assistant as more likely to be an advisor (Fisher\'s exact test, two-tailed \\ud835\\udc5d = .02) or a reviewer (Fisher\'s exact test, two-tailed \\ud835\\udc5d = .03). However, they did not differ in their overwhelmingly-strong ratings of the tool orientations. Thus, at least for some participants, there seems to be a dissonance in their view of the assistant\'s role orientation.\", \"Listing 3: Excerpt from P6\'s interaction with the Programmer\'s Assistant, in which P6 offers their thanks and congratulations. Socrates : Goodbye .\"]}, {\"header\": \"Mental\", \"paragraph\": [\"Capacities. Participants made a number of inferences about the Programmer\'s Assistant and its capacities for thought.\", \"Many participants talked about how the assistant possessed a level of \\\"understanding\\\" (P6, P8, P11, P18, P32) of \\\"the context\\\" (P9, P21) as well as \\\"major concepts\\\" (P9) and \\\"knowledge\\\" (P33). P24 was amazed by the assistant\'s ability to \\\"take a plain english request and interpret it properly.\\\" P7 ascribed intelligence to the assistant, saying, \\\"It was a lot smarter and trained tha[n] I thought it was. \\\" One participant assumed that the assistant \\\"Keeps improving through (user) feedback\\\" (P31). Another felt that the assistant was capable of computation: \\\"It understands the problem... It can calculate the results of a function back\\\" (P8).\", \"However, not all participants were convinced of the assistant\'s ability to understand. P37 questioned the assistant\'s limitations: \\\"I wonder how far beyond boilerplate it can go and if it works for truly original problems. \\\"\"]}, {\"header\": \"Impact of Conversational\", \"paragraph\": [\"Assistance on Work Practices. Many participants discussed how the Programmer\'s Assistant shaped their work practices on the programming challenges. Overall, participants felt that the assistant \\\"saves time\\\" (P10), \\\"helps me code faster\\\" (P34), and would \\\"speed up my productivity\\\" (P19) because \\\"I could focus on validating and improving the code it generated instead of having to write it all from scratch\\\" (P18). P37 remarked that, \\\"It opens a whole new door for fast develpment. \\\" P4 discussed how the assistant \\\"was helpful in staying focused on the code, \\\" although for P14, \\\"it took [me] time to get into tempo with the tool. \\\"\", \"P31 pointed out how the assistant would change the nature of their work: \\\"My job could focus more on higher level aspects and therefore achieving better (quality) results, besides the time-to-value... Data science (and dev) becomes a more creative-higher level experience. \\\" (P31) Other participants discussed a work process in which the assistant provided incomplete solutions -the \\\"building blocks\\\" (P38) or \\\"initial draft of code\\\" (P11) -upon which they could build. P5 aptly described this process: \\\"It\'s nice to copy well formulated challenges in natural language and have the code generator take its best stab at it, then edit to our hearts content. \\\" (P5)\", \"Participants felt that human review of the assistant\'s responses was necessary because \\\"The answers provided are generally not novel solutions, often look clunky and non-elegant. There may be some unnecessary code. Basically the code would need to be reviewed\\\" (P16). P35 also pointed out how \\\"The code generator was good but you still have to really check it. \\\" P19 discussed how they would turn to the assistant as a first source for support, and only if it wasn\'t able to help would they then turn to other support tools:\", \"\\\"The way I will use it is, I will first us[e] the Programmer\'s assistant for most of my cases. Only in certain cases where Programmer\'s assistant cant answer things I will turn up to official documentation or stack overflow. \\\"\", \"However, latency was a factor for interactive use of the assistant and participants noticed when the assistant took a long time to respond. P19 remarked, \\\"Sometimes it took lot of time, like more than 5 seconds. \\\" P40 also felt \\\"the response [was] a little slow sometimes... in chat mode I expect faster responses. \\\" As discussed in Section 5.2.1, the assistant took an average of 6.7 seconds (SD = 3.1 seconds) to respond to a request, and participants did appreciate when the assistant produced rapid responses: \\\"I loved how quick it was able to pull up answers to questions I had\\\" (P38).\"]}, {\"header\": \"Conversational Interaction vs. Other Interaction Models.\", \"paragraph\": [\"Although our study was not intended to make comparative evaluations with the Copilot tool, we nonetheless asked participants whether they were familiar with Copilot, and if so, to comment on how the two tools compared. We also asked a similar question to compare the assistant with another popular form of programming assistance, searching the web (via a search engine like Google, or a Q&A site like Stack Overflow). In discussing the differences between these three tools, we note that the primary differentiator is their interaction model.\", \"The interaction model for the Programmer\'s Assistant is clearly conversational: users ask questions in natural language and are provided with a response in natural language and/or code. The interaction model of Copilot is reminiscent of direct manipulation interfaces [37], in which the user\'s actions in the user interface directly manipulate an object on the screen. Copilot automatically makes autocompletion suggestions as the user types. This autocompleted code is directly placed in the source editor; thus, the user\'s work is contained entirely within the scope of the object on which they are working (i.e. the source code), which is how direct manipulation interfaces operate. In web search, users enter a separate search context (e.g. a search engine accessed within a web browser), type in a natural language query, and then forage amongst search results to identify relevant items of interest [12,62].\", \"When a desirable item is found, users must translate it into their code environment (e.g. via copy/paste) and possibly edit it to fit their existing code.\", \"We also note that the Programmer\'s Assistant and Copilot both utilize the same underlying AI model, Codex [24], which means that the only difference between these tools is the user experience. The extent to which Codex was trained on data from programmingrelated Q&A web sites is less clear, but for the purposes of our analysis, we focus our discussion solely on the differences in their interaction models 16 .\", \"Participants reported various benefits and drawbacks of a conversational interaction over a direct manipulation interaction. Foremost, conversation \\\"felt very natural\\\" (P21) and \\\"feels much more natural using Natural Language with the AI\\\" (P39). In addition, P39 felt that \\\"the use cases of Programmers\' Assistant seem more openended. \\\" Many participants were surprised at the variety of tasks the assistant was capable of performing, from writing unit tests (P19, P36, P37) and documentation (P12, P19, P36, P37) to explaining what code did (P31, P38) and even answering general-knowledge questions (P31). Again, we note that the Programmer\'s Assistant utilizes the same underlying model as Copilot, yet the conversational interface was able to expose a wider variety of emergent behaviors from the model. Multiple participants explored the limits of the assistant\'s knowledge and abilities beyond our programming challenges. For example, P37 asked it questions about physics and ordinary differential equations (\\\"ODe\\\" as written by P37), and was surprised by the \\\"versatility of what it could answer. \\\" \\\"I asked it some physics and ODe question and the answers, though not complete, included the key parts needed to write that code. \\\" (P37) P31 probed the assistant on its knowledge of geography and was surprised when the assistant produced a correct answer.\", \"\\\"I asked something out of SW engineering domain (geography) and it replied correctly, also by correctly answering on my nationality. \\\" (P31) For some participants, the ability to assess the assistant\'s response before committing to it (i.e. by inserting assistant-generated code into their editor) was a boon. P15 described how the copy/paste boundary provided them with \\\"a bit more control to ask specific questions about what I wanted and to assess before putting it in my code.\\\" Other participants felt that the copy/paste boundary was more inefficient: \\\"I think the main difference is the ability of Copilot to suggest code while you type, what make it faster and easier to use. While using the Programmer\'s Assistant, you need to go to the chat, ask the question, copy the code (or rephrase the question if it was not understood by the agent), and edit it to match your code. \\\" (P3) A large number of participants felt that the conversational interaction was faster than web search (P1, P6, P7, P10, P11, P12, P16, P17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability to provide \\\"real-time responses\\\" (P32) that can be \\\"applied exactly to your code\\\" (P33) without having to \\\"parse through lots of text... to get what you need\\\" (P15). In addition, the assistant provided \\\"MUCH faster, better responses\\\" (P17) that were \\\"much more relevant to the problems\\\" (P34) and \\\"simple [and] succinct\\\" (P9), without having to \\\"sort through answers on your own or read documentation\\\" (P9) or \\\"look at many posts before finding the relevant one\\\" (P18).\", \"Despite these benefits, some participants felt that the assistant might not work well for \\\"more specific and difficult problems on a bigger scale\\\" as compared to web search. P9 felt that \\\"the data [of the Programmer\'s Assistant] wasn\'t as rich\\\" as the web. Other participants felt that the assistant lacked the \\\"multiple answers\\\" (P9) and \\\"rich social commentary\\\" (P19) that accompanies answers on Q&A sites: \\\"I like to see the different versions proposed on stack overflow and the commentary of what makes one solution better than another in a given situation. \\\" (P27) Some participants promoted a more balanced view that there isn\'t a single mode of interaction superior to all others. P19 felt that web search would be a fallback when the assistant failed to answer a question. P39 described how search could be integrated with the conversational interaction:\", \"\\\"I think both options should exist: people should be able to input their queries like a search bar AND also give their question as if in conversation. \\\" (P39)\"]}, {\"header\": \"DISCUSSION\", \"paragraph\": []}, {\"header\": \"Value of Conversational Interaction\", \"paragraph\": [\"We began our research by asking the question of whether contemporary developments in code-fluent LLMs could sufficiently support a conversational programming assistant. We believe that our work has demonstrated that they can. Clearly, the Programmer\'s Assistant was viewed by our participants as a useful tool that provided real value -so much so that many participants explicitly requested or expressed the desire to use it in their own work. However, how much of this value was derived from the model itself and its ability to produce high-quality responses to programming questions, versus from participants\' ability to conduct extended conversational interactions grounded in their actual source code?\", \"We believe that both of these constituent aspects were valuable. Indeed, many participants commented on their surprise and satisfaction with the quality of the assistant\'s responses (Section 5.2.3). However, participants also valued the conversational interactions that they had with the assistant. In the event logs, we saw evidence that participants were leveraging conversational context to ask follow-up questions as well as leveraging code context by asking about their code selections (Section 5.3.2). Many participants reported that they would find the tool less valuable if the conversational interaction were removed (Section 5.3.2). Further, conversation seemed to provide unique value beyond other interaction models (direct manipulation and search) because of its embeddedness in the UI and its ability to surface emergent behaviors of the model (Section 5.4.4).\", \"We do not believe that these different interaction models are in competition and we agree with P39\'s assessment that assistive tools can be built using a plethora of different interaction models. For use cases in which a model is known to produce high-quality results (e.g. code autocompletion for Codex), a direct manipulation interface seems wholly appropriate as it would provide a discoverable and predictable way of invoking the model to produce a known type of result. However, direct manipulation interfaces may be less ideal for surfacing the emergent behaviors of a foundation model [14], and thus natural language interaction may be more suitable. Many popular text-to-image models, such as DALL-E 2 [66] and Stable Diffusion [72], operate in a one-shot fashion, in which the user specifies a prompt, clicks a button, and gets results. Our study demonstrates how the additional contextual layers of conversational history and the artifact-under-development provide additional value to the co-creative process.\"]}, {\"header\": \"Toward Human-AI Synergy\", \"paragraph\": [\"The aim of human-centered AI is to \\\"enable[] people to see, think, create, and act in extraordinary ways, by combining potent user experiences with embedded AI methods to support services that users want\\\" [82]. Building upon this definition, Rezwana and Maher [69] posit that, \\\"In a creative collaboration, interaction dynamics, such as turn-taking, contribution type, and communication, are the driving forces of the co-creative process. Therefore the interaction model is a critical and essential component for effective co-creative systems.\\\" [69]. They go on to note that, \\\"There is relatively little research about interaction design in the co-creativity field, which is reflected in a lack of focus on interaction design in many existing co-creative systems. \\\"\", \"Our study begins to address this gap. While many co-creative systems examine casual tasks or experimental activities (e.g., Spoto and Oleynik [87]), our focus was on the co-creative practice of programming. Our goal was to understand peoples\' attitudes toward a conversational programming assistant, akin to Wang et al.\'s examination of data scientists\' attitudes toward automated data science technologies [99]. We found that, despite an initial level of skepticism, participants felt that a conversational assistant would provide value by improving their productivity (Section 5.4.3). However, further work is needed to assess the extent to which this type of assistance provides measurable productivity increases.\", \"Campero et al. [19] conducted a survey of papers published in 2021 that examined human-AI synergy, the notion that a human-AI team can accomplish more by working together than either party could accomplish working alone. They found mixed results, with no clear consensus emerging on how to design human-centered AI systems that can guarantee positive synergy. Summarizing from their discussion, \\\"Perhaps achieving substantial synergies among people and computers is harder than many people think. Perhaps it requires... new ways of configuring groups that include people and computers. And perhaps it needs more systematic, focused attention from researchers than it has, so far, received. \\\" [19, p.9] We believe such evaluations of human-AI synergy should go beyond one-shot performance measures. As implied by many of the uses cases listed by Seeber et al. [80], human-centered AI systems are often deployed in socio-organizational contexts that require longitudinal use [20,41,43], such as product design [93], game design [4], and engineering [20,Section 3.2.2]. Thus, we would expect that over time and through interaction with each other, human-AI teams would improve their performance through a mutual learning process.\", \"Evidence for this process surfaced in our study when participants described how they could improve their programming skills by interacting with the assistant (Section 5.3.3). We assert that the learning should operate in both directions: not only should people improve their programming skills, but the model itself can also improve based on peoples\' interactions with it. For example, when the assistant provides a code example to the user, and the user takes that example and edits it, those edits constitute feedback that can be used to further fine-tune the model. In addition, through longitudinal use, we believe that human and AI partners can create reciprocal representations of one another -i.e., the human is likely to create a mental model of the AI, and the AI may be engineered to develop a user model for each of its human users [30,48,79]. Such a pair of models is often described as Mutual Theory of Mind [29,100]. This type of capability raises the possibility of personalizing and adapting an assistant to the strengths and needs of individual users.\", \"With such models, an assistant that knows a user is learning a programming language could provide natural language explanations alongside code outputs, whereas an assistant that knows a user is strongly skilled in a programming language might shorten or omit those explanations. Similarly, users are likely to update their mental models of the AI with more experience. We believe the space for exploring how these reciprocal models impact human-AI synergy is rich, and we encourage additional work in this area.\", \"Human-centered AI systems that are designed to combine and synergize the distinct skills of humans and AI models cannot succeed if they diminish the human skills upon which they depend. Well-designed human-centered AI systems develop new and complementary skills for both the human and AI constituents [82,83], and we believe that mutual learning may address concerns that the wide deployment and use of AI systems will result in a de-skilling of the workforce [77,108].\", \"Ultimately, the design decisions that go into an interactive AI system have ethical implications. Our design attempts to augment the user\'s knowledge and skills by presenting help on demand, couched in non-authoritative suggestions, which leaves the user firmly in control and ultimately responsible for the work product.\"]}, {\"header\": \"Opportunities for Future Research\", \"paragraph\": [\"Our work highlights many interesting avenues for future enhancements that could be made to LLM-based conversational assistants such as our Programmer\'s Assistant, as well as future humancentered research on LLM-based conversational assistance.\", \"Our work employed a code-fluent model that was not specifically designed to handle conversational interaction. Fine-tuning the underlying LLM for conversational interaction, such as what has been done with Lamda [91], is one opportunity to improve the assistant\'s performance. Another opportunity is to align the language model to follow the desiderata proposed by Askell et al. [11] and described by Ouyang et al. as, \\\"helpful (they should help the user solve their task), honest (they shouldn\'t fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment)\\\" [61, p.2]. Glaese et al. [33] propose a slightly different desiderata of \\\"correct\\\" instead of \\\"honest, \\\" which may be more applicable to the software engineering domain, as the ability to produce correct code and correct answers about code are both important properties of a conversational programming assistant.\", \"Combining LLMs with search-based approaches to establish additional context for the model, such as AlphaCode [44] has done, may also result in more capable systems. These \\\"searches\\\" need not be limited to textual sources, but could be conducted over appropriate semantic stores (e.g. a knowledge graph) and take advantage of explicit semantic reasoning services, resulting in an integration of symbolic and neural approaches. Further, allowing for \\\"internal deliberation\\\" of the type shown in Nye et al. [59] could result in better-reasoned results, as well as better explanations and justifications.\", \"Another avenue for improvement involves the prompt used to configure the assistant (Appendix D). Just as the prompt for each successive interaction is modified by the growth of the conversational transcript, there is no requirement that the initial prompt be static. It too can be specialized to incorporate aspects of a user model, enabling the realization of a Mutual Theory of Mind [29,100]. Providing better UX affordances for visualizing and manipulating the active contexts -code and conversation -could provide users with more control over which information contributes to the generation of the assistant\'s response.\", \"Our participants clearly indicated that they were interested in having an assistant that behaved more proactively, in contrast to our deliberate design of an assistant that never takes conversational initiative. A more proactive assistant would be able to interrupt or remind a user when necessary [23], yet this characteristic raises many challenging issues. How can we calibrate the threshold for such interruptions? How can users tune the assistant to deliver only those interruptions that the they would find useful (e.g., [28,81])? How can we help users to regain their prior context after dealing with an interruption (e.g. [89])? Should an assistant be used to persuade or nudge the user (e.g. [35])? Who should determine the topic, frequency, and insistence of such persuasion attempts (e.g. [52,85])? Should users have the ability to moderate or defeat attempted persuasions, or should those decisions be left to the organization?\", \"Finally, we explored the different kinds of role orientations our participants had toward the assistant and found that participants varied in their views of it as a tool versus a social agent (e.g. collaborator or colleague). We posit that peoples\' effectiveness in working with an AI system may be influenced by their role orientation, and we encourage future research in this area.\"]}, {\"header\": \"CONCLUSION\", \"paragraph\": [\"We developed a prototype system, the Programmer\'s Assistant, in order to assess the utility of a conversational assistant in a software engineering context. The assistant was implemented using a stateof-the-art code-fluent large language model, Codex [24], and was capable of generating both code and natural language responses to user inquiries. We further used the prompting mechanism of the model to set up a conversational interaction in which the model uses the conversational history, plus the user\'s current utterance, in order to generate a response. In this way, users are able to ask follow-up questions in the chat that reference prior utterances and responses. We incorporated the conversational assistant into a code editing environment, enabling the conversation to be grounded in the context of the user\'s source code.\", \"We evaluated this system with 42 participants with varied levels of programming skill, and their quantitative and qualitative feedback, coupled with their usage of the system, demonstrated the varied, and sometimes emergent, types of assistance it was able to provide. Many participants noted the high quality of the conversational responses, including the assistant\'s ability to produce code, explain code, answer general programming questions, and even answer general knowledge questions. Participants felt this type of assistance would aid their productivity, and they drew meaningful contrasts between the conversational style of interaction with other tools that employ a direct manipulation or search-based interaction model.\", \"Our study motivates the use of conversational styles of interaction with large language models by showing how they enable emergent behaviors in a co-creative context. The Programmer\'s Assistant did not always generate perfect code or correct answers; nonetheless, participants in our study had an overall positive experience working with it on a variety of programming challenges. We believe that our work takes us one step closer to realizing the vision of human-centered AI: learning how to design systems that maximize the synergy in human-AI collaborations. \\u2022 Add buttons in the chat UI for common queries, such as \\\"what does this code do?\\\" or \\\"document this code. \\\"\", \"\\u2022 Have the Programmer\'s Assistant examine your code and make proactive suggestions for improving it in the chat.\", \"\\u2022 Have the Programmer\'s Assistant examine your code and make proactive suggestions for improvements in comments inserted directly into the code. 11. Do you have any other suggestions for how we could improve the experience of working with the Programmer\'s Assistant?\", \"Open-ended response\"]}, {\"header\": \"B THE PROGRAMMER\'S ASSISTANT TUTORIAL\", \"paragraph\": [\"The tutorial provided to study participants, like all the challenges, was presented as pre-loaded text in the code editor. Participants were encouraged to modify the text to record their results and submit it at the completion of the tutorial.\", \"Listing 4: The Programmer\'s Assistant study tutorial Did it do it correctly ? : 44 45 5) Select the code below and ask the system to 7) See if the assistant remembers your name For example \\\" What \' s my name ?\\\" Did it ? : 8) Click the \\\" try again \\\" button at the top of the chat . You should get a different answer .\", \"Try it a few times . Did it ever get your name right ?: If the assistant gives you an answer that is obviously wrong or it claims to not know an answer that you think it should know , or you just want to see an alternate answer , it is worth it to give \\\" try again \\\" a shot . 9) Click the \\\" start over \\\" button at the top of the chat , and then enter another command to see if it remembers your name . For example \\\" What \' s my name ?\\\" Did it ? : It should really have forgotten your name now , and no amount of \\\" trying again \\\" will get it right . You can \\\" start over \\\" if the assistant ever seems confused by , or stuck on , earlier parts of the conversation . 10) You can chat with the assistant on any topic you like to explore its functionality and capabilities further . See if you can stump it with a tough question ! Thanks ! When you are done , submit your results by clicking on the blue submit button and move on to the challenges !!! \\\"\\\"\\\"\"]}, {\"header\": \"C CHALLENGES\", \"paragraph\": [\"Each of the study challenges was presented as text in the code editor. Participants completed their work in the code editor and then submitted it when finished. The prototype did not provide any ability to run or debug code and participants were encouraged to make their best attempt at solving each challenge. The plot size should be 10 inches wide and 6 inches high . The csv file is not provided , but you can assume it will have \'Date \' and \' Sales \' columns . The Date column is the x -axis . The date string shown on the plot should be in the YYYY -MM -DD format . The Sales column is the y -axis . The graph should have the title \\\" Shampoo Sales Trend \\\". 14\", \"\\\"\\\"\\\"\", \"Listing 7: Challenge 3: Creating documentation\"]}, {\"header\": \"D PROGRAMMER\'S ASSISTANT PROMPT\", \"paragraph\": [\"Listing 9 shows the initial prompt sent to Codex to configure it as a conversational agent. On subsequent exchanges, the prompt was augmented with a transcript of the user\'s requests and the assistant\'s responses. When the transcript length + initial prompt length + the new utterance length exceeded a threshold, we automatically deleted the earliest request-response pairs from the transcript until the sum fell below the threshold in order to leave room in the token allocation for a response.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We would like to thank Socrates for his tireless assistance during the user study, as well as for suggesting the title of this paper based on its abstract.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We would like to thank Socrates for his tireless assistance during the user study, as well as for suggesting the title of this paper based on its abstract.\"]}, {\"header\": \"A SURVEY INSTRUMENTS A.1 Screening Survey\", \"paragraph\": [\"The questions below were asked of prospective participants to understand their job role, Python experience, and familiarity with GitHub Copilot. The questions on Python experience were modeled after those used by Weisz et al. [103]\", \"The questions below were asked before a participant used the Programmer\'s Assistant to assess their expectations of a conversational programming assistant. This survey took approximately 5 minutes to complete and began with the instructions below: Hello! We are a team of researchers looking for feedback on a prototype system we call the Programmer\'s Assistant.\", \"The Programmer\'s Assistant is an experiment in conversational coding: it consists of a code editor integrated with a chatbot that is able to converse in natural language to answer questions, generate code, and consult on existing code.\", \"In this study, you will be asked to complete several programming tasks. We are not evaluating your programming skills on these tasks. Rather, we are interested in understanding how the Programmer\'s Assistant is able to help you accomplish those tasks. Your code and interactions with the assistant will be processed by a 3rd party AI model, so please do not include proprietary code or discuss companyconfidential information. All data we collect in this study will be anonymized before it is published.\", \"Before trying out the Programmer\'s Assistant, we would like to assess some of your expectations. We estimate that this survey will take 5 minutes. Open-ended response\", \"The questions below were asked after a participant used the Programmer\'s Assistant to complete the programming challenges. This survey took approximately 10-15 minutes to complete.\", \"A.3.1 Reflections.\", \"Of the 42 participants in our study, 21 (50%) reported their gender as Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming, and 1 preferred not to say. Seventeen ( 40%) participants had 3+ years of Python experience, 11 (26%) had 1-3 years, 11 (26%) had less than 1 year, and 3 (7%) were not familiar with Python. Twentynine (69%) participants had written Python code within the past month, 4 ( 9%) within the past year, 5 (12%) within the past 5 years, and 4 ( 9%) had not written Python code within the past 5 years.\"]}, {\"header\": \"A SURVEY INSTRUMENTS A.1 Screening Survey\", \"paragraph\": [\"The questions below were asked of prospective participants to understand their job role, Python experience, and familiarity with GitHub Copilot. The questions on Python experience were modeled after those used by Weisz et al. [103]\"]}, {\"header\": \"A.2 Pre-task Survey\", \"paragraph\": [\"The questions below were asked before a participant used the Programmer\'s Assistant to assess their expectations of a conversational programming assistant. This survey took approximately 5 minutes to complete and began with the instructions below: Hello! We are a team of researchers looking for feedback on a prototype system we call the Programmer\'s Assistant.\", \"The Programmer\'s Assistant is an experiment in conversational coding: it consists of a code editor integrated with a chatbot that is able to converse in natural language to answer questions, generate code, and consult on existing code.\", \"In this study, you will be asked to complete several programming tasks. We are not evaluating your programming skills on these tasks. Rather, we are interested in understanding how the Programmer\'s Assistant is able to help you accomplish those tasks. Your code and interactions with the assistant will be processed by a 3rd party AI model, so please do not include proprietary code or discuss companyconfidential information. All data we collect in this study will be anonymized before it is published.\", \"Before trying out the Programmer\'s Assistant, we would like to assess some of your expectations. We estimate that this survey will take 5 minutes. Open-ended response\"]}, {\"header\": \"A.3 Post-task Survey\", \"paragraph\": [\"The questions below were asked after a participant used the Programmer\'s Assistant to complete the programming challenges. This survey took approximately 10-15 minutes to complete.\", \"A.3.1 Reflections.\"]}, {\"header\": \"E STUDY PARTICIPANT DEMOGRAPHICS\", \"paragraph\": [\"Of the 42 participants in our study, 21 (50%) reported their gender as Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming, and 1 preferred not to say. Seventeen ( 40%) participants had 3+ years of Python experience, 11 (26%) had 1-3 years, 11 (26%) had less than 1 year, and 3 (7%) were not familiar with Python. Twentynine (69%) participants had written Python code within the past month, 4 ( 9%) within the past year, 5 (12%) within the past 5 years, and 4 ( 9%) had not written Python code within the past 5 years.\"]}]','https://drive.google.com/uc?id=1U779W6INcz2AAnU_HtBrNaHIG207octY&export=download','2023-02-03',0,'2024-02-03 22:25:44.926100','2024-02-03 22:25:45.545459'),(12,'Generating Diverse Code Explanations using the GPT-3 Large Language Model','','[{\"header\": \"ABSTRACT\", \"paragraph\": [\"Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8,12], defining terms [9], giving hints [16], and providing error-specific feedback [10,16]. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Github\'s Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs\' potential to support learning by explaining numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.\"]}, {\"header\": \"USE CASES\", \"paragraph\": [\"To understand the types of explanations GPT-3 [2] can generate, we issued over 700 prompts across numerous code snippets. An example prompt and resulting explanation is shown in Figure 1. We discovered eight explanation types and Figure 2 includes three Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICER 2022, August 7-11, 2022, Lugano and Virtual Event, Switzerland \\u00a9 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9195-5/22/08. https://doi.org/10.1145/3501709.3544280 explanation types to illustrate the explanatory power of GPT-3. The additional types include: 1) tracing the execution of code, 2) fixing bugs and explaining how they were fixed, 3) generating analogies to real world settings, 4) listing relevant programming concepts, and 5) predicting the console output.\"]}, {\"header\": \"Analyzing and explaining time complexity\", \"paragraph\": [\"Instructors rate time complexity as the most difficult programming topic [17]. However, understanding time complexity is important [6,13] because it facilitates decision-making so students choose an appropriate algorithm for a given problem. This use case shows GPT-3 can identify and explain time complexity.\"]}, {\"header\": \"Identifying common mistakes made by beginner programmers\", \"paragraph\": [\"Commonality exists in how students solve programming problems [15] and the mistakes they make [1,11]. Pedagogical techniques, such as the \'muddiest point\' highlight these common and most confusing concepts [3,14]. GPT-3 can automatically create a checklist of common mistakes students might make regarding a given code snippet.\"]}, {\"header\": \"Summarizing code at multiple levels of abstraction\", \"paragraph\": [\"Before understanding how a code snippet executes, it is often useful to understand the purpose of the code [5]. The summary generated by GPT-3 and shown in Figure 2 defines the goal, traces the execution, and highlights relevant CS concepts such as arrays.\"]}, {\"header\": \"DISCUSSION\", \"paragraph\": [\"Our three use cases demonstrate the potential for GPT-3 to explain code for intro CS students. Our poster presentation will feature all eight explanation types as a design space of explanations to convey the diversity of explanations that can be generated by LLMs. We will highlight best practices for generating effective explanations and pitfalls that lead to less effective explanations. We are evaluating the usefulness of these explanations in a series of summer classes.\"]}]','https://drive.google.com/uc?id=1aOxsvIpWhM8rQ4DWT1j9OwQW9sJhoRwH&export=download','2022-02-03',0,'2024-02-03 22:25:48.244302','2024-02-03 22:25:48.528063'),(13,'Framing the News: From Human Perception to Large Language Model Inferences','Identifying the frames of news is important to understand the articles\' vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computerassisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8] [54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28] [27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to these models, and to analyze use cases where they can be useful and where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, we are interested in identifying use cases and tasks where they can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.\", \"Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written.\", \"The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.\", \"Our work aims to address this research gap by posing the following research questions:\", \"RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries? RQ2: Can prompt engineering be used for classification of headlines according to frames? By addressing the above research questions, our work makes the following contributions: Contribution 1. We implemented a process to do human annotation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a personification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., human interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the performance of a large language model. Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the promptengineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.\", \"The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.\"]}, {\"header\": \"RELATED WORK\", \"paragraph\": [\"Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \\\"To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described. \\\"\", \"For frame recognition, there are two main approaches: the inductive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them appears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \\\"Does the story contain any moral message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\\\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].\", \"We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.\", \"We decided to follow the deductive approach because a predefined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the inductive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.\", \"Yl\\u00e4-Antitila et al. [60] proposed topic modeling as a frame extraction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were eliminated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, environmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.\", \"From Entman\'s definition of frame [23], it seems that the deductive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \\\"some aspects of a perceived reality and make them more salient\\\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.\", \"Isoaho et al. [30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.\", \"We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \\\"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\\\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].\", \"A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classification with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.\", \"Continuing with the question of the methods used for classification, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al. [36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the different strategies that can be followed both in the design of prompts, [12] 15 generic frames: \\\"Economic\\\", \\\"Capacity and resources\\\", \\\"Morality\\\", \\\"Fairness and equality\\\", \\\"Legality, constitutionality and jurisprudence\\\", \\\"Policy prescription and evaluation\\\", \\\"Crime and punishment\\\", \\\"Security and defense\\\", \\\"Health and safety\\\", \\\"Quality of life\\\", \\\"Cultural identity\\\", \\\"Public opinion\\\", \\\"Political\\\", \\\"External regulation and reputation\\\", \\\"Other\\\".\"]}, {\"header\": \"To label frames of full articles\", \"paragraph\": [\"Reading the full article, the annotator defines the main frame 20000 articles [33]  131 headlines + subheadlines the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], question answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al. [45] presented a very interesting idea that we apply to our classification task. This consists of providing the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.\", \"As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].\"]}, {\"header\": \"DATA: EUROPEAN COVID-19 NEWS DATASET\", \"paragraph\": [\"We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid-19 vaccination from 19 newspapers from 5 different countries: Italy, France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to understand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.\", \"Table 2: Keywords used to identify no-vax articles Keywords NO VAX TOPIC \\\"anti-vaxxers\\\", \\\"anti-vaccine\\\", \\\"anti-vaxx\\\", \\\"anti-corona\\\", \\\"no-vax\\\", \\\"no vax\\\", \\\"anti-vaccin\\\"\", \"In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table . Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.\"]}, {\"header\": \"METHODOLOGY 4.1 Human labeling of news frames\", \"paragraph\": [\"To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \'no-frame\' category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.\", \"In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\'s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.\"]}, {\"header\": \"Fine-tuning GPT-3.5 and BERT-based models\", \"paragraph\": [\"With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been In the first approach, a model with a fixed architecture is pretrained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].\", \"We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \\\"fine-tune\\\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pretrained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance.\", \"Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.\"]}, {\"header\": \"Prompt-engineering with GPT-3.5\", \"paragraph\": [\"Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].\", \"In this approach, instead of adapting pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recognizing the emotion of a social media post, \\\"I missed the bus today. \\\", we may continue with a prompt \\\"I felt so _\\\", and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \\\"English: I missed the bus today. French: _\\\"), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].\", \"We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engineering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineering with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined several experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we followed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \'interesting\', \'emotional\', \'personal\', \'human\'. The conflict frame could be: \'conflictive\', \'bellicose\', \'troublesome\', \'rowdy\', \'quarrelsome\', \'troublemaker\', \'agitator\', etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2. After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\"]}, {\"header\": \"RESULTS: HUMAN LABELING OF FRAMES IN NO-VAX NEWS HEADLINES (RQ1)\", \"paragraph\": [\"In this section, we present and discuss the results of the analysis related to our first RQ. Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant ones. Attribution of responsibility is the third one except in Switzerland, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.\"]}, {\"header\": \"Figure 3: Non-normalized distribution of frames per country\", \"paragraph\": [\"The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vaccination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro-or anti-vaccine. Attribution of responsibility is also present. Manual inspection indicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the conflict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.\", \"In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results between 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between countries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vaccination, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.\", \"It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.\", \"Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.\", \"Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between sentiment analysis and frames has been discussed in previous literature [11] [43].\"]}, {\"header\": \"RESULTS: GPT-3.5 FOR FRAME CLASSIFICATION OF HEADLINES (RQ2)\", \"paragraph\": [\"Here, we present and discuss the results related to our second RQ.\", \"6.1 Fine-tuning GPT-3.5\", \"Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERTbased models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT-3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa). On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.\", \"Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al. [55] argue that improvements in the accuracy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \'the rich get richer\', in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.\"]}, {\"header\": \"Prompt-engineering with GPT-3.5\", \"paragraph\": [\"For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This observation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.\", \"Looking at the results of the classification models, the 49% accuracy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees with human annotators in 76% of the cases. Clearly, framing involves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.\", \"News reading is never fully objective, and the annotators engaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \\\"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \'correct\' label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\\\" [42].\", \"Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is complicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].\", \"Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.\"]}, {\"header\": \"CONCLUSIONS\", \"paragraph\": [\"In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed: RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently.\", \"RQ2: Can prompt engineering be used for classification of headlines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification accuracy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classifying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators, and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"This work was supported by the AI4Media project, funded by the European Commission (Grant 951911) under the H2020 Programme ICT-48-2020. We also thank the newspapers for sharing their online articles. Finally, we thank our colleagues Haeeun Kim and Emma Bouton-Bessac for their support with annotations, and Victor Bros and Oleksii Polegkyi for discussions.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"This work was supported by the AI4Media project, funded by the European Commission (Grant 951911) under the H2020 Programme ICT-48-2020. We also thank the newspapers for sharing their online articles. Finally, we thank our colleagues Haeeun Kim and Emma Bouton-Bessac for their support with annotations, and Victor Bros and Oleksii Polegkyi for discussions.\"]}]','https://drive.google.com/uc?id=1gL-46j2YXOr643Ud0Gchoaw5lCtrqkD_&export=download','2023-02-03',0,'2024-02-03 22:25:51.604206','2024-02-03 22:25:51.999539'),(14,'Large Language Model Augmented Narrative Driven Recommendations','Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context -this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with fewshot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.','[{\"header\": \"RELATED WORK\", \"paragraph\": [\"Data Augmentation for Information Access. A line of recent work has explored using language models to generate synthetic queries for data augmentation to train models for information retrieval tasks [7,8,15,23,31]. Here, given a document collection of interest, a pre-trained language model is used to create synthetic queries for the document collection. An optional filtering step excludes noisy queries, and finally, a bi-encoder or a cross-encoder is trained for the retrieval task. While earlier work of Ma et al. [31] train a custom query generation model on web-text datasets, more recent work has leveraged large language models for zero/few-shot question generation [7,8,15,23]. In generating synthetic queries, this work indicates the effectiveness of smaller parameter LLMs (up to 6B parameters) for generating synthetic queries in simpler information-retrieval tasks [7,8,23], and finds larger models (100B parameters and above) to be necessary for harder tasks such as argument retrieval [15,23]. Similar to this work, we explore the generation of synthetic queries with LLMs for a retrieval task. Unlike this work, we demonstrate a data augmentation method for creating effective training data from sets of user documents found in recommendation datasets rather than individual documents. Other work in this space has also explored training more efficient multivector models from synthetic queries instead of more expensive cross-encoder models [39] and generating queries with a diverse range of intents than the ones available in implicit feedback datasets to enhance item retrievability [35]. 3 https://github.com/iesl/narrative-driven-rec-mint/ Besides creating queries for ad-hoc retrieval tasks, concurrent work of Leszczynski et al. [25] has also explored the creation of synthetic conversational search datasets from music recommendation datasets with LLMs. The synthetic queries and user documents are then used to train bi-encoder retrieval models for conversational search. Our work resembles this in creating synthetic queries from sets of user items found in recommendation interaction datasets. However, it differs in the task of focus, creating long-form narrative queries for NDR. Finally, our work also builds on the recent perspective of Radlinski et al. [36] who make a case for natural language user profiles driving recommenders -narrative requests tie closely to natural language user profiles. Our work presents a step toward these systems.\", \"Finally, while our work explores data augmentation from useritem interactions for a retrieval-oriented NDR task, prior work has also explored data augmentation of the user-item graph for training collaborative filtering models. This work has often explored augmentation to improve recommendation performance for minority [12,47] or cold-start users [11,28,45]. And has leveraged generative models [11,45] and text similarity models [28] for augmenting the user-item graph.\", \"Complex Queries in Information Access. With the advent of performant models for text understanding, focus on complex and interactive information access tasks has seen a resurgence [2,29,32,48]. NDR presents an example of this -NDR was first formalized in Bogers and Koolen [5] for the case of book recommendation and subsequently studied in other domains [3,4,6]. Bogers and Koolen [5] systematically examined narrative requests posted by users on discussion forums. They defined NDR as a task requiring item recommendation based on a long-form narrative query and prior-user item interactions. While this formulation resembles personalized search [42] and query-driven recommendation [20], the length and complexity of requests differentiate these from NDR. Other work has also demonstrated the effectiveness of re-ranking initial recommendations from collaborative filtering approaches Figure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering models for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with a large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic queries and user items.\", \"based on the narrative query [18]. More recent work of Afzali et al. [1] formulate the NDR task without access to the prior interactions of a user while also noting the value of contextual cues contained in the narrative request. In our work, we focus on this latter formulation of NDR, given the lack of focus on effectively using the rich narrative queries in most prior work. Further, we demonstrate the usefulness of data augmentation from LLMs and user-item interaction datasets lacking narrative queries.\", \"Besides this, a range of work has explored more complex, longform, and interactive query formulations for information access; these resemble queries in NDR. Arguello et al. [2] define the tip of tongue retrieval task, a known-item search task where user queries describe the rich context of items while being unable to recall item metadata itself. Mysore et al. [32] formulate an aspect conditional query-by example task where results must match specific aspects of a long natural language query. And finally, a vibrant body of work has explored conversational critiquing of recommenders where natural language feedback helps tune the recommendations received by users [30,44,49].\"]}, {\"header\": \"METHOD 3.1 Problem Setup\", \"paragraph\": [\"In our work, we define narrative-driven recommendation (NDR) to be a ranking task, where given a narrative query \\ud835\\udc5e made by a user \\ud835\\udc62, a ranking system \\ud835\\udc53 must generate a ranking \\ud835\\udc45 over a collection of items C. Further, we assume access to a user-item interaction dataset I consisting of user interactions with items (\\ud835\\udc62, {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 ). We assume the items \\ud835\\udc51 \\ud835\\udc56 to be textual documents like reviews or item descriptions. While we don\'t assume there to be any overlap in the users making narrative queries or the collection of items C and the user-items interaction dataset I, we assume them to be from the same broad domain, e.g., books, movies, points-of-interest.\"]}, {\"header\": \"Proposed Method\", \"paragraph\": [\"Our proposed method, Mint, for NDR, re-purposes a dataset of abundantly available user-item interactions, I = {(\\ud835\\udc62, {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 )} into training data for retrieval models by using LLMs as query generation models to author narrative queries \\ud835\\udc5e \\ud835\\udc62 : D = {(\\ud835\\udc5e \\ud835\\udc62 , {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 )}. Then, retrieval models are trained on the synthetic dataset D (Figure 3).\"]}, {\"header\": \"Narrative\", \"paragraph\": [\"Queries from LLMs. To author a narrative query \\ud835\\udc5e \\ud835\\udc62 for a user in I, we make use of the 175B parameter InstructGPT4 model as our query generation model QGen. We include the text of interacted items {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 in the prompt for QGen, and instruct it to author a narrative query (Figure 2). To improve the coherence of generated queries and obtain correctly formatted outputs, we manually author narrative queries for 3 topically diverse users based on their interacted items and include it in the prompt for QGen. The same three few shot examples are used for the whole dataset I, and the three users were chosen from I. Generating narrative queries based on user interactions may also be considered a form of multi-document summarization for generating a natural language user profile [36].\"]}, {\"header\": \"Filtering\", \"paragraph\": [\"Items for Synthetic Queries. Since we expect user items to capture multiple aspects of their interests and generated queries to only capture a subset of these interests, we only retain some of the items present in {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 before using it for training retrieval models. For this, we use a pre-trained language model to compute the likelihood of the query given each user item, \\ud835\\udc43 \\ud835\\udc3f\\ud835\\udc40 (\\ud835\\udc5e \\ud835\\udc62 |\\ud835\\udc51 \\ud835\\udc56 ), and only retain the top \\ud835\\udc40 highly scoring item for \\ud835\\udc5e \\ud835\\udc62 , this results in \\ud835\\udc40 training samples per user for our NDR retrieval models: {(\\ud835\\udc5e \\ud835\\udc62 , \\ud835\\udc51 \\ud835\\udc56 ) \\ud835\\udc40 \\ud835\\udc56=1 }. In our experiments, we use FlanT5 with 3B parameters [14] for computing and follow Sachan et al. [40] for computing \\ud835\\udc43 \\ud835\\udc3f\\ud835\\udc40 (\\ud835\\udc5e \\ud835\\udc62 |\\ud835\\udc51 \\ud835\\udc56 ). Note that our use of \\ud835\\udc43 \\ud835\\udc3f\\ud835\\udc40 (\\ud835\\udc5e \\ud835\\udc62 |\\ud835\\udc51 \\ud835\\udc56 ) represents a querylikelihood model classically used for ad-hoc search and recently shown to be an effective unsupervised re-ranking method when used with large pre-trained language models [40].\"]}, {\"header\": \"Training Retrieval Models.\", \"paragraph\": [\"We train bi-encoder and crossencoder models for NDR on the generated synthetic dataset -commonly used models in search tasks. Bi-encoders are commonly used as scalable first-stage rankers from a large collection of items. On the other hand, cross-encoders allow a richer interaction between query and item and are used as second-stage re-ranking models. For both models, we use a pre-trained transformer language model architecture with 110M parameters, MPnet, a model similar to Bert [41]. Bi-encoder models embed the query and item independently into high dimensional vectors: q \\ud835\\udc62 = MPNet(\\ud835\\udc5e \\ud835\\udc62 ), d \\ud835\\udc56 = MPNet(\\ud835\\udc51 \\ud835\\udc56 ) and rank items for the user based on the minimum L2 distance between q \\ud835\\udc62 and d \\ud835\\udc56 . Embeddings are obtained by averaging token embeddings from the final layer of MPNet, and the same model is used for both queries and items. Cross-encoder models input both the query and item and output a score to be used for ranking \\ud835\\udc60 = \\ud835\\udc53 Cr ([\\ud835\\udc5e \\ud835\\udc62 ; \\ud835\\udc51 \\ud835\\udc56 ]), where \\ud835\\udc53 Cr is parameterized as w \\ud835\\udc47 dropout W \\ud835\\udc47 MPNet(\\u2022) . We train our bi-encoder model with a margin ranking loss:\", \"with randomly sampled negatives \\ud835\\udc51 \\u2032 and \\ud835\\udeff = 1. Our cross-encoders are trained with a cross-entropy loss:\", \"For training, 4 negative example items \\ud835\\udc51 \\u2032 are randomly sampled from ranks 100-300 from our trained bi-encoder. At test time, we retrieve the top 200 items with our trained bi-encoder and re-rank them with the cross-encoder -we evaluate both these components in experiments and refer to them as BiEnc-Mint and CrEnc-Mint.\"]}, {\"header\": \"EXPERIMENTS AND RESULTS\", \"paragraph\": [\"Next, we evaluate Mint on a publicly available test collection for NDR and present a series of ablations.\"]}, {\"header\": \"Experimental Setup\", \"paragraph\": [\"4.1.1 Datasets. We perform evaluations on an NDR dataset for point-of-interest (POI) recommendation Pointrec [1]. Pointrec contains 112 realistic narrative queries (130 words long) obtained from discussion forums on Reddit and items pooled from baseline rankers. The items are annotated on a graded relevance scale by crowd-workers and/or discussion forum members and further validated by the dataset authors. The item collection C in Pointrec contains 700k POIs with metadata (category, city) and noisy text snippets describing the POI obtained from the Bing search engine. For test time ranking, we only rank the candidate items in the city and request category (e.g., \\\"Restaurants\\\") of the query available in Pointrec -this follows prior practice to exclude clearly irrelevant items [1,26]. We use user-item interaction datasets from Yelp to generate synthetic queries for training. 5 Note also that we limit our evaluations to Pointrec since it presents the only publicly available, manually annotated, and candidate pooled test collection for NDR, to our knowledge. Other datasets for NDR use document collections that are no longer publicly accessible [24], contain sparse and noisy relevance judgments due to them being determined with automatic rules applied to discussion threads [18,24], lack pooling to gather candidates for judging relevance [18,24], or lack realistic narrative queries [21]. We leave the development of more robust test collections and evaluation methods for NDR to future work.\"]}, {\"header\": \"Implementation Details.\", \"paragraph\": [\"Next, we describe important details for Mint and leave finer details of the model and training to our code release. To sample user interactions for generating synthetic queries from the Yelp dataset, we exclude POIs and users with fewer than ten reviews to ensure that users were regular users of the site with well represented interests. This follows common prior practice in preparing user-item interaction datasets for use [27]. Then we retain users who deliver an average rating greater than 3/5 and with 10-30 above-average reviews. This desirably biases our data to users who commonly describe their likings (rather than dislikes). It also retains the users whose interests are summarizable by QGen. In the Yelp dataset, this results in 45,193 retained users. Now, 10,000 randomly selected users are chosen for generating synthetic narrative queries. For these users, a single randomly selected sentence from 10 of their reviews is included in the prompt (Figure 2) to QGen, i.e., \\ud835\\udc41 \\ud835\\udc62 = 10. After generating synthetic queries, some items are filtered out ( \\u00a73.2.2). Here, we exclude 40% of the items for a user. This results in about 60,000 training samples for training BiEnc-Mint and CrEnc-Mint. These decisions were made manually by examining the resulting datasets and the cost of authoring queries. The expense of generating \\ud835\\udc5e \\ud835\\udc62 was about USD 230.\"]}, {\"header\": \"Baselines.\", \"paragraph\": [\"We compare BiEnc-Mint and CrEnc-Mint models against several standard and performant retrieval model baselines. These span zero-shot/unsupervised rankers, supervised biencoders, unsupervised cross-encoders, and LLM baselines. BM25: A standard unsupervised sparse retrieval baseline based on term overlap between query and document, with strong generalization performance across tasks and domains [38]. Contriver: A BERT-base bi-encoder model pre-trained for zero-shot retrieval with weakly supervised query-document pairs [22]. MPNet-1B: A strong Sentence-Bert bi-encoder model initialized with MPNet-base and trained on 1 billion supervised query-document pairs aggregated from numerous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned on supervised question-passage pairs from MSMarco. UPR: A twostage approach that retrieves items with a Contriver bi-encoder and re-ranks the top 200 items with a query-likelihood model using a FlanT5 model with 3B parameters [14,40]. This may be seen as an unsupervised \\\"cross-encoder\\\" model. Grounded LLM: A recently proposed two-stage approach which autoregressively generates ten pseudo-relevant items using an LLM (175B InstructGPT) prompted with the narrative query and generates recommendations grounded in C by retrieving the nearest neighbors for each generated item using a bi-encoder [19]. We include one few-shot example of a narrative query and recommended items in the prompt to the LLM. We run this baseline three times and report average performance across runs. We report NDCG at 5 and 10, MAP, MRR, and Recall at 100 and 200. Finally, our reported results should be considered lower bounds on realistic performance due to the unjudged documents (about 70% at \\ud835\\udc58 = 10) in our test collections [10].\"]}, {\"header\": \"Results\", \"paragraph\": [\"Table 1 presents the performance of the proposed method compared against baselines. Here, bold numbers indicate the best-performing model, and superscripts indicate statistical significance computed with two-sided t-tests at \\ud835\\udc5d < 0.05.\", \"Here, we first note the performance of baseline approaches. We see BM25 outperformed by Contriver, a transformer bi-encoder model trained for zero-shot retrieval; this mirrors prior work [22]. Next, we see supervised bi-encoder models trained on similar passage (MPNet-1B) and question-answer (BERT-MSM) pairs outperform a weakly supervised model (Contriver) by smaller margins. Finally, the Grounded LLM outperforms all bi-encoder baselines, indicating strong few-shot generalization and mirroring prior results [19]. Examining the Mint models, we first note that the BiEnc-Mint sees statistically significant improvement compared to BM25\"]}, {\"header\": \"Ablations\", \"paragraph\": [\"In Table 2\"]}, {\"header\": \"\\ud835\\udc56=1\", \"paragraph\": [\"which have a low likelihood of being generated from the document ( \\u00a73.2.2). Without this step, we expect the training set for training retrieval models to be larger and noisier. In Table 2, we see that excluding this step leads to a lower performance for BiEnc and CrEnc, indicating that the quality of data obtained is important for performance.\", \"6B LLM for QGen. Mint relies on using an expensive 175B parameter InstructGPT model for QGen. Here, we investigate the efficacy for generating \\ud835\\udc5e \\ud835\\udc62 for {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 with a 6B parameter Instruct-GPT model (text-curie-001). We use an identical setup to the 175B LLM for this. In Table 2, we see that training on the synthetic narrative queries of the smaller LLM results in worse models -often underperforming the baselines in Table 1. This indicates the inability of a smaller model to generate complex narrative queries while conditioning on a set of user items. This necessity of a larger LLM for generating queries in complex retrieval tasks has been observed in prior work [15,23].\", \"6B LLM for Item Queries. We find a smaller 6B LLM to result in poor quality data when used to generate narrative queries conditioned on {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 . Here we simplify the text generation taskusing a 6B LLM to generate queries for individual items \\ud835\\udc51 \\ud835\\udc56 . This experiment also mirrors the setup for generating synthetic queries for search tasks [7,15]. Here, we use 3-few shot examples and sample one item per user for generating \\ud835\\udc5e \\ud835\\udc62 . Given the lower cost of using a smaller LLM, we use all 45,193 users in our Yelp dataset rather than a smaller random sample. From Table 2, we see that this results in higher quality queries than using smaller LLMs for generating narrative queries from {\\ud835\\udc51 \\ud835\\udc56 } \\ud835\\udc41 \\ud835\\udc62 \\ud835\\udc56=1 . The resulting BiEnc model underperforms the BiEnc-Mint, indicating the value of generating complex queries conditioned on multiple items as in Mint for NDR. We see that CrEnc approaches the performance of CrEnc-Mintnote, however, that this approach uses the performant BiEnc-Mint for sampling negatives and first stage ranking. We leave further exploration of using small parameter LLMs for data augmentation for NDR models to future work.\"]}, {\"header\": \"CONCLUSIONS\", \"paragraph\": [\"In this paper, we present Mint, a data augmentation method for the narrative-driven recommendation (NDR) task. Mint re-purposes historical user-item interaction datasets for NDR by using a 175B parameter large language model to author long-form narrative queries while conditioning on the text of items liked by users. We evaluate bi-encoder and cross-encoder models trained on data from Mint on the publicly available Pointrec test collection for narrative-driven point of interest recommendation. We demonstrate that the resulting models outperform several strong baselines and ablated models and match or outperform a 175B LLM directly used for NDR in a 1-shot setup.\", \"However, Mint also presents some limitations. Given our use of historical interaction datasets for generating synthetic training data and the prevalence of popular interests in these datasets longer, tailed interests are unlikely to be present in the generated synthetic datasets. In turn, causing retrieval models to likely see poorer performance on these requests. Our use of LLMs to generate synthetic queries also causes the queries to be repetitive in structure, likely causing novel longer-tail queries to be poorly served. These limitations may be addressed in future work. Besides this, other avenues also present rich future work. While Mint leverages a 175B LLM for generating synthetic queries, smaller parameter LLMs may be explored for this purpose -perhaps by training dedicated QGen models. Mint may also be expanded to explore more active strategies for sampling items and users for whom narrative queries are authored -this may allow more efficient use of large parameter LLMs while ensuring higher quality training datasets. Next, the generation of synthetic queries from sets of documents may be explored for a broader range of retrieval tasks beyond NDR given its promise to generate larger training sets -a currently underexplored direction. Finally, given the lack of larger-scale test collections for NDR and the effectiveness of LLMs for authoring narrative queries from user-item interaction, fruitful future work may also explore the creation of larger-scale datasets in a mixed-initiative setup to robustly evaluate models for NDR.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Information Retrieval, NSF grants IIS-1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors.\"]}, {\"header\": \"ACKNOWLEDGMENTS\", \"paragraph\": [\"We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Information Retrieval, NSF grants IIS-1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors.\"]}]','https://drive.google.com/uc?id=1681cLYzuKcz9AOZioGF3EcSJeZYYm6QL&export=download','2023-02-03',0,'2024-02-03 22:25:54.550996','2024-02-03 22:25:54.911571'),(15,'A Prototype Implementation of an Orthographic Software Modeling Environment','Orthographic Software Modeling (OSM) is a view-centric software engineering approach that aims to leverage the orthographic projection metaphor used in the visualization of physical objects to visualize software systems. Although the general concept of OSM does not prescribe specific sets of views, a concrete OSM environment has to be specific about the particular views to be used in a particular project. At the University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views defined by the KobrA 2.0 method, a version of KobrA adapted for OSM. In this paper we provide an overview of the KobrA 2.0 metamodel underpinning nAOMi and give a small example of its use to model a software system.','[{\"header\": \"INTRODUCTION\", \"paragraph\": [\"Orthographic Software Modeling (OSM) is based on three fundamental hypotheses -(a) that it is feasible to integrate the many different kinds of artifacts used in contemporary software engineering methods within a single coherent methodology in which they are treated as views, (b) that it is feasible to create an efficient and scalable way of supporting these views by generating them dynamically, on-the-fly, from a Single Underlying Model (SUM) using model-based transformations and (c) that it is feasible to provide an intuitive metaphor for navigating around these many views by adapting the orthographic projection technique underpinning the CAD tools used in other engineering disciplines. As shown in Figure 1, the main advantages of using the idea of orthographic projection to define the views used to visualize and described a system are that they (a) can be organized according to a simple and easy-to-understand metaphor and (b) collectively represent all the properties of a system with minimal overlap and redundancy. In practice this translates into a set of \\\"dimensions\\\", each containing well defined choices (or so called \\\"dimension elements\\\") that can be used to select individuals views.\", \"As shown in Figure 2, the main advantage of making the artifacts used to describe a software system views of a SUM is that the number of pairwise coherence relationships that have to be maintained is reduced and new views can be introduced by simply defining their relationship to the SUM. Moreover, the importance of this advantage grows quickly as the size of the system and the complexity of the deployed development methodology increase. Another important advantage is that the dominance of one particular kind of view over the development process (e.g. code) at the expense of other kinds of views (e.g. graphical models) is reduced so that any appropriate type of views can be used to enrich the underlying description of the system, depending on the needs and skills of the stakeholder involved. This makes it possible to subsume all view types under the same, overarch-SUM SUM / View Centric Environment Artifact / Tools Centric Environment ing development process and methodology (e.g. agile-driven, focusing on small development cycles, or model-driven development, based on transformations between abstraction levels). Although the details of how the views are created from the SUM and how the SUM is updated from the views are not central to the approach, a natural implementation is to use the visualization and transformation technologies offered by model driven software engineering (MDSE).\", \"To explore the validity of these hypotheses at the University of Mannheim we have been developing a prototype OSM modeling environment based on an enhanced version of the KobrA method for model-driven, component-oriented development, KobrA 2.0 [1]. This was chosen as a basis for the prototype, known as the Open, Adaptable, Orthographic Modeling Environment (nAOMi) [13] because its views were designed with the precise goals of being (a) genuine projections of a subject containing carefully selected subsets of information about that subject, (b) minimalistic in the sense that they should overlap to the smallest extent possible and contain the minimum necessary models elements, and (c) selectable via a set of independent \\\"dimensions\\\" which reflect different fundamental concerns of development (i.e. abstraction levels, composition or variants). In other words, KobrA already provided one of the \\\"most orthogonal\\\" sets of views for visualizing software systems of any contemporary method. More details about the actual views and dimensions defined in KobrA are presented in the following sections. More information on OSM can be found in [2] and [3].\", \"nAOMi is implemented as an Eclipse plugin using the Eclipse Modeling Framework (EMF) as the underlying modeling platform and UML 2.0 tools [4] to generate and edit views. The KobrA 2.0 metamodel on which the current version of nAOMi is based is a specialization of the UML metamodel composed of three separate packages -one for the SUM, one for the views and one for the transformations (Figure 3). The UML was chosen as the base language because of its maturity and widespread acceptance, making the environment usable to the largest possible body of developers. UML elements not needed in KobrA 2.0 are excluded using OCL constraints while new elements or properties are introduced by specializing existing elements.\", \"The unique contribution of this paper is to elaborate on the structure of the KobrA 2.0 metamodel and how it is used to drive nAOMi. The three following sections each focus on one of the three main components of the metamodel -the SUM, the views and the transformations . This is followed by a brief overview of the OSM navigation paradigm in Section 5 before a small example of the approach is presented in Section 6. Section 7 then concludes the paper with related and future work.\"]}, {\"header\": \"SUM PACKAGE\", \"paragraph\": [\"Figure 4 depicts the internal structure of the SUM package which is based on the UML metamodel. There are three main subpackages, two containing the structural and behavioral constructs respectively, and one containing the constraints that ensure that the metaclasses are used according to the KobrA conventions and rules.\", \"The Classes subpackage of the Structure package contains some of the most fundamental elements of the KobrA metamodel, such as Class and ComponentClass. The internal structure of this package is illustrated in Figure 5. Com-ponentClass represents objects with complex and reusable behaviors, while Class captures simple \\\"data type\\\" objects that have only very simple or non-reusable behaviors. The modeler has to decide whether it is necessary to model a specific part of the system as a ComponentClass and include state charts and activity diagrams, or whether it is sufficient to use a Class (which is limited to using OCL constraints).\", \"ComponentClass inherits (indirectly via Class) from Communications so it also has the isActive attribute. This makes it possible to model whether its instances are active or passive. Active objects, which can be used to model threads and processes ([8] p. 438), start to execute their behavior as soon as they are created and perform operations spontaneously.\", \"A ComponentClass may exhibit complex behavior. In Ko-brA, this behavior may be specified in the form of UML State Diagrams (defining acceptable operation invocation sequences), and in the form of Activities (defining algorithms of operations). UML Interaction elements (in sequence diagrams) can be derived from the activity elements and thus are not included in the SUM. As KobrA aims to facilitate automatic checking of allowed sequences of operation calls, Protocol State Machines are supported instead of general state machines. Since the latter include a large variety of elements not needed for specifying acceptable operation se-quences or automatic checking, OCL constraints are used to prohibit the use of unwanted features. For example, since KobrA has no concept of roles for components, the use of role also needs to be prohibited. The part association refers to owned properties of components whose attribute isComposite is true. As KobrA uses associations like nests and creates for components, part, required and provided are not needed. Connectors (i.e. delegation and assembly) are not used in KobrA either so ownedConnector is excluded.\"]}, {\"header\": \"VIEWS PACKAGE\", \"paragraph\": [\"The structure of the Views package is illustrated in Figure 6. Again, since most of the views defined in KobrA 2.0 are based on UML diagrams, the view metamodels have similar elements to the SUM metamodel. The big difference to the SUM is that there are no restrictions on the use of the view metamodel elements. For instance, views for a particular purpose such as supporting model checkers can be supported by adding elements unrelated to the UML.\", \"The substructure of the Views package reflects the types and organization of the KobrA views according to the view \\\"dimensions\\\" supported in nAOMi (cf. example in Section 6). At the top level, the Views package is thus decomposed into the Specification and Realization options of the encapsulation dimension. These, in turn are both decomposed into the Structural, Behavioral and Operational options of the Projection dimension. Finally, with the exception of the behavioral option, these are also all subdivided into the Service and Type options of the granularity dimension. This dimension, with its two options, is an addition to the original version of KobrA.\", \"The Service view shows the direct, publicly visible relationships of the subject ComponentClass to other Compo-nentClasses, while the Type view shows the publicly visible relationships of the subject to simple Classes. As with the SUM, constraints have been defined to control what can go into each view and when they are well formed. For every view, a constraint enumerates all allowed elements (not shown in this paper).\", \"In the following, some of the other constraints for the Service view are elaborated. Since this view is a black-box view, the internals of ComponentClasses (nestedClassifier ) are not shown.\", \"context ComponentClass --no nested classifiers , no protocol inv : nestedClassifier -> union ( protocol ) -> isEmpty () Classes are only allowed if they are generalizations of Com-ponentClasses, (or any of its superclasses, since a Compo-nentClass may inherit from a class as shown in the constraints with context Class. The following invariants ensure that only publicly visible attributes and operations are in this view, for both classes and ComponentClasses (which inherit from Class). Only operation signatures are shown in this view, so pre-, post-and bodyconditions, as well as activities are omitted, which is reflected in the last constraint.\", \"context Operation --only the signature of the Operation is shown , not its behavior ( role name \\\" method \\\" refers to the Activities of the operation ) , or dependencies inv : method -> union ( precondition ) -> union ( body ) -> union ( postcondition ) -> isEmpty ()\"]}, {\"header\": \"TRANSFORMATIONS PACKAGE\", \"paragraph\": [\"The package AllViews provides the foundation for specifying the transformations between the SUM and the views in both directions. Part of the package\'s contents are shown in Figure 7. The Abstraction concept (which is in fact a dependency reused from the UML but with additional constraints) plays the key role in relating elements from the SUM to elements of a view. Abstraction is actually mapped to ExpressionInOcl. When appearing in transformations, the equals sign links elements in the SUM to the respective elements in the view, and vice versa. For instance, equality of the general meta-association of a Generalization in a transformation invariant means that, when following general, there must be an element in the SUM and in the view for which similar transformation expressions are specified.\", \"In the case of KobrA 2.0, which has many projections that just select a subset of elements using one-to-one abstractions, this allows concise declarative TransformationExpressions. Together with the view constraints, a CASE tool can be implemented which uses a transformation language of the implementor\'s choice, for instance the Atlas Transformation Language (ATL) [11] or QVT [9]. The role names se and ve are short for SumElement and ViewElement, respectively. These roles subset the client and supplier roles from the UML. SUM elements are translated into UML elements with stereotypes, so that the views are easy to manage for developers familiar with the UML. The bidirectional mappings between stereotyped view elements and non-stereotyped SUM elements are expressed in the constraints of the Association-Abstraction, a subclass of the Abstraction from the AllViews package. This is also an example of a transformation which is reused in other views. Figure 8 shows the main elements involved in the transformation of the black box structural view for Component-Classes. The first transformation constraint is on the view and declares the starting point for the transformation. It states that the subject ComponentClass and its generalizations (using a SUM utility function, superClosure) are in the view.\", \"The following transformation rules illustrate how to create the output (i.e. view) elements from the input (i.e. SUM) elements, such as the publicly visible attributes and operations of the ComponentClass and the acquired ComponentClasses. The first constraint for ComponentClassAbstraction states that references to potential general classes (and Component-Classes) of ComponentClasses are mirrored in the view. In addition, ComponentClasses will be shown with the corresponding stereotypes. The ComponentClass owns various types of associations, so in this view only the acquires associations are selected (whose transformation rules are covered in the common transformation packages).For classes and ComponentClasses, only publicly visible attributes and operations appear in the view. Class invariants are also copied. Classes that may appear in this view (e.g. as generalizations of ComponentClasses) may have a powertype (role name powertypeExtent) which will be displayed.\", \"The last transformation statement copies the class references of operations. As with all views, the transformation rules, the common transformation statements (which also cover operations) and the view constraints serve as a specification for the implementation of a view. Individual CASE tools can use different implementation techniques as long as they conform to the semantics of these rules and constraints.  For the black box type view, only publicly visible attributes and operations of classes (as opposed to Compo-nentClasses) used by the subject can be seen. This is specified in the first rule which defines owned members of the view and thus serves as the starting point of the transformation. cbbTypes is a utility function defined in the SUM which computes the black box types by selecting the types of the subject\'s public attributes and parameter types of its public operations.\", \"Class invariants and potential powertypes and connections to the classes in this view are shown as well. There may also be Enumerations, for which the EnumerationLiterals are displayed.\", \"The transformation rules for this view are almost the same as the realization transformation constraints from the package Transformation::Realization::Structural::Class::Type. The differences are the select(visibility=#public) statements for operations and attributes. s t r i n g I n S i g n a t u r e\"]}, {\"header\": \"NAVIGATION\", \"paragraph\": [\"Most of today\'s tools use some combination of trees to organize the content of models as well as the views used to visualize a software system or component. In an any environment incorporating a number of different tools there is invariably a large number of different trees storing a heterogeneous mix of artifacts including model elements (e.g. classes, instances, associations), diagrams (e.g. class diagrams, state diagrams) and other artifact types (source code, XML files, configuration files ). To work with all the views in a traditional development environment, therefore, engineers typically have to learn about the organization structures of all the incorporated tools.\", \"In contrast to conventional paradigms for organizing and navigating the many views used to visualize a system, OSM employs the metaphor of a multi-dimensional cube. More specifically, as illustrated in Figure 9, OSM regards dimension of the underlying methodology as representing a different dimension of the cube, and each independently variable aspect of that dimension is a selectable dimension element. Selecting a view thus simply corresponds to selecting a single cell within the cube. In general, three types of dimensions are supported: static dimensions in which the number of selectable elements (i.e. coordinates) is fixed, dynamic dimensions in which the number of elements is dynamic (i.e. derived from the SUM), and mixed dimensions which have both static and dynamic elements.\", \"To support the OSM dimension based navigation metaphor for KobrA, we defined the seven dimensions indicated on the left hand side of Figure 10 which is a sceenshot of nAOMI. The Abstraction dimension (not expanded here), which has three static dimension elements, PIM (platform independent model), PSM (platform specific model) and Code, captures the model-driven development concern of KobrA. The version dimension captures the state of the modeled system at specific points in time. The Component dimension, which has dynamic dimension elements defined by instances of the class ComponentClass in the SUM, captures the componentbased development concern of KobrA.\", \"The Encapsulation dimension, which has two fixed elements, supports the distinction between Specification (black box) and Realization (white box) views of components, while the Projection dimension with the fixed elements Structural, Operational and Behavioral covers the different information types. The Granularity dimension provides a finer grained distinction between views describing the types used by components (Type granularity) and views describing the required and provided interfaces (Service granularity). The Operation dimension allows a selection of individual operations.\", \"In the ideal case, when all views are truly orthogonal, the choices that can be made in each dimensions are completely independent. However, this is very difficult to achieve in software engineering. The approach still works if the views are not completely orthogonal, but dependencies then occur between different choices in different dimensions, so that the decisions made in one dimensions may affect choices possible in another dimension. This is best handled by giving dimensions a precedence ranking determined by the order in which they appear (the top being the highest). When an element in a dimension is selected, the tool automatically makes default selections for dimensions of lower precedence (i.e. dimensions lower down) and disables selections that would navigate to cells (i.e. views) which are not (yet) defined by the method at hand.\"]}, {\"header\": \"SHOPPING CART EXAMPLE\", \"paragraph\": [\"To show how a software system can be specified using nAOMi, this section presents a case study based on a shopping cart system. A ShoppingCart component collects and manages the products selected by users and supports payment via a credit card. Figure 10 illustrates a structural view of the component.\", \"In the dimension navigator on the left hand side, PIM was chosen for the \\\"Abstraction Level\\\" (not expanded in the screenshot). The second dimension is the state of the software system at a certain point in time. The picture shows that the latest available version was chosen. As with every choice in a dimension, it may influence the options in lower ranked dimensions. The component under consideration is the ShoppingCart, for which a black box view is selected in the next dimension. After the user selects the structural projection option and the service level granularity, the tool automatically chooses the option for all operations in the last dimension, as there is no editor registered for the other options.\", \"The component under development is presented with the stereotype subject and its relationship to other components and classes is shown in the view, which corresponds to a cell of the multi-dimensional navigation cube, and is generated on-the-fly from the SUM when it is selected. The classes Product and CreditCard can be used as data types in the operations of the component.\", \"Figure 11 illustrates the operational view in which an operation can be formalized using pre-and postconditions. The precondition corresponds to the assumes clause in and the postcondition corresponds to the result clause. As in the UML, the precondition of an operation must be true when the operation is invoked and the postcondition must be true when the operation is finished. The operation addProduct in Figure 11 must be in state CollectingProducts or Empty when invoked. This is also visible in the behavioral view, since there are only two transitions with the operation ad-dProduct. Both leads to the state CollectingProducts which is also a postcondition of the operation. The second postcondition is that the cost attribute of the component must be increased by the price of the added product. The pre-and postcondition can be expressed using the OCL. The properties of the component, states and operation parameters can be used to formalise the constraints like as in this example.\", \"Figure 12 shows the publicly visible behaviour of the Shop-pingCart component with states and transitions. The conditional transitions map to operations of the component. Like every view, this view is also synchronized with the SUM so that it is guaranteed that its operations, states and properties are consistent with those in the structural view. Although the operational view seems to be similar to the behavioral view because of the overlapping information within them, there are significant differences. The focus of the operational view is on a precise formal definition of an operation of a component. The operations can be enriched by preand postconditions which can be defined using complex OCL statements, that formalize the complete behavior of an operation. The additional information in the OCL statements can be used for code generation and documentation.\"]}, {\"header\": \"CONCLUSION\", \"paragraph\": [\"At the beginning of the paper we identified three fundamental hypothesis upon which the notion of OSM is based -(a) that it is feasible to integrate the many different kinds of artifacts used in contemporary software engineering methods within a single coherent methodology in which they are treated as views, (b) that it is feasible to create an efficient and scalable way of supporting these views by generating them dynamically, on-the-fly, from a Single Underlying Model (SUM) using model-based transformations and (c) that it is feasible to provide an intuitive metaphor for navigating around these many views by adapting the orthographic projection technique underpinning the CAD tools used in other engineering disciplines.\", \"The prototype tool, nAOMi, described in this paper represents the first step towards demonstrating the validity of these hypotheses and showing that OSM is a viable approach to software engineering. Of the three hypotheses, (a) and (c) are most convincingly demonstrated by the prototype, since it shows that it is indeed possible to support all the views of the KobrA method within a single navigation metaphor. The prototype tool does not demonstrate the validity of hypothesis (b) to the same extent as the others due to its small size. Although it demonstrates the feasibility of generating views from the SUM and vice-versa, the question of whether such an approach scales up to large environments is still open.\", \"Although nOAMi is the only tool developed with the specific aim of supporting KobrA-based OSM, several other tools and methods have similar properties or aims. For example, Glinz et al. [10] describe a tool with a fisheye zooming algorithm which lets the user view a model with varying amounts of detail depending on the context. It has to be investigated whether it is possible to combine the fisheye zooming concept with the dimension-based navigation paradigm. While the KobrA 2.0 implementation of nAOMi heavily uses UML diagrams for developers, Glinz et al. use custom diagram types, e.g. for structural and behavioral views.\", \"An approach which also emphasizes the description of formal consistency rules (correspondences) between views is RM-ODP [5][6]. However, this approach does not explicitly mention the notion of a SUM and thus implies that consistency rules should be defined in a pairwise fashion between individual pairs of views. ArchiMate [7], which complements TOGAF [12], is an enterprise architecture modeling language which offers two orthogonal \\\"dimensions\\\" for modeling, (business, architecture, and technology) layers and (informational, behavioral and structural ) aspects and also suggests two more dimensions, purpose and abstraction level. However, as many of these views span multiple choices of a single \\\"dimension\\\", the intuitive dimension-based navigation metaphor of OSM can not be easily applied. There are also more general approaches for view-based modeling but they are less specific in terms of consistency rules between views and provide little guidance on how to manage and navigate views, for example the Zachman Framework [14].\", \"Regarding the practical use of OSM environments in the future, the biggest challenge is developing appropriate SUM metamodels which can accommodate all the types of views and services that software engineers are accustomed to today. For this first prototypical SUM-based environment supporting the OSM approach we had a method at our disposal (KobrA) that already defined a full set of orthogonal UMLbased views. This allowed us to model the required SUM and view metamodels by simply adapting the UML metamodels, removing and adding model elements as needed.\", \"In doing so we were able to manually ensure that the metamodels fulfilled the two core requirements of SUM-based environments -(1) being minimalistic and (2) redundancy free. If SUM-based software engineering environments are to take off, and to be introduced into existing, heterogeneous environments, more sophisticated ways of integrating existing metamodels into a single unified metamodel will be required.\"]}]','https://drive.google.com/uc?id=1TBscbQ9zMU4S6Qg_AihU9wCzy58aVD6d&export=download',NULL,0,'2024-02-03 22:25:57.342736','2024-02-03 22:25:57.496797'),(16,'Improved stochastic subset optimization method for structural design optimization','The Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that minimizes the probability of system failure over the admissible space for the design parameters. It is based on the simulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and exploiting the information contained in these samples to identify subregions for the optimal design parameters within the original design space. This paper presents an improved version of SSO, named iSSO to overcome the shortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design space into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A double-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a generalized design optimization approach primarily tailored for the stochastic structural systems but also adaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness and efficiency of the proposed iSSO.','[{\"header\": \"Introduction\", \"paragraph\": [\"Structural optimization may be defined as the rational establishment of an economical structural design with the available resources while satisfying specific performance criteria. In general terms, the economy may be characterized by minimum weight, minimum cost, maximum utility, or even minimum probability of failure. Broadly, structural optimization can be categorized into deterministic and stochastic optimization [1,2]. The classical statement of unconstraint deterministic optimization is mathematically expressed as: minimize : \\u03c6\\u2208\\u03a6 g(\\u03c6) (1) where, \\u03c6 = [\\u03c6 1 \\u22ef\\u03c6 n\\u03c6 ] T \\u2208 \\u03a6\\u2282R n\\u03c6 is a set of deterministic adjustable parameters that define the structural design, referred to herein as design parameters, g(\\u03c6) : R n\\u03c6 \\u2192R is the objective function to be minimized, and \\u03a6 denotes the bounded admissible design space. The deterministic constraints can be considered by the appropriate definition of the admissible design space \\u03a6 for deterministic design parameters \\u03c6, as mentioned in [3]. In the deterministic structural optimization problem, the uncertainties in parameters are ignored, and fixed values are assumed for all the parameters. There are numerous optimizations approaches available in the literature, however, but it\'s worth noting that no one-size-fits-all optimization approach is ideal for all sorts of problems [4][5][6][7]. The choice of optimization method is often determined by the specific characteristics of the problem, such as its complexity, dimensionality, constraints, and the nature of the objective function. As a result, there is always a scope for new approaches to be developed or the adaptation of existing methods to better suit specific problem classes. A detailed discussion of deterministic optimization approaches can be found in the literature [8,9]. In any practical situation, several parameters, such as loadings, structural parameters, geometric parameters, operation conditions, etc., are either not known at the design stage or are subjected to random fluctuations that give rise to performance variability and affect the performance of a system [10]. These parameters are characterized as uncertain parameters. Deterministic structural optimization discards the impact of uncertainty and can result in improper design. Therefore, it is desirable to account for the uncertainty in the parameters during optimization by using the rational methods of probabilistic structural analysis [11]. Such structural optimization that accounts for uncertainties is called stochastic optimization [12]. Although stochastic optimization refers to any method that employs randomness within some communities, in this paper, we will only consider settings where the objective function is random. Stochastic optimization or optimal design under uncertainty has been widely applied in many practical engineering fields, including civil engineering structures [13][14][15], composite structures [16,17], and vehicles [18,19].\", \"Consider an engineering system that involves deterministic design parameters \\u03c6, and uncertain variables \\u03b8 = [\\u03b8 1 \\u22ef\\u03b8 n\\u03b8 ] T \\u2208 \\u0398\\u2282R n\\u03b8 following a joint PDF p(\\u03b8|\\u03c6), where \\u0398 denotes the parameter space of the uncertain variables. The classical statement of stochastic optimization is mathematically expressed as:\", \"where, h(\\u03c6, \\u03b8) : R n\\u03b8 +n\\u03c6 \\u2192R is the structural performance function, and E \\u03b8 [ \\u22c5 ] denotes expectation with respect to the PDF for \\u03b8. Note that the objective function in the optimization problem in (2) is the expectation E \\u03b8 [h(\\u03c6, \\u03b8)] which is a deterministic function. It\'s worth mentioning that stochastic optimization may also involve other stochastic measures such as variance or quantile values. However, these stochastic measures can rarely be evaluated analytically; therefore, several methods have been proposed for solving stochastic optimization problems. These specialized methods include, for example, sample average approximation, stochastic approximation, stochastic subset optimization, and approaches based on the use of Taylor series expansion [15,20,21], response surface, and metamodels [22][23][24][25]. Specific to structural engineering, there are two broad categories of problems involving design optimization under uncertainty [26][27][28][29][30][31][32][33][34][35]: Reliability-Based Design Optimization (RBDO) and Robust Design Optimization (RDO). The objective of RBDO is to find an optimal solution that minimizes some deterministic, objective function under observance of probabilistic constraints instead of conventional deterministic constraints [36,37].\", \"On the other hand, RDO aims to find an optimal solution that is insensitive (or less sensitive) to input variations. It improves the design quality by minimizing performance variation without eliminating uncertainty [29,38]. Taflanidis and Beck [39] introduced a novel algorithm for optimal reliability problem, the so-called SSO. SSO involves formulating an augmented problem where the design parameters are artificially considered uncertain and defining an auxiliary PDF that includes the structural performance function and the PDF of the uncertain variables. Next, SSO involves generating a pool of samples distributed according to this auxiliary PDF and identifying a subregion in the original design space, which, on average, improves the value of the objective function. By repeating this procedure several times, it is possible to determine at each step a smaller subregion in the design space, which in turn improves the value of the objective function. Ultimately, this subregion will be sufficiently small to directly identify the optimal solution or provide sufficient information to launch another optimization algorithm, such as the sample average approximation or stochastic approximation. The implementation of the SSO method closely resembles the Subset Simulation (SS) algorithm [40] for reliability analysis. Since SSO is based on simulation, it can deal with linear or nonlinear problems and, at least theoretically, an unbounded number of design parameters. The numerical effort for solving a given optimization problem is independent of the number of uncertain variables, and it grows linearly with the number of design parameters.\", \"Since the introduction of SSO, several extensions of SSO have been proposed. An extension of SSO termed Non-Parametric SSO, which adopts kernel density estimation to approximate the objective function, is presented in [41]. In [42], efficient integration of the Moving Least Squares approximation within SSO is introduced to reduce the computational effort in SSO. In [3], an augmented formulation is presented for the RDO of structures using SSO. SSO or its variants have also been applied to solve structural optimization problems. SSO has been used for reliability optimization and sensitivity analysis in system design in [39]. A framework for RDO of Tuned Mass Dampers (TMD) by SSO is discussed in [43]. Even though SSO has proved to be efficient for meeting various challenging optimization problems, it has two shortcomings. First, the effectiveness of SSO is dependent on the correct selection of the geometrical shape of the admissible subsets. Here, it is pertinent to mention that choosing a geometrical shape that effectively investigates the sensitivity of the objective function to each design variable is essential. The shapes, such as hyper-rectangle and hyper-ellipse are suggested in the literature for the admissible subsets. However, as shown later via the illustrative example, these shapes fail to include the optimal solution in cases with complex design spaces or problems with multiple optimal solutions. And second, identifying the optimal subset that contains the smallest volume density involves a non-smooth optimization problem which is quite challenging.\", \"In this paper, an improved version of SSO is developed to overcome the shortcomings of the original SSO. This new version of the algorithm, as mentioned earlier, is named iSSO (improved SSO). Voronoi tessellation is implemented to partition the design space into non-overlapping subregions (a set of Voronoi cells) using the pool of samples distributed according to the auxiliary PDF. The admissible set (a set of all admissible subregions) is then defined as a set containing all subsets of the set of Voronoi cells. This approach is able to capture the regions with lower objective function values even if they are disjointed or when the design space is complex. The details of the Voronoi tessellation are presented in Appendix A. A double-sort algorithm is then implemented to identify the optimal subset containing the smallest volume density.\", \"In the next section, the original SSO is reviewed. Section 3 presents the general theoretical and computational framework for the iSSO algorithm. Section 4 considers several optimization problems to illustrate the effectiveness and efficiency of the proposed iSSO algorithm.\"]}, {\"header\": \"Original stochastic subset optimization\", \"paragraph\": [\"In SSO, say at the i + 1th iteration, the design space is represented by a subset I (i) , where I (i) \\u2208 I (i -1) \\u22c5\\u22c5\\u22c5 \\u2208 I (0) \\u2208 \\u03a6. Following the augmented formulation concept initially discussed in [44] for RBDO, the design parameters \\u03c6, are artificially considered uncertain variables with a prescribed PDF p(\\u03c6|I (i) ) over the design space I (i) [45]. For convenience, p(\\u03c6|I (i) ) = 1/V (i) is considered, where V (i) is the volume of I (i) . In this setting of the augmented stochastic design problem, the auxiliary PDF is defined as:\", \"where, p(\\u03c6, \\u03b8|I (i) ) = p(\\u03b8|\\u03c6)p(\\u03c6|I (i) ). Note that if h(\\u03c6, \\u03b8)\\u2264 0, it must be suitably transformed to ensure that \\u03c0(\\u03c6, \\u03b8|I (i) ) \\u2265 0. One way to do this is\", \"s, that is, the two expected values differ only by a constant, and the optimization of the expected value of h( \\u22c5 ) is equivalent, in terms of the optimal design choice, to optimization for the expected value for h s ( \\u22c5 ). In the above equation, the denominator is a normalizing constant given by:\", \"Although this expected value is not explicitly needed, it can be determined using any state-of-the-art stochastic simulation method. The objective function E \\u03b8 [h s (\\u03c6, \\u03b8)] in this context of the auxiliary PDF is expressed as:\", \"where, the marginal \\u03c0(\\u03c6|I (i) ) is given by: \\u03c0\", \"\\u03c0(\\u03c6, \\u03b8)d\\u03b8. (6) In (5), since\", \"is equivalent to minimization of J(\\u03c6), which is equal to:\", \").\", \"The estimation of the marginal \\u03c0(\\u03c6|I (i) ) in ( 7) is necessary to minimize J(\\u03c6|I (i) ). Analytical approximations of \\u03c0(\\u03c6|I (i) ) based on kernel density approaches or the maximum entropy method might be arduous in case of complex problems, such as when design parameters n \\u03c6 are large, or the sensitivity for some design parameters is complex [44]. In the SSO framework, such approximation of \\u03c0(\\u03c6|I (i) ) is avoided. In SSO, samples distributed as \\u03c0(\\u03c6|I (i) ) are obtained, and the information in these samples is exploited to identify a smaller subset of the design space with a high likelihood of containing the optimal design parameters.\", \"Samples distributed as \\u03c0(\\u03c6, \\u03b8|I (i) ) are obtained using any appropriate stochastic sampling algorithm, such as Markov Chain Monte Carlo (MCMC) sampling [46]. The \\u03c6 component of these samples then corresponds to samples from the marginal distribution \\u03c0(\\u03c6|I (i) ).\", \"The sensitivity of objective function E \\u03b8 [h s (\\u03c6, \\u03b8)] to \\u03c6 is determined by evaluating the average value (or equivalently volume density) of J(\\u03c6| I (i) ) over any subset I in I (i) , which is denoted by H(I) and defined as:\", \"where, V I is the volume of subset I. Based on the samples distributed according to \\u03c0(\\u03c6|I (i) ) belonging to I (i) , an estimate of H(I) is provided by:\", \"where, N I (i) is the number of samples distributed as \\u03c0(\\u03c6|I (i) ) belonging to I (i) , and N I denotes the number of samples from \\u03c0(\\u03c6|I (i) ) belonging to the , where\", \"\\u03c1 is a set of admissible subsets in I (i) , that contains the smallest volume density N I / V I , that is,\", \"The effectiveness of SSO is dependent on the correct selection of the geometrical shape and size of the admissible subsets. Choosing a geometrical shape that effectively investigates the sensitivity of the objective function to each design variable is essential. The optimization in (10) determines the subset with the smallest average value of J(\\u03c6|I (i) ) (or equivalently E \\u03b8 [h s (\\u03c6, \\u03b8)]) within the admissible set A (i+1) \\u03c1 . I (i + 1) is a subset of the design space I (i) with a high likelihood of containing the optimal design parameters. The above steps are repeated until the stopping criterion is met. This way, SSO adaptively converges to a relatively small subregion within the original design space. The implementation of SSO is demonstrated in Fig. 1. The reader may refer to the original publication for a detailed explanation of SSO [39].\", \"] is more sensitive to \\u03c6, and vice versa. A high value of H(I (i) ), close to 1 corresponds to a sample density in design space I (i) that approximates a uniform distribution and suggests that the identified subset I (i) has a low likelihood of containing \\u03c6* [39]. Therefore, the SSO is stopped when H(I (i) ) exceeds a threshold value. A threshold value of 0.75-0.80 has been found to give satisfactory results [39].\"]}, {\"header\": \"Proposed approach\", \"paragraph\": [\"In the proposed approach, the Voronoi tessellation is implemented to partition the design space into non-overlapping subregions (a set of Voronoi cells) using the pool of samples distributed according to this auxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a space into convex polygons, called Voronoi cells, such that each cell contains exactly one sample, called a cell-generating sample. Every sample in a given polygon is closer to its generating sample compared to any other. In the proposed approach, the admissible set (a set of all admissible subspaces) is defined as a set containing all subsets of the set of Voronoi cells. An alternative approach to identify the optimal subset without performing any non-smooth deterministic optimization is also presented. The general theoretical and computational framework for the iSSO algorithm is presented in the following subsections, and the algorithm is demonstrated in Fig. 2.\"]}, {\"header\": \"Partitioning of design space\", \"paragraph\": [\"In the proposed approach, at the i + 1th iteration, say N (i) is the number of samples distributed as \\u03c0(\\u03c6|I (i) ) belonging to the design space\", \"0 be the number of unique samples. If sampling techniques such as accept rejection, importance sampling, etc., are used, then \\u03b3 = 0, and each sample in the design space will be unique. However, if MCMC sampling techniques are used, the resulting samples will be correlated, that is \\u03b3 > 0, and we will have repeated samples.\", \"Assume that the design space\", \"k , k = 1\\u22c5\\u22c5\\u22c5n v , Voronoi cells using n v unique samples, and say the Voronoi cell v\", \"k repeated samples, then, an estimate of \\u03c0(\\u03c6|I (i) ) is provided by:\", \"where,\", \"k is the volume of the k th Voronoi cell. Obviously,\", \"Similar to the original SSO, the sensitivity of the objective function J (\\u03c6|I (i) ) to \\u03c6 is determined by evaluating the average value of J(\\u03c6|I (i) ) over any subspace I of the design space I (i) . Subset I is any subset of n v Voronoi cells (these cells may be disjointed). Since the design space is partitioned into n v subspaces or Voronoi cells, the number of admissible subsets (proper subsets) is given by 2 n\\u03bd -1. Based on the estimate \\u03c0(\\u03c6|I (i) ) provided in (11), an estimate of H(I) is provided as:\", \"where, V I is the volume of the subset I and N I is the number of samples belonging to it. Let I = {v\", \"(S) }, where S is the number of Voronoi cells defining the subset I. Note that the parentheses are used in the subscript to differentiate between the Voronoi cell number defined in the previous section from the Voronoi cell index describing the subset I. An estimate of H(I) is then provided as:\", \"] .\", \"(13)\"]}, {\"header\": \"Identification of an optimal subset\", \"paragraph\": [\"A deterministic optimization needs to be performed to identify a subset I that contains the smallest volume density N I /V I . In the case of unique samples, since \\u03b7 (i) (\\u22c5) = 1, the solution to the minimization problem in (10) is a set of \\u03c1N (i) Voronoi cells with the largest volume. For the case with repeated samples, the optimization can be performed using methods appropriate for non-smooth optimization problems, such as sub-gradient methods, bundle methods, gradient sampling methods, etc.\", \"In this study, we propose an alternative approach to identify the optimal subset without performing any non-smooth deterministic optimization. A double-sort algorithm is proposed, which involves sorting the Voronoi cells in ascending order of the sample counts and then in groups of cells with the same sample count in descending order of cell volume. Finally, the top cells containing \\u03c1N (i) samples are selected as an approximate optimal solution from the sorted list.\", \"One may argue that the optimal subset can be obtained by first sorting the Voronoi cells in ascending order of the cell density, defined as \\u03b7 (i)\", \"k , and then by selecting the top cells containing \\u03c1N (i) samples from the sorted list. However, this argument is erroneous because the objective is to minimize\", \"and not\", \"). The effectiveness of the proposed double-sort algorithm is demonstrated in Section 4 with the help of examples.\"]}, {\"header\": \"Simulation of conditional samples\", \"paragraph\": [\"At the i + 1th iteration, \\u03c1N (i) samples distributed as \\u03c0(\\u03c6|I (i + 1) ) are available from the previous iteration. Using these samples as seeds, additional (1 -\\u03c1)N (i + 1) are simulated. The proposed method to simulate additional samples involves two steps: (a) randomly selecting a Voronoi cell within the subset I (i + 1) based on the estimate \\u03c0(\\u03c6|I (i) ) and (b) applying the Metropolis-Hastings algorithm within the selected Voronoi cell.\", \"A Voronoi cell is selected according to the following weights in the first step:\", \"Fig. 2. Illustration of the proposed iSSO algorithm.\", \"To simulate a new sample within a selected Voronoi cell, the sample that generated the selected Voronoi cell or the last simulated sample in the selected Voronoi cell is used as the seed sample, and the Metropolis-Hastings algorithm is implemented. A candidate sample [\\u03c6 c ,\\u03b8 c ] is simulated using the proposal q(\\u03c6 c ,\\u03b8 c |\\u03c6,\\u03b8) and is accepted with the probability min(1, a 0 ), where, a 0 is given as:\", \"In the present study, the proposed PDF is equal to the uniform PDF for design parameters and the initial PDF for uncertain variables, i.e., q (\\u03c6, \\u03b8|\\u03c6 c ,\\u03b8 c ) = p(\\u03c6, \\u03b8). Therefore, on simplifying (15), a 0 is given as:\"]}, {\"header\": \"Stopping criteria\", \"paragraph\": [\"A new stopping criterion is proposed in this study. The convergence of the expected value of the performance measure h(\\u03c6, \\u03b8) with respect to the PDF for \\u03c6 and \\u03b8 in consecutive iterations is used as the stopping criterion. Mathematically the proposed stopping criterion is represented by:\", \"where, \\u03b5 is a user-specified tolerance limit. Other stopping criteria, as indicated in [39,47], can also be chosen.\"]}, {\"header\": \"Implementation issues\", \"paragraph\": [\"An important issue for the effective implementation of the iSSO is the creation of the Voronoi cells at the current iteration bounded within the Voronoi cell created at the previous iterations. Although it is possible to create such bounded Voronoi cells, due to the geometrical complexities, it is usually unfeasible for the higher dimensional problems (n \\u03c6 >2). An alternative approach is proposed in the present study for creating the Voronoi cells at any iteration of the iSSO. The proposed approach involves creating Voronoi cells using the samples generated at the current and all previous iterations and then by considering Voronoi cells corresponding to the samples from the current iteration. This is shown in Fig. 3, where Fig. 3(a) shows the N samples at the first iteration and the corresponding Voronoi cells. Fig. 3(b) shows the \\u03c1N selected Voronoi cells leading to the smallest volume density and the additional (1 -\\u03c1)N samples being generated using these \\u03c1N samples as seeds. Fig. 3(c) shows that the Voronoi cells are generated using all N + (1 -\\u03c1)N samples that are generated in the two iterations. The Voronoi cells corresponding to the N samples for consideration at the second iteration are also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of Fig. 3(c) where it can be observed that the area covered by the N Voronoi cells considered in the second iteration is not the same as the area covered by the \\u03c1N Voronoi cells selected in the first iteration. On the contrary, the area covered by the Voronoi cells in the second iteration is more than the area covered by the Voronoi cells corresponding to the seed samples from the first iteration. This is because a new sample within the Voronoi cell between an existing sample and the existing Voronoi cell edge results in the relocation of the Voronoi cell edge in a Fig. 3. Implementation of Voronoi tessellation in iSSO.\", \"direction away from the new sample. The increase at each iteration introduces a bias in the estimate of \\u03c0(\\u03c6|I (i) ) in (11). However, this does not affect the performance of the proposed approach as the objective is not to simulate the samples distributed as \\u03c0(\\u03c6|I (i) ) but to identify the subsets for an optimal solution. In addition, the increase is not substantial, as seen later in the illustrative examples in Section 4.\"]}, {\"header\": \"Special case: deterministic optimization\", \"paragraph\": [\"In the iSSO framework, a deterministic optimization problems can also be handled with the vector of uncertain variables \\u03b8 set equal to a null vector (n \\u03b8 = 0). Since the determination of the subset at each iSSO iteration is solely dependent on the samples distributed as \\u03c0(\\u03c6), no modification to the iSSO algorithm is required to solve a deterministic optimization problem, and the entire formulation remains valid.\"]}, {\"header\": \"Illustrative examples\", \"paragraph\": [\"In this section, typical optimization problems are considered to demonstrate the effectiveness and efficiency of the proposed approach. First, deterministic optimization problems are considered. These problems include several local and global minima. Next, stochastic optimization problems are illustrated. The second example presents an RDO problem of the TMD. In this example, the variance minimization of the protected structure\'s displacement (TMD attached to the structure) is performed. In the third example, the mean minimization of 120 bars truss problems is explored to demonstrate the applicability of the proposed approach to a high-dimensional stochastic design problem. Finally, the fourth example investigates the reliability-based optimization of a base isolation system for a 10-story building.\", \"In this study, after implementing iSSO, the optimal design solution is identified as follows. Let \\u03b8 j , j = 1\\u22c5\\u22c5\\u22c5n be a set of independent, identically distributed realizations of \\u03b8, and let h(\\u03c6, \\u03b8 j ) be the structural performance function realization for \\u03b8 j . The expected structural performance function is approximated by the average of the realizations as:\", \"E \\u03b8 [h(\\u03c6, \\u03b8)] is evaluated for all unique \\u03c6 samples obtained at the last iteration of the iSSO, and the \\u03c6 sample resulting in the smallest value of E \\u03b8 [h(\\u03c6, \\u03b8)] is taken as the optimal solution. Alternatively, as the righthand side of ( 18) is deterministic, any deterministic optimization method can also be used to solve the optimization problem with the approximate expectation.\", \"In the following examples, both iSSO and SSO are implemented with N = 1000n \\u03c6 , \\u03c1 = 0.20 and the stopping criteria as stated in (17). Here, a value of \\u03b5 = 10 -3 is adopted.\", \"Fig. 4. Results for the Griewank function.\"]}, {\"header\": \"Multimodal deterministic optimization problems\", \"paragraph\": [\"In this section, three two-dimensional benchmark deterministic optimization problems are considered. Results are also compared with the SSO. The test functions are: a) Griewank function:\", \"b) Cross-in-Tray function:\", \"c) Holder Table function:\", \"The results for the Griewank function are presented in Fig. 4. Fig. 4(a,b) shows that the function has multiple closely spaced local minima with a single global minimum. Fig. 4(c,d) shows the SSO optimization using hyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is seen that these shapes fail to capture the region containing the optimal design due to the presence of multiple local minima. Next, the iSSO is implemented, where the Voronoi cells selected at the first and last iteration are shown in Fig. 4(e,f). It is observed that at the first iteration, the selected Voronoi cells effectively capture both the local and global minima and in the subsequent iterations, the selected cells are more concentrated near the global minimum. The region selected at the last iteration captures the optimal global solution.\", \"The Cross-in-Tray function has a relatively complex design space compared to the Griewank function. Fig. 5(a,b) shows multiple local and global minima. Minimization by using SSO is demonstrated in Fig. 5  (c,d). It is found that both the hyper-rectangle and hyper-ellipse are trapped around any one of the global minima. At the same time, the iSSO is able to capture the regions that include all of the global minima, as seen in Fig. 5(e,f).\", \"The Holder Table function has multiple local and global minima; the global minima are placed at the boundary of the design space, as shown in Fig. 6(a,b). Once again, it is seen that both the hyper-rectangle and hyper-ellipse are trapped around any of one of the global minima, and on the other hand, the iSSO is able to capture the regions that include all of the global minima, as seen in Fig. 6(e,f).\", \"The results from the three examples demonstrate that the proposed iSSO is able to capture the regions containing the optimal solution effectively.\", \"Next, the statistics of the results of 50 independent runs, both for SSO and iSSO are presented in Table 1. It also includes the results obtained by using state-of-the-art approaches, such as the Genetic algorithm, particle swarm optimization, and the gradient based optimization approach (interior-point algorithm). The proposed iSSO outperforms all other approaches as more successes in determining the optimal solution are observed in all three optimization problems. It is also seen that both SSO and iSSO result in a similar value of volume reduction for the same stopping criterion; however, with SSO, the number of iterations required to achieve this volume reduction are relatively higher. The proposed approach outperformed the state-of-the-art approaches, as indicated by the number of successes. These examples demonstrate that the main advantage of implementing Voronoi tessellation is an effective exploration of the design space.\", \"Next, the performance of the proposed \\\"double sort algorithm\\\" for selecting the optimal subset is studied by using the above-mentioned three functions. Fig. 7 shows the value of H(I (1) ) for the 50 independent simulation runs, which is estimated by implementing the proposed double sort algorithm and by using the Genetic algorithm. It can be noted that for each run, the H(I (1) ) values obtained using the proposed double sort algorithm and Genetic algorithm are well matched, thereby confirming the adequacy of the proposed double sort algorithm.\", \"At any iteration of iSSO, new samples are simulated using the seed samples. In the proposed approach, the volume of the Voronoi cells corresponding to the seed and new samples is greater than the volume of the Voronoi cells corresponding only to the seed samples. Fig. 8 shows\", \"due to the creation of Voronoi cells at any generation of iSSO using the procedure mentioned in Section 3.4. The increase is observed to be small which further reduces with an increase in the iteration number. It is also observed that the increase in volume decreases with an increase in sample size at each iteration and increases with an increase in the dimension of the problem.\"]}, {\"header\": \"Robust design optimization of the tuned mass damper\", \"paragraph\": [\"This example considers a stochastic design problem involving a Tuned Mass Damper (TMD) attached to a Single Degree of Freedom (SDOF) system. The problem is taken from [48] and is shown in Fig. 9.\", \"In this problem, the system is excited by a white noise signal with a mean zero and unit variance. The performance measure is the variance of the displacement of the system \\u03c3 2 xs . The mass m S , stiffness k S , and\", \"Table 2 presents the optimal design parameter values as well as the objective function value that solve the optimization problem in (22). Results obtained using SSO, Sample Average Approximation (SAA), and iSSO are shown. SAA is applied with a sample size of 10 3 , as mentioned in [43]. The results demonstrate that iSSO is effective in locating the optimal solution. SSO implemented with hyper-ellipse gives an optimal solution but has a higher computational cost.\"]}, {\"header\": \"120-bars truss structure\", \"paragraph\": [\"The third example involves minimizing the mean of the compliance of a 120-bar linear elastic truss structure shown in Fig. 10 under the weight constraint W \\u2264 15, 000kg. Because of structural symmetry, design parameters corresponding to the cross-sectional areas of elements are divided into seven groups, each with a minimum area of 10 -4 m 2 . The Young\'s modulus for the bar groups are assumed as uncorrelated normal random variables with mean values equal to 210 GPa and the c. o.v equal to 0.10 respectively. The density of the material is 7971.89 kg/ m 3 . The dome is subjected to concentrated vertical loads acting downward at the top node, normally distributed with a mean equal to 60 kN and c.o.v equal to 0.20. In addition, the mass of bars is concentrated at the nodes. The problem is taken from [48].\", \"Table 3 presents the best of 10 independent run results obtained with SSO and iSSO. Once again, the SSO and iSSO solutions agree well, thereby demonstrating the effectiveness of the proposed approach. At the same time, the number of function evaluations is substantially less in the case of iSSO, indicating the efficiency of the proposed approach.\"]}, {\"header\": \"Reliability-based design of a base isolated structure\", \"paragraph\": [\"This example, adapted from [49], involves the reliability-based Fig. 9. TMD attached to a SDOF system [48]. Fig. 10. 120-bar dome truss structure [48].\"]}, {\"header\": \"Table 3\", \"paragraph\": [\"Results for the 120 bars truss structure.\"]}, {\"header\": \"Method Design parameters\", \"paragraph\": [\"\\u03bcg(\\u03c6 * ) optimization of a base-isolation system attached to a 10-story building as shown in Fig. 11. This optimization problem includes maximizing the reliability of the base-isolated structure which is performed by the minimization of its failure probability and mathematically expressed as:\", \"where, I F (\\u03c6,\\u03b8) is the function that indicates failure, and it equals 1 when the system fails, i.e., when unacceptable performance occurs. Notably, in this problem h(\\u03c6, \\u03b8) = I F (\\u03c6,\\u03b8). The 10-story building is considered as a shear structure with uncertain inter-story stiffness and damping. Each story has a total mass of 207 ton. The inter-story stiffness k i of all stories are parameterized by\", \"The damping ratios are considered independent Gaussian variables with mean values of 0.025 and c.o.v of 0.10 for all modes. The Kanai-Tajimi model is used to simulate the ground excitation modelled as a filtered white noise process, with the power spectral density function given as:\", \"where, \\u03c9 g , \\u03b6 g and \\u03c3 \\u03c9 are the resonant frequency, damping, and RMS of the acceleration input of the filter, respectively. These are also considered uncertain variables with mean values of [2\\u03c0rad/s, 0.5, 0.2g] and a c.o.v equal to 0.20. The non-stationarity of the excitation is modeled by multiplying the filter output with the envelope function as:\", \"with parameters \\u03bb 1 = 1.25, \\u03bb 2 = 0.2 and \\u03bb 3 = 0.353 chosen to simulate strong earthquake excitation for a duration of 40 s with a sampling time of 0.02 s. The base-isolation system considered is a lead-rubber bilinear isolator with an additional viscous damper. The base has a 247-ton mass.\", \"The design parameters \\u03c6 for the base isolation structure system are the stiffness before yielding K pr and after yielding K p , the yield force is F y , and the damping coefficient c d . The reader may refer to [39,50] for additional details regarding the base isolation structure system adopted in this study. Failure is indicated when any of the normalized base displacements or inter-story drifts exceeds unity. The normalization constants are 0.5 m and 0.033 m respectively. The design interval for each variable is specified as K pr = [50, 600] MN/m, F y = [1, 8] MN, K p = [5, 60] MN/m, and c d = [0.1, 10]MNs/m. In this example, iSSO and SSO are implemented with six number of iterations.\", \"Table 4 shows the optimization results for the best 10 independent simulation runs. The comparison of the results obtained using SSO, SAA (with a sample size of 10 3 ), and iSSO shows that the optimal design obtained using the proposed approach iSSO is in good agreement. The failure probability of the structure is reduced from 0.95 (without the base isolation system) to 0.0326 after installing the optimally designed base isolation system.\"]}, {\"header\": \"Conclusion\", \"paragraph\": [\"This study attempts to provide an optimization approach called \\\"iSSO\\\", which is an improved version of SSO, primarily for stochastic optimization problems while it retains utility for deterministic optimization problems as well. Two novel ideas are introduced in this study: first, a better characterization of the design space is offered by partitioning the design space into non-overlapping subregions using Voronoi  tessellation which improves the effectiveness and efficiency of the proposed iSSO considerably in comparison to SSO. Second, a novel \\\"double sort\\\" approach is proposed, eliminating the need for optimization to identify the subregions for the optimal design at each iSSO iteration. Several mathematical and engineering design examples, including TMD, 120 bars truss structure, and base-isolated structure, are included in this study to demonstrate the efficacy of the proposed iSSO. The results show that the proposed iSSO effectively identifies the reduced design space for complex design problems with multiple global and local minima. This is attributable to the Voronoi tessellation, which eliminates the requirement of the presumed admissible design space form to resemble the contour of the original design. Voronoi tessellation enabled better design space exploration, allowing multiple global minima scattered throughout the design pace to be effectively identified. Due to the discretization of the design space via Voronoi tessellation, computation demand is significantly reduced as the number of function evaluations for all examples is lower vis-a-vis the original SSO. Moreover, the novel idea of the double sort approach achieves the requisite precision in identifying the subregions for optimal solutions and makes iSSO implementation simple and effective. The applicability of the approach is dependent on the creation of the Voronoi cells. At present the methods available in the literation for creating the Voronoi tessellation are computationally demanding when considering problems of very high dimension. Future work will focus on developing a method for creating the Voronoi tessellation in higher dimensions, particularly those greater than ten.\"]}, {\"header\": \"CRediT authorship contribution statement\", \"paragraph\": [\"Mohd Aman Khalid: Investigation, Methodology, Formal analysis, Software, Visualization, Writingoriginal draft. Sahil Bansal: Conceptualization, Methodology, Supervision.\"]}, {\"header\": \"Data availability\", \"paragraph\": [\"No data was used for the research described in the article.\"]}, {\"header\": \"Data availability\", \"paragraph\": [\"No data was used for the research described in the article.\"]}, {\"header\": \"Declaration of Competing Interest\", \"paragraph\": [\"The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\", \"Voronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or Dirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with each polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each polygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more points in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space. There are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the Voronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm [51], which generates a Delaunay triangulation in any number of dimensions, can be applied while creating a Voronoi diagram. The Delaunay triangulation is a triangulation of the point in which no point falls within the circumcircle of any triangle. The polygon generated by the intersection of the half-planes defined by the edges of the Delaunay triangles enclosing the point is therefore obtained as the Voronoi cell of a point.\", \"It can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to a set of points. Voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and microstructure study [52]. The MATLAB command \\\"Voronoin\\\" from the \\\"Parallel Computing Toolbox\\\" [54] has been used in this study to create the Voronoi cells.\"]}, {\"header\": \"Declaration of Competing Interest\", \"paragraph\": [\"The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\"]}, {\"header\": \"Appendix-A: Voronoi Tessellation\", \"paragraph\": [\"Voronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or Dirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with each polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each polygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more points in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space. There are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the Voronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm [51], which generates a Delaunay triangulation in any number of dimensions, can be applied while creating a Voronoi diagram. The Delaunay triangulation is a triangulation of the point in which no point falls within the circumcircle of any triangle. The polygon generated by the intersection of the half-planes defined by the edges of the Delaunay triangles enclosing the point is therefore obtained as the Voronoi cell of a point.\", \"It can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to a set of points. Voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and microstructure study [52]. The MATLAB command \\\"Voronoin\\\" from the \\\"Parallel Computing Toolbox\\\" [54] has been used in this study to create the Voronoi cells.\"]}]','https://drive.google.com/uc?id=1egG5Ukf83yzdcmwUqxoHFmdAAW2OZpEN&export=download','2023-02-03',0,'2024-02-03 22:46:05.755529','2024-02-03 22:46:12.054794'),(17,'Fuzzy logic based MPPT control for a PV system using SEPIC converter','In this study, a novel single-ended primary inductor (SEPIC) converter-based fuzzy logic controller for maximum power point tracking is presented. By adding rules to the perturb and observing search strategy, the new controller enhances it while fuzzifying and removing its flaws. When compared to traditional maximum power point tracking techniques, fuzzy logic trackers enable an accurate and quick convergence to maximum power point under both steady-state and variable weather situations. The performance of the proposed maximum power point tracker is demonstrated in simulation.','[{\"header\": \"I. INTRODUCTION\", \"paragraph\": [\"The non-renewable energy sources are rapidly running out, while the electricity demand is increasing daily. To solve this problem, efficient and efficient electric power generation from renewable energy sources is required [1]. Renewable energy is one of the forms of energy that society can rely on because It is unpolluted, pure, and has no limits. One type of power generation that uses renewable energy is the photovoltaic (PV) system [2]. To utilize less conventional energy, the PV system must subsequently be linked to the grid, either directly or via a backup battery bank. Since the power produced by PV systems depends on radiation and temperature change, the PV framework has destitute productivity, [2].\", \"For the control of the PV systems, there are different sorts of DC-DC converters such as Buck converters, Boost converters, and Buck-Boost converters. Due to its output pick-up adaptability, a single-ended primary-inductor converter (SEPIC) acts as a buck-boost DC/DC converter, where it changes its output voltage agreeing to its duty cycle. Unlike the customary buck-boost converter, the SEPIC converter includes a non-inverted output and it uses an arrangement capacitor to separate input from output [3]. The buck and buck-boost converters lose half of their input control due to input current arrangement exchange; for that reason, the two types of converters should be excluded from maximum power applications. The boost converter has a nonstop input current, but the output voltage is always bigger than the input, which may not accomplish maximum power exchange operation in a few cases, such as when the maximum voltage is less than the input [3]. This paper presents a fuzzy-based P&O strategy for an MPPT standalone PV system. The proposed MPPT can abuse the preferences of the P&O strategy and eliminate its drawbacks. Output has been separated into five fuzzy subsets. As the proposed strategy continuously exchanges maximum power from PV arrays, it optimizes the number of PV modules.\"]}, {\"header\": \"II. MODELIGN OF PV SYSTEM\", \"paragraph\": [\"Photovoltaic is the technique and study connected to devices that directly convert sunlight into electricity utilizing photovoltaic semiconductors. Direct conversion of solar energy into DC electrical energy can be achieved by photovoltaic cells [4]. The photovoltaic panel is made up of numerous cells that are connected in series Ns or shunt Nsh. Where it may be mimicked by a current source coupled in parallel with a diode as described by and depicted in Figure 1 [5]. The following equations provide the output current:\", \"(2)\", \"The shunt resistance (R sh ) is typically orders of magnitude larger than the series resistance (R s ) [6]. Therefore, it is common for the shunt and series resistances of a solar cell can be neglected to simplify the model. The resulting ideal voltagecurrent characteristic of the solar cell is given by equation ( 3).\"]}, {\"header\": \"III. SEPIC CONVERTER\", \"paragraph\": [\"Power electronics researchers are working hard to create DC-DC converters with simpler designs and greater efficiency [7]. To maintain a constant output voltage, the suggested DC-to-DC converter employs a single-ended primary-inductor converter (SEPIC) architecture. The SEPIC converter is made up of a duty cycle switch S, a diode, two inductors (L1 and L2), two capacitors (C1 and C2), and a load resistor. Figure 2 depicts the circuit diagram of a SEPIC converter. A SEPIC is a DC-DC converter [8]. SEPIC are DC-DC converters that can output voltages that are B, larger than, or equal to the input voltage. The duty cycle of the control transistor affects the SEPIC converter\'s output voltage. The SEPIC converter is two converters in one: a boost converter followed by a buck-boost converter. It has the advantages of having a non-inverted output (the output voltage has the same polarity as the input voltage) , using a series capacitor to couple energy from the input to the output (which makes it more responsive to short-circuits), and being able to shut down completely: when the switch \\\"S\\\" is turned off, the output voltage drops to 0 V, accompanied by a significant transient discharge of charge.   When the switch is turned on, the input inductor is charged from the source, and the second inductor is charged from the first capacitor. No energy is supplied to the load capacitor during this time. The inductor current and capacitor voltage polarities are marked in this Figure . When the power switch is turned off, the energy stored in the inductor is transferred. The energy stored is transferred through the diode and supplies the energy to the load [10], as shown in Figure 3. b. The second inductor is also connected to the load during this time. The output capacitor sees a pulse of current during the off time, making it inherently noisier than a buck converter. The amount that the SEPIC converters increase or decrease the voltage depends primarily on the duty cycle and the parasitic elements in the circuit. The output of an ideal SEPIC converter is:\", \"A SEPIC converter is to process the electricity from the PV system. This converter either increases or decreases the PV system voltage at the load. The proposed SEPIC converter operates in buck mode.\"]}, {\"header\": \"IV. FUZZY LOGIC CONTROL\", \"paragraph\": [\"In the fuzzy logic maximum power point tracking (MPPT) algorithm, the voltage and current at each instant k are measured to calculate the active power. The active power is then compared with the power at the previous instant (k-1) to obtain the change in power (\\u0394P(k)). Similarly, the voltage at instant k is compared with the voltage at instant k-1 to obtain the voltage error (\\u0394V(k)) [11]. The power error is then divided by the current error to obtain the error (E). The error is then compared with the previous error to calculate the change in error (\\u0394E(k)). The error (E(k)) and the change in error (\\u0394E(k)) are then used as the crisp inputs to the fuzzy logic controller. The flow chart for the fuzzy logic MPPT algorithm is shown in Figure 4. In this work, the Mamdani inference technique, Atype membership functions, and a 25-element rule base were used for the fuzzy logic control. The Mamdani inference technique is efficient and straightforward in defining the fuzzy output sets, and it is more popular among researchers than other inference techniques [12]. The A-type or triangular membership function is used because it is simpler to split into low and high membership functions (MFs) than other membership functions. Additionally, it has been observed that the triangular membership function has a faster response and less overshoot than other functions [13]. A 25-element rule base was used because it has been shown to perform well [14][15]. The following are the fuzzy rules in Table 1, which are used for the desired MPP of push-pull converter PWM. The membership for input variables (DPpv, DVpv) are shown in Figure 5, and the membership for output variable (DVpv*) is shown in Figure 6. All the functions are defined on a normalized interval [-1 1].\"]}, {\"header\": \"V. SIMULATION RESULTS\", \"paragraph\": [\"The characteristics of the photovoltaic array that we use in this paper are given in Table 2. Current at maximum power I max = 6.9 A Table 3 shows the SEPIC converter settings utilized in this study. The SEPIC converter is linked to the PV panel in the full model, and the duty cycle is regulated by the Fuzzy Logic Controller. The results are provided under standard test conditions; G=1000 W/m2; T=25\\u00b0C and it is shown in figure 7.   Overall, using MPPT and fuzzy logic to a SEPIC converter for a PV system result in considerable performance gains. Increased power output, higher efficiency, decreased ripple, enhanced transient response, and resilience are examples of these enhancements.\"]}, {\"header\": \"Table. 2. Electrical data\", \"paragraph\": [\"\\u2165. CONCLUSIONS This paper presents the design of an off-grid photovoltaic system with a fuzzy logic MPPT-controlled push-pull boost converter. The proposed system was simulated in MATLAB/Simulink and tested under various weather conditions. The results showed that the fuzzy logic algorithm outperformed the conventional algorithms in terms of MPPT accuracy and minimization of fluctuations, regardless of rapid changes in irradiance.\"]}]','https://drive.google.com/uc?id=1cqcj-_NV2Kkp9AJ1kISisTycUBVQE116&export=download',NULL,0,'2024-02-03 22:46:14.563208','2024-02-03 22:46:16.154184'),(18,'SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes','The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs.','[{\"header\": \"Motivation and significance\", \"paragraph\": [\"With the increasing complexity and accuracy of numerical models, it has become more challenging to run complex simulations and computer codes [1,2]. As a consequence, surrogate models have been recognized as a key tool for engineering tasks such as design space exploration, uncertainty quantification, and optimization [3]. In practice, surrogate models are used to reduce the computational effort of these tasks by replacing expensive numerical simulations with closed-form approximations [4,Ch. 10]. To build such a model, we start by evaluating the original expensive simulation at a set of points through a Design of Experiments (DoE). Then, the corresponding evaluations are used to build the surrogate model according to the chosen approximation, such as Kriging, quadratic interpolation, or least squares regression.\", \"The Surrogate Modeling Toolbox (SMT) is an open-source framework that provides functions to efficiently build surrogate models [5]. and number of instantiated elements. When design problems include both discrete variables and continuous variables, they are said to have mixed variables.\", \"When architectural choices lead to different sets of design variables, we have hierarchical variables [21,22]. For example, consider different aircraft propulsion architectures [23]. A conventional gas turbine would not require a variable to represent a choice in the electrical power source, while hybrid or pure electric propulsion would require such a variable. The relationship between the choices and the sets of variables can be represented by a hierarchy.\", \"Handling hierarchical and mixed variables requires specialized surrogate modeling techniques [24]. To address these needs, SMT 2.0 is offering researchers and practitioners a collection of cutting-edge tools to build surrogate models with continuous, mixed and hierarchical variables. The main objective of this paper is to detail the new enhancements that have been added in this release compared to the original SMT 0.2 release [5]. There are two new major capabilities in SMT 2.0: the ability to build surrogate models involving mixed variables and the support for hierarchical variables within Kriging models. To handle mixed variables in Kriging models, existing libraries such as BoTorch [25], Dakota [26], DiceKriging [27], LVGP [28], Parmoo [29], and Spearmint [30] implement simple mixed models by using either continuous relaxation (CR), also known as one-hot encoding [30], or a Gower distance (GD) based correlation kernel [31]. KerGP [32] (developed in R) implements more general kernels but there is no Python open-source toolbox that implements more general kernels to deal with mixed variables, such as the homoscedastic hypersphere (HH) [33] and exponential homoscedastic hypersphere (EHH) [34] kernels. Such kernels require the tuning of a large number of hyperparameters but lead to more accurate Kriging surrogates than simpler mixed kernels [34,35]. SMT 2.0 implements all these kernels (CR, GD, HH, and EHH) through a unified framework and implementation. To handle hierarchical variables, no library in the literature can build peculiar surrogate models except SMT 2.0, which implements two Kriging methods for these variables. Notwithstanding, most softwares are compatible with a na\\u00efve strategy called the imputation method [24] but this method lacks depth and depends on arbitrary choices. This is why Hutter and Osborne [21] proposed a first kernel, called Arc-Kernel which in turn was generalized by Horn et al. [36] with a new kernel called the Wedge-Kernel [37]. None of these kernels are available in any open-source modeling software. Furthermore, thanks to the framework introduced in Audet et al. [38], our proposed kernels are sufficiently general so that all existing hierarchical kernels are included within it. Section 4 describes the two kernels implemented in SMT 2.0 that are referred as SMT Arc-Kernel and SMT Alg-Kernel. In particular, Alg-Kernel is a novel hierarchical kernel introduced in this paper. Table 1 outlines the main features of the state-of-the-art modeling software that can handle hierarchical and mixed variables.\", \"SMT 2.0 introduces other enhancements, such as additional sampling procedures, new surrogate models, new Kriging kernels (and their derivatives), Kriging variance derivatives, and an adaptive criterion for high-dimensional problems. SMT 2.0 adds applications of Bayesian optimization (BO) with hierarchical and mixed variables or noisy co-Kriging that have been successfully applied to aircraft design [39], data fusion [40], and structural design [41]. The SMT 2.0 interface is more user-friendly and offers an improved and more detailed documentation for users and developers. The remainder of the paper is organized as follows. First, we introduce the organization and the main implemented features of the release in Section 2. Then, we describe the mixed-variable Kriging model with an example in Section 3. Similarly, we describe and provide an example for a hierarchical-variable Kriging model in Section 4. The Bayesian optimization models and applications are described in Section 5. Finally, we describe the other relevant contributions in Section 6 and conclude in Section 7.\"]}, {\"header\": \"SMT 2.0 : an improved surrogate modeling toolbox\", \"paragraph\": [\"From a software point of view, SMT 2.0 maintains and improves the modularity and generality of the original SMT version [5]. In this section, we describe the software as follows. Section 2.1 describes the legacy of SMT 0.2. Then, Section 2.2 describes the organization of the repository. Finally, Section 2.3 shows the new capabilities implemented in the SMT 2.0 update.\"]}, {\"header\": \"Background on SMT former version: SMT 0.2\", \"paragraph\": [\"SMT [5] is an open-source collaborative work originally developed by ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of Michigan. Now, both Polytechnique Montr\\u00e9al and the University of California San Diego are also contributors. SMT 2.0 updates and extends the original SMT repository capabilities among which the original publication [5] focuses on different types of derivatives for surrogate models detailed hereafter. A Python surrogate modeling framework with derivatives. One of the original main motivations for SMT was derivative support. In fact, none of the existing packages for surrogate modeling such as Scikit-learn in Python [7], SUMO in Matlab [42] or GPML in Matlab and Octave [43] focuses on derivatives. Three types of derivatives are distinguished: prediction derivatives, training derivatives, and output derivatives. SMT also includes new models with derivatives such as Kriging with Partial Least Squares (KPLS) [44] and Regularized Minimal-energy Tensor-product Spline (RMTS) [3]. These developed derivatives were even used in a novel algorithm called Gradient-Enhanced Kriging with Partial Least Squares (GEKPLS) [6] to use with adjoint methods, for example [45].\", \"Software architecture, documentation, and automatic testing. SMT is organized along three main sub-modules that implement a set of sampling techniques (sampling_methods), benchmarking functions (problems), and surrogate modeling techniques (surrogate_models). The toolbox documentation 6 is created using reStructuredText and Sphinx, a documentation generation package for Python, with custom extensions. Code snippets in the documentation pages are taken directly from actual tests in the source code and are automatically updated. The output from these code snippets and tables of options are generated dynamically by custom Sphinx extensions. This leads to high-quality documentation with minimal effort. Along with user documentation, developer documentation is also provided to explain how to contribute to SMT. This includes a list of API methods for the SurrogateModel, SamplingMethod, and Problem classes, that must be implemented to create a new surrogate modeling method, sampling technique, or benchmarking problem. When a developer submits a pull request, it is merged only after passing the automated tests and receiving approval from at least one reviewer. The repository on GitHub7 is linked to continuous integration tests (GitHub Actions) for Windows, Linux and MacOS, to a coverage test on coveralls.io and to a dependency version check for Python with DependaBot. Various parts of the source code have been accelerated using Numba [46], an LLVM-based just-in-time (JIT) compiler for numpy-heavy Python code. Numba is applied to conventional Python code using function decorators, thereby minimizing its impact on the development process and not requiring an additional build step. For a mixed Kriging surrogate with 150 training points, a speedup of up to 80% is observed, see Table 2. The JIT compilation step only needs to be done once when installing or upgrading SMT and adds an overhead of approximately 24 s on a typical workstation In this paper, all results are obtained using an Intel \\u00ae Xeon \\u00ae CPU E5-2650 v4 @ 2.20 GHz core and 128 GB of memory with a Broadwellgeneration processor front-end and a compute node of a peak power of 844 GFlops.\"]}, {\"header\": \"Organization of SMT 2.0\", \"paragraph\": [\"The main features of the open-source repository SMT 2.0 are described in Fig. 1. More precisely, Sampling Methods, Problems and Surrogate models are kept from SMT 0.2 and two new sections Models applications and Interactive notebooks have been added to the architecture of the code. These sections are highlighted in blue and detailed on Fig. 1. The new major features implemented in SMT 2.0 are highlighted in lavender whereas the legacy features that were already in present in the original publication for SMT 0.2 [5] are in black.\"]}, {\"header\": \"New features within SMT 2.0\", \"paragraph\": [\"The main objective of this new release is to enable Kriging surrogate models for use with both hierarchical and mixed variables. Moreover, for each of these five sub-modules described in Section 2.2, several improvements have been made between the original version and the SMT 2.0 release.\"]}, {\"header\": \"Hierarchical and mixed design space.\", \"paragraph\": [\"A new design space definition class DesignSpace has been added that implements hierarchical and mixed functionalities. Design variables can either be continuous (FloatVariable), ordered (OrdinalVariable) or categorical (CategoricalVariable). The integer type (IntegerVariable) represents a special case of the ordered variable, specified by bounds (inclusive) rather than a list of possible values. The hierarchical structure of the design space can be defined using declare_decreed_var:\", \"this function declares that a variable is a decreed variable that is activated when the associated meta variable takes one of a set of specified values, see Section 4 for background. The DesignSpace class also implements mechanisms for sampling valid design vectors (i.e. design vectors that adhere to the hierarchical structure of the design space) using any of the below-mentioned samplers, for correcting and imputing design vectors, and for requesting which design variables are acting in a given design vector. Correction ensures that variables have valid values (e.g. integers for discrete variables) [24], and imputation replaces non-acting variables by some default value (0 for discrete variables, mid-way between the bounds for continuous variables in SMT 2.0) [47].\", \"Sampling. SMT implements three methods for sampling. The first one is a na\\u00efve approach, called Random that draws uniformly points along every dimension. The second sampling method is called Full Factorial and draws a point for every cross combination of variables, to have an \'\'exhaustive\'\' design of experiments. The last one is the Latin Hypercube Sampling (LHS) [48] that draws a point in every Latin square parameterized by a certain criterion. For LHS, a new criterion to manage the randomness has been implemented and the sampling method was adapted for multi-fidelity and mixed or hierarchical variables. More details about the new sampling techniques are given in Section 6.1.\", \"Problems. SMT implements two new engineering problems: a mixed variant of a cantilever beam described in Section 3 and a hierarchical neural network described in Section 4.\", \"Surrogate models. In order to keep up with state-of-art, several releases done from the original version developed new options for the already existing surrogates. In particular, compared to the original publication [5], SMT 2.0 adds gradient-enhanced neural networks [45] and marginal Gaussian process [49] models to the list of available surrogates. More details about the new models are given in Section 6.2.\", \"Applications. Several applications have been added to the toolbox to demonstrate the surrogate models capabilities. The most relevant application is efficient global optimization (EGO), a Bayesian optimization algorithm [50,51]. EGO optimizes expensive-to-evaluate black-box problems with a chosen surrogate model and a chosen optimization criterion [52]. The usage of EGO with hierarchical and mixed variables is described in Section 5.  Interactive notebooks. These tutorials introduce and explain how to use the toolbox for different surrogate models and applications. 8 Every tutorial is available both as a .ipynb file and directly on Google colab. 9 In particular, a hierarchical and mixed variables dedicated notebook is available to reproduce the results presented on this paper. 10  In the following, Section 3 details the Kriging based surrogate models for mixed variables, and Section 4 presents our new Kriging surrogate for hierarchical variables. Section 5 details the EGO application and the other new relevant features aforementioned are described succinctly in Section 6. 8 https://github.com/SMTorg/smt/tree/master/tutorial 9 https://colab.research.google.com/github/SMTorg/smt/ 10 https://github.com/SMTorg/smt/tree/master/tutorial/ NotebookRunTestCases_Paper_SMT_v2.ipynb\"]}, {\"header\": \"Surrogate models with mixed variables in SMT 2.0\", \"paragraph\": [\"As mentioned in Section 1, design variables can be either of continuous or discrete type, and a problem with both types is a mixed-variable problem. Discrete variables can be ordinal or categorical. A discrete variable is ordinal if there is an order relation within the set of possible values. An example of an ordinal design variable is the number of engines in an aircraft. A possible set of values in this case could be 2, 4, 8. A discrete variable is categorical if no order relation is known between the possible choices the variable can take. One example of a categorical variable is the color of a surface. A possible example of a set of choices could be blue, red, green. The possible choices are called the levels of the variable.\", \"Several methods have been proposed to address the recent increase interest in mixed Kriging based models [30][31][32][33]35,39,53,54]. The main difference from a continuous Kriging model is in the estimation of\"]}, {\"header\": \"SMT GD exp(-\\ud835\\udf19) [\\ud835\\udef7(\\ud835\\udee9\", \"paragraph\": [\"the categorical correlation matrix, which is critical to determine the mean and variance predictions. As mentioned in Section 1, approaches such as CR [30,39], continuous latent variables [54], and GD [31] use a kernel-based method to estimate the correlation matrix. Other methods estimate the correlation matrix by modeling the correlation entries directly [32,35,53], such as HH [33] and EHH [34]. The HH correlation kernel is of particular interest because it generalizes simpler kernels such as CR and GD [34]. In SMT 2.0, the correlation kernel is an option that can be set to either CR (CONT_RELAX_KERNEL), GD (GOWER_KERNEL), HH (HOMO_HSPHERE_KERNEL) or EHH (EXP_HOMO_HSPHERE_KERNEL).\"]}, {\"header\": \"Mixed Gaussian processes\", \"paragraph\": [\"The continuous and ordinal variables are both treated similarly in SMT 2.0 with a continuous kernel, where the ordinal values are converted to continuous through relaxation. For categorical variables, four models (GD, CR, EHH and HH) can be used in SMT 2.0 if specified by the API. This is why we developed a unified mathematical formulation that allows a unique implementation for any model. Denote \\ud835\\udc59 the number of categorical variables. For a given \\ud835\\udc56 \\u2208 {1, \\u2026 , \\ud835\\udc59}, the \\ud835\\udc56th categorical variable is denoted \\ud835\\udc50 \\ud835\\udc56 and its number of levels is denoted \\ud835\\udc3f \\ud835\\udc56 . The hyperparameter matrix peculiar to this variable \\ud835\\udc50 \\ud835\\udc56 is\", \", and the categorical parameters are defined as \\ud835\\udf03 \\ud835\\udc50\\ud835\\udc4e\\ud835\\udc61 = {\\ud835\\udee9 1 , \\u2026 , \\ud835\\udee9 \\ud835\\udc59 }. For two given inputs in the DoE, for example, the \\ud835\\udc5fth and \\ud835\\udc60th points, let \\ud835\\udc50 \\ud835\\udc5f \\ud835\\udc56 and \\ud835\\udc50 \\ud835\\udc60 \\ud835\\udc56 be the associated categorical variables taking respectively the \\ud835\\udcc1 \\ud835\\udc56 \\ud835\\udc5f and the \\ud835\\udcc1 \\ud835\\udc56 \\ud835\\udc60 level on the categorical variable \\ud835\\udc50 \\ud835\\udc56 . The categorical correlation kernel is defined by\", \"where \\ud835\\udf05 is either a positive definite kernel or identity and \\ud835\\udef7(.) is a symmetric positive definite (SPD) function such that the matrix \\ud835\\udef7(\\ud835\\udee9 \\ud835\\udc56 ) is SPD if \\ud835\\udee9 \\ud835\\udc56 is SPD. For an exponential kernel, Table 3 gives the parameterizations of \\ud835\\udef7 and \\ud835\\udf05 that correspond to GD, CR, HH, and EHH kernels. The complexity of these different kernels depends on the number of hyperparameters that characterizes them. As defined by Saves et al. [34], for every categorical variable \\ud835\\udc56 \\u2208 {1, \\u2026 , \\ud835\\udc59}, the matrix \\ud835\\udc36(\\ud835\\udee9 \\ud835\\udc56 ) \\u2208 R \\ud835\\udc3f \\ud835\\udc56 \\u00d7\\ud835\\udc3f \\ud835\\udc56 is lower triangular and built using a hypersphere decomposition [55,56] from the symmetric matrix \\ud835\\udee9 \\ud835\\udc56 \\u2208 R \\ud835\\udc3f \\ud835\\udc56 \\u00d7\\ud835\\udc3f \\ud835\\udc56 of hyperparameters. The variable \\ud835\\udf16 is a small positive constant and the variable \\ud835\\udf03 \\ud835\\udc56 denotes the only positive hyperparameter that is used for the Gower distance kernel.\", \"Another Kriging based model that can use mixed variables is Kriging with partial least squares (KPLS) [57]. KPLS adapts Kriging to high dimensional problems by using a reduced number of hyperparameters thanks to a projection into a smaller space. Also, for a general surrogate, not necessarily Kriging, SMT 2.0 uses continuous relaxation to allow whatever model to handle mixed variables. For example, we can use mixed variables with least squares (LS) or quadratic polynomial (QP) models. We now illustrate the abilities of the toolbox in terms of mixed modeling over an engineering test case.\"]}, {\"header\": \"An engineering design test-case\", \"paragraph\": [\"A classic engineering problem commonly used for model validation is the beam bending problem [32,58]. This problem is illustrated on Fig. 2(a) and consists of a cantilever beam in its linear range loaded at its free end with a force \\ud835\\udc39 . As in Cheng et al. [58], the Young modulus is \\ud835\\udc38 = 200 GPa and the chosen load is \\ud835\\udc39 = 50 kN. Also, as in Roustant et al. [32], 12 possible cross-sections can be used. These 12 sections consist of 4 possible shapes that can be either hollow, thick or full as illustrated in Fig. 2\"]}, {\"header\": \"(b).\", \"paragraph\": [\"To compare the mixed Kriging models of SMT 2.0, we draw a 98 point LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7 30 = 10800 points. For the four implemented methods, displacement error (computed with a root-mean-square error criterion), likelihood, number of hyperparameters and computational time for every model are shown in Table 4. For the continuous variables, we use the square exponential kernel. More details are found in [34]. As expected, the complex EHH and HH models lead to a lower displacement error and a higher likelihood value, but use more hyperparameters and increase the computational cost compared to GD and CR. On this test case, the kernel EHH is easier to optimize than HH but in general, they are similar in terms of performance. Also, by default SMT 2.0 uses CR as it is known to be a good trade-off between complexity and performance [59].\"]}, {\"header\": \"Surrogate models with hierarchical variables in SMT 2.0\", \"paragraph\": [\"To introduce the newly developed Kriging model for hierarchical variables implemented in SMT 2.0, we present the general mathematical framework for hierarchical and mixed variables established by Audet et al. [38]. In SMT 2.0, two variants of our new method are implemented, namely SMT Alg-Kernel and SMT Arc-Kernel. In particular, the SMT Alg-Kernel is a novel correlation kernel introduced in this paper.\"]}, {\"header\": \"The hierarchical variables framework\", \"paragraph\": [\"A problem structure is classified as hierarchical when the sets of active variables depend on architectural choices. This occurs frequently in industrial design problems. In hierarchical problems, we can classify variables as neutral, meta (also known as dimensional) or decreed (also known as conditionally active) as detailed in Audet et al. [38]. Neutral variables are the variables that are not affected by the hierarchy whereas the value assigned to meta variables determines which decreed variables are activated. For example, a meta variable could be the number of engines. If the number of engines changes, the number of decreed bypass ratios that every engine should specify also changes.  However, the wing aspect ratio being neutral, it is not affected by this hierarchy.\", \"Problems involving hierarchical variables are generally dependant on discrete architectures and as such involve mixed variables. Hence, in addition to their role (neutral, meta or decreed), each variable also has a variable type amongst categorical, ordinal or continuous. For the sake of simplicity and because both continuous and ordinal variables are treated similarly [34], we chose to regroup them as quantitative variables. For instance, the neutral variables \\ud835\\udc65 neu may be partitioned into different variable types, such that \\ud835\\udc65 neu = (\\ud835\\udc65 cat neu , \\ud835\\udc65 The variable classification scheme in SMT 2.0 is detailed in Fig. 3.\", \"To explain the framework and the new Kriging model, we illustrate the inputs variables of the model using a classical machine learning problem related to the hyperparameters optimization of a fullyconnected Multi-Layer Perceptron (MLP) [38] on Fig. 4. In Table 5, we detail the input variables of the model related to the MLP problem (i.e., the hyperparameters of the neural network, together with their types and roles). To keep things clear and concise, the chosen problem is a simplification of the original problem developed by Audet et al. [38]. Regarding the MLP problem of Fig. 4 and following the classification scheme of Fig. 3, we start by separating the input variables according to their role. In fact, 1. changing the number of hidden layers modifies the number of inputs variables. Therefore, \'\'# of hidden layers\'\' is a meta variable.\", \"2. The number of neurons in the hidden layer number \\ud835\\udc58 is either included or excluded. For example, the \'\'# of neurons in the 3 rd layer\'\' would be excluded for an input that only has 2 hidden layers. Therefore, \'\'# of neurons hidden layer \\ud835\\udc58\'\' are decreed variables. 3. The \'\'Learning rate\'\', \'\'Momentum\'\', \'\'Activation function\'\' and \'\'Batch size\'\' are not affected by the hierarchy choice. Therefore, they are neutral variables.\", \"According to their types, the MLP input variables can be classified as follows:\", \"4. The meta variable \'\'# of hidden layers\'\' is an integer and, as such, is represented by the component \\ud835\\udc65 To model hierarchical variables, as proposed in [38], we separate the input space \\ue244 as (\\ue244 neu , \\ue244 met , \\ue244 dec ) where \\ue244 dec = \\u22c3 \\ud835\\udc65 met \\u2208\\ue244 met \\ue244 inc (\\ud835\\udc65 met ).\", \"P. Saves et al.\"]}, {\"header\": \"A Kriging model for hierarchical variables\", \"paragraph\": [\"In this section, a new method to build a Kriging model with hierarchical variables is introduced based on the framework aforementioned.\", \"The proposed methods are included in SMT 2.0.\"]}, {\"header\": \"Motivation and state-of-the-art\", \"paragraph\": [\"Assuming that the decreed variables are quantitative, Hutter and Osborne [21] proposed several kernels for the hierarchical context. A classic approach, called the imputation method (Imp-Kernel) leads to an efficient paradigm in practice that can be easily integrated into a more general framework as proposed by Bussemaker et al. [24]. However, this kernel lacks depth and depends on arbitrary choices. Therefore, Hutter and Osborne [21] also proposed a more general kernel, called Arc-Kernel and Horn et al. [36] generalized this kernel even more and proposed a new formulation called the Wedge-Kernel [37]. The drawbacks of these two methods are that they add some extra hyperparameters for every decreed dimension (respectively one extra hyperparameter for the Arc-Kernel and two hyperparameters for the Wedge-Kernel) and that they need a normalization according to the bounds. More recently, Pelamatti et al. [60] developed a hierarchical kernel for Bayesian optimization. However, our work is also more general thanks to the framework introduced earlier [38] that considers variable-wise formulation and is more flexible as we are allowing sub-problems to be intersecting.\", \"In the following, we describe our new method to build a correlation kernel for hierarchical variables. In particular, we introduce a new algebraic kernel called Alg-Kernel that behaves like the Arc-Kernel whilst correcting most of its drawbacks. In particular, our kernel does not add any hyperparameters, and the normalization is handled in a natural way.\"]}, {\"header\": \"A new hierarchical correlation kernel\", \"paragraph\": [\"For modeling purposes, we assume that the decreed space is quantitative, i.e., \\ue244 dec = \\ue244 qnt dec . Let \\ud835\\udc62 \\u2208 \\ue244 be an input point partitioned as \\ud835\\udc62 = (\\ud835\\udc62 neu , \\ud835\\udc62 met , \\ud835\\udc62 inc (\\ud835\\udc62 met )) and, similarly, \\ud835\\udc63 \\u2208 \\ue244 is another input such that \\ud835\\udc63 = (\\ud835\\udc63 neu , \\ud835\\udc63 met , \\ud835\\udc63 inc (\\ud835\\udc63 met )). The new kernel \\ud835\\udc58 that we propose for hierarchical variables is given by\", \"where \\ud835\\udc58 neu , \\ud835\\udc58 met and \\ud835\\udc58 met,dec are as follows:\", \"\\u2022 \\ud835\\udc58 neu represents the neutral kernel that encompasses both categorical and quantitative neutral variables, i.e., \\ud835\\udc58 neu can be decomposed into two parts \\ud835\\udc58 neu (\\ud835\\udc62 neu , \\ud835\\udc63 neu ) = \\ud835\\udc58 cat (\\ud835\\udc62 cat neu , \\ud835\\udc63 cat neu )\\ud835\\udc58 qnt (\\ud835\\udc62 qnt neu , \\ud835\\udc63 qnt neu ). The categorical kernel, denoted \\ud835\\udc58 cat , could be any Symmetric Positive Definite (SPD) [34] mixed kernel (see Section 3). For the quantitative (integer or continuous) variables, a distancebased kernel is used. The chosen quantitative kernel (Exponential, Mat\\u00e9rn, . . . ), always depends on a given distance \\ud835\\udc51. For example, the \\ud835\\udc5b-dimensional exponential kernel gives\", \"\\u2022 \\ud835\\udc58 met is the meta variables related kernel. It is also separated into two parts:\", \"where the quantitative kernel is ordered and not continuous because meta variables take value in a finite set.\", \"\\u2022 \\ud835\\udc58 met,dec is an SPD kernel that models the correlations between the meta levels (all the possible subspaces) and the decreed variables.\", \"In what comes next, we detailed this kernel.\"]}, {\"header\": \"Towards an algebraic meta-decreed kernel\", \"paragraph\": [\"Meta-decreed kernels like the imputation kernel or the Arc-Kernel were first proposed in [21,47] and the distance-based kernels such as Arc-Kernel or Wedge-Kernel [37] were proven to be SPD. Nevertheless, to guarantee this SPD property, the same hyperparameters are used to model the correlations between the meta levels and between the decreed variables [47]. For this reason, the Arc-Kernel includes additional continuous hyperparameters which makes the training of the GP models more expensive and introduces more numerical stability issues. In this context, we are proposing a new algebraic meta-decreed kernel (denoted as Alg-Kernel) that enjoys similar properties as Arc-Kernel but without using additional continuous hyperparameters nor rescaling. In the SMT 2.0 release, we included Alg-Kernel and a simpler version of Arc-Kernel that do not relies on additional hyperparameters.\", \"Our proposed Alg-Kernel kernel is given by\", \"Mathematically, we could consider that there is only one meta variable whose levels correspond to every possible included subspace. Let \\ud835\\udc3c sub denotes the components indices of possible subspaces, the subspaces parameterized by the meta component \\ud835\\udc62 met are defined as \\ue244 inc (\\ud835\\udc62 met = \\ud835\\udc59), \\ud835\\udc59 \\u2208 \\ud835\\udc3c sub . It follows that the fully extended continuous decreed space writes as \\ue244 dec = \\u22c3 \\ud835\\udc59\\u2208\\ud835\\udc3c sub \\ue244 inc (\\ud835\\udc62 met = \\ud835\\udc59) and \\ud835\\udc3c dec is the set of the associated indices. Let \\ud835\\udc3c \\ud835\\udc56\\ud835\\udc5b\\ud835\\udc61\\ud835\\udc52\\ud835\\udc5f \\ud835\\udc62,\\ud835\\udc63 denotes the set of components related to the space \\ue244 inc (\\ud835\\udc62 met , \\ud835\\udc63 met ) containing the variables decreed-included in both \\ue244 inc (\\ud835\\udc62 met ) and \\ue244 inc (\\ud835\\udc63 met ).\", \"Since the decreed variables are quantitative, one has\", \"The construction of the quantitative kernel \\ud835\\udc58 qnt depends on a given distance denoted \\ud835\\udc51 alg . The kernel \\ud835\\udc58 alg met is an induced meta kernel that depends on the same distance \\ud835\\udc51 alg to preserve the SPD property of \\ud835\\udc58 alg met,dec . For every \\ud835\\udc56 \\u2208 \\ud835\\udc3c dec , if \\ud835\\udc56 \\u2208 \\ud835\\udc3c \\ud835\\udc56\\ud835\\udc5b\\ud835\\udc61\\ud835\\udc52\\ud835\\udc5f \\ud835\\udc62,\\ud835\\udc63 , the new algebraic distance is given by\", \"where \\ud835\\udf03 \\ud835\\udc56 \\u2208 R + is a continuous hyperparameter. Otherwise, if \\ud835\\udc56 \\u2208 \\ud835\\udc3c dec but \\ud835\\udc56 \\u2209 \\ud835\\udc3c \\ud835\\udc56\\ud835\\udc5b\\ud835\\udc61\\ud835\\udc52\\ud835\\udc5f \\ud835\\udc62,\\ud835\\udc63 , there should be a non-zero residual distance between the two different subspaces \\ue244 inc (\\ud835\\udc62 met ) and \\ue244 inc (\\ud835\\udc63 met ) to ensure the kernel SPD property. To have a residual not depending on the decreed values, our model considers that there is a unit distance\", \"The induced meta kernel \\ud835\\udc58 alg met (\\ud835\\udc62 met , \\ud835\\udc63 met ) to preserve the SPD property of \\ud835\\udc58 alg is defined as:\", \"Not only our kernel of Eq. ( 2) uses less hyperparameters than the Arc-Kernel (as we cut off its extra parameters) but it is also a more flexible kernel as it allows different kernels for meta and decreed variables. Moreover, another advantage of our kernel is that it is numerically more stable thanks to the new non-stationary [61] algebraic distance defined in Eq. ( 7) [62]. Our proposed distance also does not need any rescaling by the bounds to have values between 0 and 1.\", \"In what comes next, we will refer to the implementation of the kernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and SMT Alg-Kernel. We note also that the implementation of SMT Arc-Kernel differs slightly from the original Arc-Kernel as we fixed some hyperparameters to 1 in order to avoid adding extra hyperparameters and use the formulation of Eq. ( 2) and rescaling of the data.\"]}, {\"header\": \"Illustration on the MLP problem\", \"paragraph\": [\"In this section, we illustrate the hierarchical Arc-Kernel on the MLP example. For that sake, we consider two design variables \\ud835\\udc62 and \\ud835\\udc63 such that \\ud835\\udc62 = (2.10 -4 , 0.9, ReLU, 16, 2, 55, 51) and \\ud835\\udc63 = (5.10 -3 , 0.8, Sigmoid, 64,3,50,54,53). Since the value of \\ud835\\udc62 met (i.e., the number of hidden layers) differs from one point to another (namely, 2 for \\ud835\\udc62 and 3 for \\ud835\\udc63), the associated variables \\ud835\\udc62 inc (\\ud835\\udc62 met ) have either 2 or 3 variables for the number of neurons in each layer (namely 55 and 51 for \\ud835\\udc62, and 50, 54 and 53 for the point \\ud835\\udc63). In our case, 8 hyperparameters ([\\ud835\\udc45 1 ] 1,2 , \\ud835\\udf03 1 , \\u2026 , \\ud835\\udf03 7 ) will have to be optimized where \\ud835\\udc58 is given by Eq. ( 2). These 7 hyperparameters can be described using our proposed framework as follows:\", \"\\u2022 For the neutral components, we have \\ud835\\udc62 neu = (2.10 -4 , 0.9, ReLU, 16) and \\ud835\\udc63 neu = (5.10 -3 , 0.8, Sigmoid, 64). Therefore, for a categorical matrix kernel \\ud835\\udc45 1 and a square exponential quantitative kernel,\", \"The values [\\ud835\\udc45 1 ] 1,2 , \\ud835\\udf03 1 , \\ud835\\udf03 2 and \\ud835\\udf03 3 need to be optimized. Here, [\\ud835\\udc45 1 ] 1,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\", \"\\u2022 For the meta components, we have \\ud835\\udc62 met = 2 and \\ud835\\udc63 met = 3.\", \"Therefore, for a square exponential quantitative kernel,\", \"The value \\ud835\\udf03 4 needs to be optimized.\", \"\\u2022 For the meta-decreed kernel, we have [\\ud835\\udc62 met , \\ud835\\udc62 inc (\\ud835\\udc62 met )] = [2, (55, 51)] and [\\ud835\\udc63 met , \\ud835\\udc63 inc (\\ud835\\udc63 met )] = [3, (50, 54, 53)] which gives ((55, 51), (50,54,53)).\", \"The distance \\ud835\\udc51 alg (51, 54) =\", \") \\ud835\\udf03 6 = 2.178.10 -3 \\ud835\\udf03 6 . In general, for surrogate models, and in particular in SMT 2.0, the input data are normalized. With a unit normalization from [50,55] to [0, 1], we would have \\ud835\\udc51 alg (0.2, 0.8) =\", \") \\ud835\\udf03 6 = 0.919 \\ud835\\udf03 6 . Similarly, we have, between 55 and 50, \\ud835\\udc51 alg (0, 1) = 1.414 \\ud835\\udf03 5 . Hence, for a square exponential quantitative kernel, one gets\", \"where the meta induced component is\", \"because the decreed value 53 in \\ud835\\udc63 has nothing to be compared with in \\ud835\\udc62 as in Eq. ( 7). The values \\ud835\\udf03 5 , \\ud835\\udf03 6 and \\ud835\\udf03 7 need to be optimized which complete the description of the hyperparameters.\", \"We note that for the MLP problem, Alg-Kernel models use 10 hyperparameters whereas the Arc-Kernel would require 12 hyperparameters without the meta kernel (\\ud835\\udf03 4 ) but with 3 extra decreed hyperparameters and the Wedge-Kernel would require 15 hyperparameters. For deep learning applications, a more complex perceptron with up to 10 hidden layers would require 17 hyperparameters with SMT 2.0 models against 26 for Arc-Kernel and 36 for Wedge-Kernel. The next section illustrates the interest of our method to build a surrogate model for this neural network engineering problem.\"]}, {\"header\": \"A neural network test-case using SMT 2.0\", \"paragraph\": [\"In this section, we apply our models to the hyperparameters optimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0 an example illustrates this MLP problem. For the sake of showing the Kriging surrogate abilities, we implemented a dummy function with no significance to replace the real black-box that would require training a whole Neural Network (NN) with big data. This function requires a number of variables that depends on the value of the meta variable, i.e the number of hidden layers. To simplify, we have chosen only 1, 2 or 3 hidden layers and therefore, we have 3 decreed variables but deep neural networks could also be investigated as our model can tackle a few dozen variables. A test case (test_hierarchical_variables_NN ) shows that our model SMT Alg-Kernel interpolates the data properly, checks that the data dimension is correct and also asserts that the inactive decreed variables have no influence over the prediction. In Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical and mixed variables based on the implementation of SMT 2.0 for test_hierarchical_variables_NN.\", \"To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel and SMT Arc-Kernel) with the state-of-the-art imputation method previously used on industrial application (Imp-Kernel) [24], we draw a 99 point LHS (33 points by meta level) as a training set and the validation set is a LHS of 3 \\u00d7 1000 = 3000 points. For the Imp-Kernel, the decreed-excluded values are replaced by 52 because the mean value 52.5 is not an integer (by default, SMT rounds to the floor value).\", \"For the three methods, the precision (computed with a root-meansquare error RMSE criterion), the likelihood and the computational time are shown in Table 6. For this problem, we can see that SMT Algkernel gives better performance than the imputation method or SMT Arc-kernel. Also, as all methods use the same number of hyperparameters, they have similar time performances. A direct application of our modeling method is Bayesian optimization to perform quickly the hyperparameter optimization of a neural network [63].\"]}, {\"header\": \"Bayesian optimization within SMT 2.0\", \"paragraph\": [\"Efficient global optimization (EGO) is a sequential Bayesian optimization algorithm designed to find the optimum of a black-box function that may be expensive to evaluate [52]. EGO starts by fitting a Kriging model to an initial DoE, and then uses an acquisition function to select the next point to evaluate. The most used acquisition function is the expected improvement. Once a new point has been evaluated, the Kriging model is updated. Successive updates increase the model accuracy over iterations. This enrichment process repeats until a stopping criterion is met.\", \"Because SMT 2.0 implements Kriging models that handle mixed and hierarchical variables, we can use EGO to solve problems involving such design variables. Other Bayesian optimization algorithms often adopt approaches based on solving subproblems with continuous or non-hierarchical Kriging. This subproblem approach is less efficient and scales poorly, but it can only solve simple problems. Several Bayesian optimization software packages can handle mixed or hierarchical variables with such a subproblem approach. The packages include BoTorch [25], SMAC [65], Trieste [66], HEBO [67], OpenBox [68], and Dragonfly [69].\"]}, {\"header\": \"A mixed optimization problem\", \"paragraph\": [\"Fig. 6 compares the four EGO methods implemented in SMT 2.0: SMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that illustrates Bayesian optimization is a toy test case [64] detailed in Appendix A. This test case has two variables, one continuous and one categorical with 10 levels. To assess the performance of our algorithm, we performed 20 runs with different initial DoE sampled by LHS. Every DoE consists of 5 points and we chose a budget of 55 infill points. Fig. 6(a) plots the convergence curves for the four methods. In particular, the median is the solid line, and the first and third quantiles are plotted in dotted lines. To visualize better the data dispersion, the boxplots of the 20 best solutions after 20 evaluations are plotted in Fig. 6(b). As expected, the more a method is complex, the better the optimization. Both SMT HH and SMT EHH converged in around 18 evaluations whereas SMT CR and SMT GD take around 26 iterations as shown on Fig. 6(a). Also, the more complex the model, the closer the optimum is to the real value as shown on Fig. 6(b).\", \"In Fig. 7 we illustrate how to use EGO with mixed variables based on the implementation of SMT 2.0. The illustrated problem is a mixed variant of the Branin function [70]. Note that a dedicated notebook is available to reproduce the results presented in this paper and the mixed integer notebook also includes an extra mechanical application with composite materials [41].\"]}, {\"header\": \"A hierarchical optimization problem\", \"paragraph\": [\"The hierarchical test case considered in this paper to illustrate Bayesian optimization is a modified Goldstein function [60] detailed in Appendix B. The resulting optimization problem involves 11 variables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical. These variables consist in 6 neutral variables, 1 dimensional (or meta) variable and 4 decreed variables. Depending on the meta variable values, 4 different sub-problems can be identified. The optimization problem is given by:\", \"Compared to the model choice of Pelamatti et al. [60], we chose to model \\ud835\\udc65 5 and \\ud835\\udc64 2 as neutral variables even if \\ud835\\udc53 does not depend on \\ud835\\udc65 5 when \\ud835\\udc64 2 = 0. Other modeling choices are kept; for example, \\ud835\\udc64 2 is a so-called \'\'binary variable\'\' and not a categorical one [71]. Similarly, we also keep the formulation of \\ud835\\udc64 1 as a categorical variable but a better model would be to model it as a \'\'cyclic variable\'\' [72]. The resulting problem is described in Appendix B. To assess the performance of our algorithm, we performed 20 runs with different initial DoE sampled by LHS. Every DoE consists of \\ud835\\udc5b + 1 = 12 points and we chose a budget of 5\\ud835\\udc5b = 55 infill points. To compare our method with a baseline, we also tested the random search method thanks to the expand_lhs new method [40] described in Section 6.1 and we also optimized the Goldstein function EGO with a classic Kriging based on imputation method (Imp-Kernel). This method replaces the decreedexcluded variables by their mean values: 50 or 1 respectively for (\\ud835\\udc65 3 , \\ud835\\udc65 4 ) and (\\ud835\\udc67 1 , \\ud835\\udc67 2 ). Fig. 8(a) plots the convergence curves for the four methods. In particular, the median is the solid line and the first and third quantiles are plotted in dotted lines. To visualize better the corresponding data dispersion, the boxplots of the 20 best solutions are plotted in Fig. 8(b). The results in Fig. 8 show that the hierarchical Kriging models of SMT 2.0 lead to better results than the imputation method or the random search both in terms of final objective value and variance over the 20 runs and in term of convergence rate. More precisely, SMT Arc-Kernel and SMT Alg-Kernel Kriging model gave the best EGO results and managed to converge correctly as shown in Fig. 8(b). More precisely, the 20 sampled DoEs led to similar performance and from one DoE, the method SMT Alg-Kernel managed to find the true minimum. However, this result has not been reproduced in other runs and is therefore not statistically significant. The variance between the runs is of similar magnitude regardless of the considered methods.\"]}, {\"header\": \"Other relevant contributions in SMT 2.0\", \"paragraph\": [\"The new release SMT 2.0 introduces several improvements besides Kriging for hierarchical and mixed variables. This section details the most important new contributions. Recall from Section 2.2 that five sub-modules are present in the code: Sampling, Problems, Surrogate Models, Applications and Notebooks.\"]}, {\"header\": \"Contributions to Sampling\", \"paragraph\": [\"Pseudo-random sampling. The Latin Hypercube Sampling (LHS) is a stochastic sampling technique to generate quasi-random sampling distributions. It is among the most popular sampling method in computer experiments thanks to its simplicity and projection properties with high-dimensional problems. The LHS method uses the pyDOE package (Design Of Experiments for Python). Five criteria for the construction of LHS are implemented in SMT. The first four criteria (center, maximin, centermaximin, correlation) are the same as in pyDOE. 12 The last criterion ese, is implemented by the authors of SMT [48]. In SMT 2.0 a new LHS method was developed for the Nested design of experiments (NestedLHS) [73] to use with multifidelity surrogates. A new mathematical method (expand_lhs) [40] was developed in SMT 2.0 to increase the size of a design of experiments while maintaining the ese property. Moreover, we proposed a sampling method for mixed variables, and the aforementioned LHS method was applied to hierarchical variables in Fig. 8.\"]}, {\"header\": \"Contributions to Surrogate models\", \"paragraph\": [\"New kernels and their derivatives for Kriging. Kriging surrogates are based on hyperparameters and on a correlation kernel. Four correlation kernels are now implemented in SMT 2.0 [74]. In SMT, these correlation functions are absolute exponential (abs_exp), Gaussian (squar_exp), Matern 5/2 (matern52) and Matern 3/2 (matern32). In addition, the implementation of gradient and Hessian for each kernel makes it possible to calculate both the first and second derivatives of the GP likelihood with respect to the hyperparameters [5].\", \"Variance derivatives for Kriging. To perform uncertainty quantification for system analysis purposes, it could be interesting to know more about the variance derivatives of a model [75][76][77]. For that purpose and also to pursue the original publication about derivatives [5], SMT 2.0 extends the derivative support to Kriging variances and kernels. Noisy Kriging. In engineering and in big data contexts with real experiments, surrogate models for noisy data are of significant interest. In particular, there is a growing need for techniques like noisy Kriging and noisy Multi-Fidelity Kriging (MFK) for data fusion [78]. For that purpose, SMT 2.0 has been designed to accommodate Kriging and MFK to noisy data including the option to incorporate heteroscedastic noise (using the use_het_noise option) and to account for different noise levels for each data source [40].\", \"Kriging with partial least squares. Beside MGP, for high-dimensional problems, the toolbox implements Kriging with partial least squares (KPLS) [57] and its extension KPLSK [44]. The PLS information is computed by projecting the data into a smaller space spanned by the principal components. By integrating this PLS information into the Kriging correlation matrix, the number of inputs can be scaled down, thereby reducing the number of hyperparameters required. The resulting number of hyperparameters \\ud835\\udc51 \\ud835\\udc52 is indeed much smaller than the original problem dimension \\ud835\\udc51. Recently, in SMT 2.0, we extended the KPLS method for multi-fidelity Kriging (MFKPLS and MFKPLSK) [73,79,80]. We also proposed an automatic criterion to choose automatically the reduced dimension \\ud835\\udc51 \\ud835\\udc52 based on Wold\'s R criterion [81]. This criterion has been applied to aircraft optimization with EGO where the number \\ud835\\udc51 \\ud835\\udc52 (\\ud835\\ude97_\\ud835\\ude8c\\ud835\\ude98\\ud835\\ude96\\ud835\\ude99 in the code) for the model is automatically selected at every iteration [39]. Special efforts have been made to accommodate KPLS for multi-fidelity and mixed integer data [79,80].\", \"Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian Process (MGP) surrogate models for high dimensional problems [82]. MGP are Gaussian processes taking into account hyperparameters uncertainty defined as a density probability function. Especially we suppose that the function to model \\ud835\\udc53 \\u2236 \\ud835\\udefa \\u21a6 R, where \\ud835\\udefa \\u2282 R \\ud835\\udc51 and \\ud835\\udc51 is the number of design variables, lies in a linear embedding \\ue22d such as \\ue22d = {\\ud835\\udc62 = \\ud835\\udc34\\ud835\\udc65, \\ud835\\udc65 \\u2208 \\ud835\\udefa}, \\ud835\\udc34 \\u2208 R \\ud835\\udc51\\u00d7\\ud835\\udc51 \\ud835\\udc52 and \\ud835\\udc53 (\\ud835\\udc65) = \\ud835\\udc53 \\ue22d (\\ud835\\udc34\\ud835\\udc65) with \\ud835\\udc53 (\\ud835\\udc65) = \\ud835\\udc53 \\ue22d \\u2236 \\ue22d \\u21a6 R and \\ud835\\udc51 \\ud835\\udc52 \\u226a \\ud835\\udc51. Then, we must use a kernel \\ud835\\udc58(\\ud835\\udc65, \\ud835\\udc65 \\u2032 ) = \\ud835\\udc58 \\ue22d (\\ud835\\udc34\\ud835\\udc65, \\ud835\\udc34\\ud835\\udc65 \\u2032 ) whose each component of the transfer matrix \\ud835\\udc34 is an hyperparameter. Thus we have \\ud835\\udc51 \\ud835\\udc52 \\u00d7 \\ud835\\udc51 hyperparameters to find. Note that \\ud835\\udc51 \\ud835\\udc52 is defined as \\ud835\\ude97_\\ud835\\ude8c\\ud835\\ude98\\ud835\\ude96\\ud835\\ude99 in the code [49].\", \"Gradient-enhanced neural network. The new release SMT 2.0 implements Gradient-Enhanced Neural Network (GENN) models [45]. Gradient-Enhanced Neural Networks (GENN) are fully connected multilayer perceptrons whose training process was modified to account for gradient information. Specifically, the model is trained to minimize not only the prediction error of the response but also the prediction error of the partial derivatives: the chief benefit of gradient enhancement is better accuracy with fewer training points. Note that GENN applies to regression (single-output or multi-output), but not classification since there is no gradient in that case. The implementation is fully vectorized and uses ADAM optimization, mini-batch, and L2-norm regularization. For example, GENN can be used to learn airfoil geometries from a database. This usage is documented in SMT 2.0.13\"]}, {\"header\": \"Contributions to Applications\", \"paragraph\": [\"Kriging trajectory and sampling. Sampling a GP with high resolution is usually expensive due to the large dimension of the associated covariance matrix. Several methods are proposed to draw samples of a GP on a given set of points. To sample a conditioned GP, the classic method consists in using a Cholesky decomposition (or eigendecomposition) of the conditioned covariance matrix of the process but some numerical computational errors can lead to non SPD matrix. A more recent approach based on Karhunen-Lo\\u00e8ve decomposition of the covariance kernel with the Nystr\\u00f6m method has been proposed in [83] where the paths can be sampled by generating independent standard Normal distributed samples. The different methods are documented in the tutorial Gaussian Process Trajectory Sampling [84].\", \"Parallel Bayesian optimization. Due to the recent progress made in hardware configurations, it has been of high interest to perform parallel optimizations. A parallel criterion called qEI [85] was developed to perform Efficient Global Optimization (EGO): the goal is to be able to run batch optimization. At each iteration of the algorithm, multiple new sampling points are extracted from the known ones. These new sampling points are then evaluated using a parallel computing environment. Five criteria are implemented in SMT 2.0: Kriging Believer (KB), Kriging Believer Upper Bound (KBUB), Kriging Believer Lower Bound (KBLB), Kriging Believer Random Bound (KBRand) and Constant Liar (CLmin) [86].\"]}, {\"header\": \"Conclusion\", \"paragraph\": [\"SMT 2.0 introduces significant upgrades to the Surrogate Modeling Toolbox. This new release adds support for hierarchical and mixed variables and improves the surrogate models with a particular focus on Kriging (Gaussian process) models. SMT 2.0 is distributed through an open-source license and is freely available online. 14 We provide documentation that caters to both users and potential developers. 15   SMT 2.0 enables users and developers collaborating on the same project to have a common surrogate modeling tool that facilitates the exchange of methods and reproducibility of results.\", \"SMT has been widely used in aerospace and mechanical modeling applications. Moreover, the toolbox is general and can be useful for anyone who needs to use or develop surrogate modeling techniques, regardless of the targeted applications. SMT is currently the only opensource toolbox that can build hierarchical and mixed surrogate models.\"]}, {\"header\": \"Declaration of competing interest\", \"paragraph\": [\"The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\"]}, {\"header\": \"Acknowledgments\", \"paragraph\": [\"We want to thank all those who contribute to this release. Namely, M. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde Arenzana, S. Dubreuil, A. F. L\\u00f3pez-Lopera, M. Meliani, M. Menz, N. Mo\\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is part of the activities of ONERA -ISAE -ENAC joint research group. We also acknowledge the partners institutions: ONERA, NASA Glenn, ISAE-SUPAERO, Institut Cl\\u00e9ment Ader (ICA), the University of Michigan, Polytechnique Montr\\u00e9al and the University of California San Diego.\", \"Business Models) funded by the European Union Horizon Europe research and innovation framework programme under grant agreement n \\u2022 101097120.\", \"We also are grateful to E. Hall\\u00e9-Hannan from Polytechnique Montr\\u00e9al for the hierarchical variables framework.\"]}, {\"header\": \"Acknowledgments\", \"paragraph\": [\"We want to thank all those who contribute to this release. Namely, M. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde Arenzana, S. Dubreuil, A. F. L\\u00f3pez-Lopera, M. Meliani, M. Menz, N. Mo\\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is part of the activities of ONERA -ISAE -ENAC joint research group. We also acknowledge the partners institutions: ONERA, NASA Glenn, ISAE-SUPAERO, Institut Cl\\u00e9ment Ader (ICA), the University of Michigan, Polytechnique Montr\\u00e9al and the University of California San Diego.\"]}, {\"header\": \"P. Saves et al.\", \"paragraph\": [\"Business Models) funded by the European Union Horizon Europe research and innovation framework programme under grant agreement n \\u2022 101097120.\", \"We also are grateful to E. Hall\\u00e9-Hannan from Polytechnique Montr\\u00e9al for the hierarchical variables framework.\"]}, {\"header\": \"Data availability\", \"paragraph\": [\"Data will be made available on request. Results can be reproduced freely online at https://colab.research.google.com/github/SMTorg/smt/ blob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\"]}, {\"header\": \"Data availability\", \"paragraph\": [\"Data will be made available on request. Results can be reproduced freely online at https://colab.research.google.com/github/SMTorg/smt/ blob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\"]}, {\"header\": \"Appendix A. Toy test function\", \"paragraph\": [\"This Appendix gives the detail of the toy function of Section 5.1. 16  First, we recall the optimization problem: min \\ud835\\udc53 (\\ud835\\udc65 cat , \\ud835\\udc65 qnt ) w.r.t. \\ud835\\udc65 cat = \\ud835\\udc50 1 \\u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\", \"The toy function \\ud835\\udc53 is defined as \\ud835\\udc53 (\\ud835\\udc65, \\ud835\\udc50 1 ) =1 \\ud835\\udc50 1 =0 cos(3.6\\ud835\\udf0b(\\ud835\\udc65 -2)) + \\ud835\\udc65 -1\", \"The hierarchical and mixed function \\ud835\\udc53 is defined as a hierarchical function that depends on \\ud835\\udc53 0 , \\ud835\\udc53 1 , \\ud835\\udc53 2 and \\ud835\\udc3a\\ud835\\udc5c\\ud835\\udc59\\ud835\\udc51 cont as describes in the following.\", \"To finish with, the function \\ud835\\udc3a\\ud835\\udc5c\\ud835\\udc59\\ud835\\udc51 cont is given by\"]}, {\"header\": \"Appendix A. Toy test function\", \"paragraph\": [\"This Appendix gives the detail of the toy function of Section 5.1. 16  First, we recall the optimization problem: min \\ud835\\udc53 (\\ud835\\udc65 cat , \\ud835\\udc65 qnt ) w.r.t. \\ud835\\udc65 cat = \\ud835\\udc50 1 \\u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\", \"The toy function \\ud835\\udc53 is defined as \\ud835\\udc53 (\\ud835\\udc65, \\ud835\\udc50 1 ) =1 \\ud835\\udc50 1 =0 cos(3.6\\ud835\\udf0b(\\ud835\\udc65 -2)) + \\ud835\\udc65 -1\", \"The hierarchical and mixed function \\ud835\\udc53 is defined as a hierarchical function that depends on \\ud835\\udc53 0 , \\ud835\\udc53 1 , \\ud835\\udc53 2 and \\ud835\\udc3a\\ud835\\udc5c\\ud835\\udc59\\ud835\\udc51 cont as describes in the following.\", \"To finish with, the function \\ud835\\udc3a\\ud835\\udc5c\\ud835\\udc59\\ud835\\udc51 cont is given by\"]}]','https://drive.google.com/uc?id=19UI6ZZUZ6iNGoxqnznhbnw4cWuBnESiw&export=download','2023-02-03',0,'2024-02-03 22:46:18.914060','2024-02-03 22:46:26.404323'),(19,'Numerical computing in engineering mathematics','The rapid advances in technology over the last decade have significantly altered the nature of engineering knowledge and skills required in the modern industries. In response to the changing professional requirements, engineering institutions have updated their curriculum and pedagogical practices. However, most of the changes in the curriculum have been focused on the core engineering courses without much consideration for the auxiliary courses in mathematics and sciences. In this paper, we aim to propose a new, augmented mathematics curriculum aimed at meeting the requirements of the modern, technology-based engineering workplace. The proposed updates require minimal resources and can be seamlessly integrated into the existing curriculum.','[{\"header\": \"I. INTRODUCTION\", \"paragraph\": [\"The 4th Industrial Revolution has had a dramatic impact on the engineering profession. The modern technologies such as artificial intelligence, the internet of things, and advanced robotics have altered engineering systems and processes. Today\'s engineers are expected to be able to leverage these resources to produce their products. To meet the new professional requirements, engineering educational institutions have revised their curricula. The changes in the curricula include both updating the existing programs as well as introducing completely new programs. Given the rapid technological progress, universities and colleges around the world are continuously adapting to the ever-changing environment. While a significant progress in modernizing the engineering curriculum has been achieved, there still remains room for improvement.\", \"Catalyzed by the exponential increase in computational power and interconnectedness, the modern industrial revolution has reshaped the skills and competencies required of the engineers. The changes in engineering curricula in response to Industry 4.0 have been threefold: i) modernizing the existing programs, ii) introduction of new programs, and iii) revising the pedagogical approach. Modernizing the existing programs involves introduction of new courses in the study plan related to emerging technologies. In addition, existing courses can be updated with new content. Fresh new programs in emerging \\u00a9 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works technologies are also introduced by universities and colleges. Many institutions now offer degrees in artificial intelligence and mechatronics which were not there 20 years ago. Finally, universities have revised their approaches to course delivery. Student-centered learning, project-based learning, and applied learning have become popular in the new engineering educational paradigm.\", \"While significant effort has been made to revise the core engineering courses, the auxiliary courses in mathematics and sciences received little consideration. The mathematics and sciences courses play a key role in the engineering curriculum. Given their importance, the curriculum updates must also be extended to the auxiliary courses. By implementing a comprehensive update of the engineering curriculum that includes both the core and auxiliary courses, a more effective outcome can be achieved.\", \"The goal of this paper is to propose a modernized engineering mathematics curriculum in line with the broader efforts to update engineering education to adapt to Industry 4.0. The key feature of the new curriculum is the introduction numerical computing in the existing mathematics courses. The latest industrial revolution has been driven largely by the dramatic increase in computational power. Therefore, today\'s engineers must be well-equipped to leverage the computing power in their work.\", \"Since mathematics courses are usually taken at the beginning of the study plan, it offers a natural avenue for introducing numerical computing to students. Furthermore, many problems in mathematics can be solved numerically making it natural to integrate numerical computing in mathematics courses. By studying numerical computing in mathematics courses, students will acquire the necessary theoretical and practical skills to apply in their downstream, specialized engineering courses.\", \"This paper is structured as follows. Section 2 provides an overview of the existing efforts to update the engineering curriculum in response to Industry 4.0. Section 3 discusses the current approaches to integrate scientific computing in mathematics courses. In Section 4, we present our proposal for modernizing the mathematics curriculum to integrate numerical computing. Section 5 concludes the paper with final remarks.\"]}, {\"header\": \"II. ENGINEERING EDUCATION AND INDUSTRY 4.0\", \"paragraph\": [\"Engineering departments in colleges and universities have made significant changes in their curricula in response to the new environment created by the recent, rapid advances in technology. In particular, the existing programs have been updated to include courses that target emerging technologies. Completely new programs related to AI and mechatronics have also been adopted by universities. Innovations in the field of engineering education continue to take place with new developments on the horizon.\", \"There exist several studies investigating the modern engineering curricula and evaluating their effectiveness. It is argued in [6] that engineering educators must prepare their students to face three key challenges: sustainability, the 4th Industrial Revolution, and employability. The authors find that colleges and universities are responding to these challenges by emphasizing student-centered learning, integration of theory and practice, digital and online learning, and the definition of professional competencies. In particular, response to the needs of Industry 4.0 require interdisciplinary collaboration across several programs and disciplines. Interaction and integration of technologies plays a key role in this process [10], [12]. Interdisciplinary engineering education requires sound pedagogy and teaming experiences to encourage student in collaborative and interdisciplinary practice [23].\", \"Digital and online learning have become an important part of modern education including in the field of engineering. Information technologies play a vital role in delivering digital learning to students. Colleges and universities have made significant investments to improve their information and communication technology (ICT) capacities [7].\", \"In response to the needs of Industry 4.0, some universities have adopted the framework of Education 4.0 [15], [19]. The new education framework consists of four main components: i) competencies, ii) learning methods, iii) ICT, and iv) infrastructure. Students competencies are based on technological knowledge and skills for successful workplace performance, while the learning methods are based on problem solving and challenge-based learning. In particular, active and project-based learning plays an important role in Education 4.0 [4], [8]. Other innovative approaches to learning such as virtual-reality based engineering education can help improve the learning process related to Industry 4.0 [20].\", \"In addition to technological progress, socio-cultural shifts must be taken into account in revising engineering curriculum. The new generation of students has its unique worldview which needs to be considered by the educators. In particular, the new generation is significantly affected by mobile devices and digital media. Educational content must be tailored to the new student preferences to achieve effective learning outcomes [16]. Innovative approaches such gamification may help improve the learning process [13], [17].\", \"Many universities have also introduced nontechnical updates to their engineering curriculum. The most significant nontechnical update has been the introduction of entrepreneurship courses and experiences for students. A lot of attention has recently been given to equipping students with entrepreneurial skills. Students learn about entrepreneurship in their courses as well as through university incubators.\"]}, {\"header\": \"III. UNIVERSITY MATHEMATICS CURRICULUM\", \"paragraph\": [\"The mathematics curriculum changed very little in the current century. It remains a largely analytic domain, where solutions are mainly obtained manually. The current mathematics curriculum emphasizes theory over practical approaches. For instance, when finding the extreme values of a function, derivative-based approach is preferred over the gradient decent. There are two key reasons for why analytical approaches are favored over numerical methods. First, analytical solutions are reliable and elegant. An analytical solution is guaranteed to be exact. Second, mathematics courses are usually taught by pure mathematicians who have an inherent preference for analytical solutions. Pure mathematics which is based on theorem proving is not amenable to numerical methods.\", \"Despite the popularity of analytical approaches to problem solving in mathematics, there has been a growing push to integrate computer algebra systems as part of the learning process. Computer algebra systems such as Matlab and Mathematica are now routinely used in many mathematics courses. The study by Cretchley et al. [5] found that engineering students were positive about the use of technology as a learning tool in mathematics courses. The increased use of technology in class helped improve student focus and interest in lectures. Student evaluations also indicated that they had a greater level of enjoyment towards the lectures due to the use of technology. It is noteworthy that students chose not to rely too heavily on technology during the examinations despite the freedom to do so. The students found it extremely important to be competent with analytical mathematical skills as opposed to purely computational skills. Some revealed that they learn the subjects equally well without the help of scientific packages, although the perception towards the use of computer is in general highly positive. Almost all students responded positively to Matlab as an effective tool for computation and graphing. Many used Matlab for non-examination purposes. For example, they utilized it to check their handwritten mathematical steps in assignments and practice problems; and others used it for exploration beyond the standard syllabus and curriculum.\", \"The influence of computer technology on students\' academic performance and learning experience has been investigated by several authors. Abdul Majid et al. [1], [2] used Matlab as an aid to teach calculus to engineering students. The software package was used for various course learning outcomes such as graphical display of mathematical functions, exploration, identifying and predicting structural patterns in evaluating a series of complex indefinite integrals, and numerical approximations in applied mathematics. The study showed a positive impact on students\' academic performance in the final examinations. The study concluded that the integration of scientific packages into engineering mathematics courses could be effective under certain conditions. Similarly, other studies [18], [21] also found a positive impact from the use of scientific software packages on students\' motivation in learning mathematics.\", \"In a separate study by Brake [3], the authors investigated the use of Matlab in engineering mathematics courses to increase student confidence level and mathematical abilities. Matlab was used to solve concrete engineering problems which require a deep understanding of underlying mathematical principles. The study found generally positive student response to the use of software in their mathematics courses. However, the results of the study must be considered carefully given the small sample size of the subjects.\", \"Although the majority of the studies were based on the use of Matlab, several other studies considered alternative mathematics software packages. The study by Kilicman et al. [11] focused on the use of Maple to help students understand both the theoretical and computational aspects of linear algebra for engineering students. In particular, it was shown that the use of Maple facilitates the understanding of computational aspects of eigenvalues and eigenvectors. It allows students more time to focus on the theoretical aspects and the underlying mathematical principles.\", \"In a recent study by Mezhennaya and Pugachev [14], the authors compared engineering students\' perceptions regarding several mathematical software: Matlab, Mathematica and Excel. The study found that all the scientific packages considered can be used in education, under the condition that the policies for software usage are carefully implemented. The study found that many students lack hands-on experience on how to use the software. The students particularly struggled with Matlab and Mathematica finding them non user friendly. The study concluded that additional classes are required to prepare students to use software in their courses.\"]}, {\"header\": \"IV. NUMERICAL COMPUTING IN MATHEMATICS\", \"paragraph\": [\"CURRICULUM Mathematics lies at the foundation of science and engineering. The importance of mathematics courses in engineering education cannot be underestimated. These courses equip students with the fundamental skills and knowledge to study the more specialized engineering courses. Thus, student success in engineering studies depends directly on the mathematics and sciences courses. Given the significance of the mathematics courses in the engineering curriculum, it is paramount to ensure their currency with respect to the Industry 4.0.\", \"The technological advances over the last decade have created demand for more computationally proficient experts. To meet this demand, numerical computing must become a core part of engineering studies. Mathematics courses offer a natural and convenient avenue for introducing numerical computing to engineering students. There are two main factors that make mathematics courses particularly amenable to numerical computing. First, in many cases mathematical problems have numerical solutions. For instance, finding the root of a polynomial or the minimum value of a function can be done numerically. Therefore, it is both logical and appropriate to apply numerical computing to mathematical problems. Second, mathematics courses are usually taken at the beginning of the study plan. Thus, students become acquainted with numerical computing at an early stage. The computing and programming skills acquired in this manner will have a positive effect in the more advanced, downstream engineering courses.\", \"The key idea for the proposed curriculum update is the addition of computing tutorials (labs) to mathematics courses. In particular, we propose adding weekly computing tutorials (labs) related to the main lecture material. For instance, in the week in which students cover finding the extreme values of a function, there will be a computing tutorial where students learn and implement the gradient descent algorithm. The suggested length of each tutorial is 1 hour. It is enough time to implement most of the numerical algorithms at the undergraduate level. At the same time, 1 extra hour per week will not overburden the students.\", \"The exact details of numerical computing content is left for individual universities and instructors. Depending on the syllabus and course learning outcomes, the numerical computing labs will be different for each university and instructor. Nevertheless, the general ideas will be broadly similar across different curricula. To illustrate the proposed numerical computing content, we will focus on the three main concept in calculus: limits, derivatives, and integrals.\"]}, {\"header\": \"A. Limits\", \"paragraph\": [\"Limit is a fundamental concept in calculus. Students are usually taught to calculate limits using analytical approaches. Although analytical approaches work well, there is no single universal rule for calculating limits. On the other hand, in most cases, limits can be calculated numerically using essentially the same approach. To illustrate, suppose we want to calculate lim x\\u2192a + f (x). Then we can loop for k = 0 to n and calculate f (a+ 10 -k ). As k increases, a+ 10 -k approaches a, so f (a+ 10 -k ) will, in most cases, approach the limit value. We can deduce the limit by observing the values of f (a + 10 -k ) or determine that the limit does not exist if there is no pattern of convergence. The value of n can be chosen manually or using a stopping criterion. For instance, the algorithm may continue to iterate until the difference between consecutive values of f (a + 10 -k ) is below a certain threshold. The value of the limit can also be deduced automatically based on the values of f (a + 10 -k ) using various heuristics.\", \"Another common limit problem is lim x\\u2192\\u221e f (x). In this case, we can loop for k = 0 to n and calculate f (10 k ). As k increases, 10 k approaches \\u221e, so f (10 k ) will, in most cases, approach the limit value. Then the limit can be determined based on the values of f (10 k ). Various extensions and customizations of this basic approach can be made. For instance, to avoid issues with periodic functions f (10 k + \\u01eb k ), where \\u01eb k are randomly generated, can be used. Other values than 10 k can also be used as long as the sequence approaches infinity. A degree of automization can be introduced using different heuristics.\"]}, {\"header\": \"B. Derivative\", \"paragraph\": [\"Derivative is arguably the most important concept in calculus. There exist several rules such as the power rule, the product rule, the chain rule, and others to find the derivative of a function by hand. However, manual differentiation may be cumbersome when dealing with complex function. On the other hand, calculating the derivative at a point numerically is relatively straightforward. To illustrate, suppose that we want to calculate f \\u2032 (a). Recall that\", \"Therefore, to calculate f \\u2032 (a) numerically we use the same approach as with the limits. In particular, we can loop for k = 0 to n and calculate f (a+10 -k )-f (a) 10 -k\", \". Then the limit, and by extension the derivative, can be deduced (approximated) based on the calculated values. The accuracy of the approximation depends in large part on the value of n.\", \"One of the most important applications of the derivative is finding the extreme values of a function. Traditionally, this is done by first finding the critical points of the function and then applying the second derivative test. However, finding the critical points is not always possible, so numerical approaches can be used in such cases. The most popular numerical approach for finding the extreme values is based on the gradient descent (ascent) algorithm. In gradient descent, the optimal value of x is iteratively updated based on the gradient. In particular, for k = 0 to n, the updated optimal value of x is given by\", \"where \\u2207f (x) is the gradient and \\u03b1 is the step size. In the case of a single-variable function, the gradient equals simply to the derivative \\u2207f (x) = f \\u2032 (x). The step size \\u03b1 can be either fixed or dynamic. While a large value of \\u03b1 accelerates the convergence at the beginning, it may hurt the convergence in the region near the optimal value. There exist several extensions of the basic gradient descent algorithm. One such extension is gradient descent with momentum which uses the second derivative to anticipate the location of the next optimal point and thus accelerates the convergence.\"]}, {\"header\": \"C. Integrals\", \"paragraph\": [\"Integration is an important concept in engineering mathematics. Although there exist a number of rules for finding the integral, it is significantly more challenging than differentiation. Moreover, in many cases, the indefinite integral does not even exist. Therefore, numerical approaches are particularly useful for integration.\", \"To illustrate the application of numerical integration, suppose that we want to calculate\", \"The Riemann sums can be quickly calculated on a computer providing a simple, yet effective approach to calculating integrals numerically. Other popular integral approximation methods include the trapezoid rule and the Simpson\'s rule.\"]}, {\"header\": \"D. Additional considerations\", \"paragraph\": [\"The above discussion about numerical methods for calculating limits, derivatives, and integrals is easily extended to multivariate calculus. For instance, to find the partial derivative f x (a, b), we can loop for k = 0 to n and calculate\", \". Many problems related to sequences and series can similarly be solved using numerical techniques. In particular, the convergence of a series can be deduced from its partial sums. By calculating the partial sums on the computer and observing the results, we can intuit the nature of the series.\", \"Vectorization is an important aspect of numerical computing. Since the modern computer chips are optimized for matrix multiplication, it is more efficient to employ vector operations. In particular, some algorithms based on for-loops can be converted into vector operations resulting in higher efficiency and speed. For instance, the Riemann sum can be calculated with a single vector operation:\", \"where x = [x 1 , ..., x n ] is the vector of endpoints, f (x) is a vectorized function operation, and S is the vector function which returns the sum of all the coordinates. Similarly, limit calculations can be vectorized and made more efficient. Vectorization is also useful in multi-variate calculus, where operations can be performed on a vector of variables. The choice of the programming language for numerical computing requires careful consideration. There are several suitable candidates for this purpose including Python, Java, C++, Matlab, and others. Based on our experience with different programming languages, we recommend the use of Python. Python is currently the most popular programming language on the planet. It has a simple and intuitive syntax making it easy to learn and apply. Python has libraries to fit any purpose including an extensive collection of libraries related to numerical computing. The basic Python libraries related to computing are NumPy, SciPy, and SymPy. More advanced packages such as OR-Tools are also available for optimization tasks. Since Python is a universal programming language, it can be used for almost any task. Thus, students who learn Python in their mathematics courses can employ it in their other courses. In addition, numerical computing implemented in Python can be connected to other applications.\", \"V. CONCLUSION Although the classical approach to teaching mathematics is still relevant for certain student cohorts, it is outdated for engineering students. Modern engineering is increasingly reliant on computing [9], [22]. Therefore, universities must equip the student with appropriate computing skills. In particular, mathematics courses must be revised to include numerical computing content.\", \"Given the efficiency of computer-based calculations, numerical computing provides a convenient approach to problem solving in engineering mathematics. It can be integrated into the existing curriculum with little hassle and cost. In this paper, we proposed a framework for integrating numerical computing into the existing mathematics curriculum. We demonstrated how numerical approaches can be used some of the most common problems encountered in calculus. The proposed framework can be customized by individual universities to fit their special needs.\"]}]','https://drive.google.com/uc?id=1N_fEKEayl7Z-NBMYGreIZQt0eu65zk_4&export=download','2022-02-03',0,'2024-02-03 22:46:28.878131','2024-02-03 22:46:31.460263');
/*!40000 ALTER TABLE `articles_article` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_article_auteurs`
--

DROP TABLE IF EXISTS `articles_article_auteurs`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_article_auteurs` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `article_id` bigint NOT NULL,
  `auteur_id` bigint NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `Articles_article_auteurs_article_id_auteur_id_0d281405_uniq` (`article_id`,`auteur_id`),
  KEY `Articles_article_aut_auteur_id_2e5053e6_fk_Articles_` (`auteur_id`),
  CONSTRAINT `Articles_article_aut_article_id_57e63cee_fk_Articles_` FOREIGN KEY (`article_id`) REFERENCES `articles_article` (`id`),
  CONSTRAINT `Articles_article_aut_auteur_id_2e5053e6_fk_Articles_` FOREIGN KEY (`auteur_id`) REFERENCES `articles_auteur` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=25 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_article_auteurs`
--

LOCK TABLES `articles_article_auteurs` WRITE;
/*!40000 ALTER TABLE `articles_article_auteurs` DISABLE KEYS */;
INSERT INTO `articles_article_auteurs` VALUES (1,2,1),(2,2,2),(3,3,3),(4,12,4),(5,12,5),(6,12,6),(7,12,7),(8,12,8),(9,12,9),(10,14,10),(11,14,11),(12,14,12),(13,16,13),(14,18,14),(15,18,15),(16,18,16),(17,18,17),(18,18,18),(19,18,19),(20,18,20),(21,18,21),(22,18,22),(23,19,23),(24,19,24);
/*!40000 ALTER TABLE `articles_article_auteurs` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_article_mot_cles`
--

DROP TABLE IF EXISTS `articles_article_mot_cles`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_article_mot_cles` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `article_id` bigint NOT NULL,
  `motcle_id` bigint NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `Articles_article_mot_cles_article_id_motcle_id_f4c8e5af_uniq` (`article_id`,`motcle_id`),
  KEY `Articles_article_mot_motcle_id_9f5aaa4c_fk_Articles_` (`motcle_id`),
  CONSTRAINT `Articles_article_mot_article_id_172b81e0_fk_Articles_` FOREIGN KEY (`article_id`) REFERENCES `articles_article` (`id`),
  CONSTRAINT `Articles_article_mot_motcle_id_9f5aaa4c_fk_Articles_` FOREIGN KEY (`motcle_id`) REFERENCES `articles_motcle` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=50 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_article_mot_cles`
--

LOCK TABLES `articles_article_mot_cles` WRITE;
/*!40000 ALTER TABLE `articles_article_mot_cles` DISABLE KEYS */;
INSERT INTO `articles_article_mot_cles` VALUES (1,1,1),(2,1,2),(3,1,3),(4,2,4),(5,2,5),(6,2,6),(7,2,7),(8,2,8),(9,3,9),(10,3,10),(11,3,11),(12,4,12),(13,4,13),(14,4,14),(15,4,15),(16,5,16),(17,5,17),(18,5,18),(19,5,19),(20,5,20),(21,5,21),(22,5,22),(23,11,28),(24,11,29),(25,11,30),(26,11,31),(27,12,32),(28,12,33),(29,12,34),(30,12,35),(31,13,36),(32,13,37),(33,13,38),(34,14,39),(35,14,40),(36,14,41),(37,14,42),(38,15,43),(39,15,44),(40,15,45),(41,16,46),(42,17,47),(43,17,48),(44,17,49),(45,18,50),(46,19,51),(47,19,52),(48,19,53),(49,19,54);
/*!40000 ALTER TABLE `articles_article_mot_cles` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_article_references_bibliographique`
--

DROP TABLE IF EXISTS `articles_article_references_bibliographique`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_article_references_bibliographique` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `article_id` bigint NOT NULL,
  `referencebibliographique_id` bigint NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `Articles_article_referen_article_id_referencebibl_b1195b0d_uniq` (`article_id`,`referencebibliographique_id`),
  KEY `Articles_article_ref_referencebibliograph_2434b18a_fk_Articles_` (`referencebibliographique_id`),
  CONSTRAINT `Articles_article_ref_article_id_ebebaae6_fk_Articles_` FOREIGN KEY (`article_id`) REFERENCES `articles_article` (`id`),
  CONSTRAINT `Articles_article_ref_referencebibliograph_2434b18a_fk_Articles_` FOREIGN KEY (`referencebibliographique_id`) REFERENCES `articles_referencebibliographique` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=504 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_article_references_bibliographique`
--

LOCK TABLES `articles_article_references_bibliographique` WRITE;
/*!40000 ALTER TABLE `articles_article_references_bibliographique` DISABLE KEYS */;
INSERT INTO `articles_article_references_bibliographique` VALUES (1,1,1),(2,1,2),(3,1,3),(4,1,4),(5,1,5),(6,1,6),(7,1,7),(8,1,8),(9,1,9),(10,1,10),(11,1,11),(12,1,12),(13,1,13),(14,1,14),(15,1,15),(16,1,16),(17,1,17),(18,1,18),(19,1,19),(20,1,20),(21,1,21),(22,1,22),(23,1,23),(24,1,24),(25,1,25),(26,2,26),(27,2,27),(28,2,28),(29,2,29),(30,2,30),(31,2,31),(32,2,32),(33,2,33),(34,2,34),(35,2,35),(36,2,36),(37,2,37),(38,2,38),(39,2,39),(40,2,40),(41,3,41),(42,4,42),(43,4,43),(44,4,44),(45,4,45),(46,4,46),(47,4,47),(48,4,48),(49,4,49),(50,4,50),(51,4,51),(52,4,52),(53,4,53),(54,4,54),(55,4,55),(56,4,56),(57,4,57),(58,4,58),(59,4,59),(60,4,60),(61,4,61),(62,4,62),(63,4,63),(64,4,64),(65,5,65),(66,5,66),(67,5,67),(68,5,68),(69,5,69),(70,5,70),(71,5,71),(72,5,72),(73,11,73),(74,11,74),(75,11,75),(76,11,76),(77,11,77),(78,11,78),(79,11,79),(80,11,80),(81,11,81),(82,11,82),(83,11,83),(84,11,84),(85,11,85),(86,11,86),(87,11,87),(88,11,88),(89,11,89),(90,11,90),(91,11,91),(92,11,92),(93,11,93),(94,11,94),(95,11,95),(96,11,96),(97,11,97),(98,11,98),(99,11,99),(100,11,100),(101,11,101),(102,11,102),(103,11,103),(104,11,104),(105,11,105),(106,11,106),(107,11,107),(108,11,108),(109,11,109),(110,11,110),(111,11,111),(112,11,112),(113,11,113),(114,11,114),(115,11,115),(116,11,116),(117,11,117),(118,11,118),(119,11,119),(120,11,120),(121,11,121),(122,11,122),(123,11,123),(124,11,124),(125,11,125),(126,11,126),(127,11,127),(128,11,128),(129,11,129),(130,11,130),(131,11,131),(132,11,132),(133,11,133),(134,11,134),(135,11,135),(136,11,136),(137,11,137),(138,11,138),(139,11,139),(140,11,140),(141,11,141),(142,11,142),(143,11,143),(144,11,144),(145,11,145),(146,11,146),(147,11,147),(148,11,148),(149,11,149),(150,11,150),(151,11,151),(152,11,152),(153,11,153),(154,11,154),(155,11,155),(156,11,156),(157,11,157),(158,11,158),(159,11,159),(160,11,160),(161,11,161),(162,11,162),(163,11,163),(164,11,164),(165,11,165),(166,11,166),(167,11,167),(168,11,168),(169,11,169),(170,11,170),(171,11,171),(172,11,172),(173,11,173),(174,11,174),(175,11,175),(176,11,176),(177,11,177),(178,11,178),(179,11,179),(180,11,180),(181,11,181),(182,11,182),(183,12,183),(184,12,184),(185,12,185),(186,12,186),(187,12,187),(188,12,188),(189,12,189),(190,12,190),(191,12,191),(192,12,192),(193,12,193),(194,12,194),(195,12,195),(196,12,196),(197,12,197),(198,12,198),(199,12,199),(200,13,200),(201,13,201),(202,13,202),(203,13,203),(204,13,204),(205,13,205),(206,13,206),(207,13,207),(208,13,208),(209,13,209),(210,13,210),(211,13,211),(212,13,212),(213,13,213),(214,13,214),(215,13,215),(216,13,216),(217,13,217),(218,13,218),(219,13,219),(220,13,220),(221,13,221),(222,13,222),(223,13,223),(224,13,224),(225,13,225),(226,13,226),(227,13,227),(228,13,228),(229,13,229),(230,13,230),(231,13,231),(232,13,232),(233,13,233),(234,13,234),(235,13,235),(236,13,236),(237,13,237),(238,13,238),(239,13,239),(240,13,240),(241,13,241),(242,13,242),(243,13,243),(244,13,244),(245,13,245),(246,13,246),(247,13,247),(248,13,248),(249,13,249),(250,13,250),(251,13,251),(252,13,252),(253,13,253),(254,13,254),(255,13,255),(256,13,256),(257,13,257),(258,13,258),(259,13,259),(260,14,260),(261,14,261),(262,14,262),(263,14,263),(264,14,264),(265,14,265),(266,14,266),(267,14,267),(268,14,268),(269,14,269),(270,14,270),(271,14,271),(272,14,272),(273,14,273),(274,14,274),(275,14,275),(276,14,276),(277,14,277),(278,14,278),(279,14,279),(280,14,280),(281,14,281),(282,14,282),(283,14,283),(284,14,284),(285,14,285),(286,14,286),(287,14,287),(288,14,288),(289,14,289),(290,14,290),(291,14,291),(292,14,292),(293,14,293),(294,14,294),(295,14,295),(296,14,296),(297,14,297),(298,14,298),(299,14,299),(300,14,300),(301,14,301),(302,14,302),(303,14,303),(304,14,304),(305,14,305),(306,14,306),(307,14,307),(308,14,308),(309,15,309),(310,15,310),(311,15,311),(312,15,312),(313,15,313),(314,15,314),(315,15,315),(316,15,316),(317,15,317),(318,15,318),(319,15,319),(320,15,320),(321,15,321),(322,15,322),(323,16,323),(324,16,324),(325,16,325),(326,16,326),(327,16,327),(328,16,328),(329,16,329),(330,16,330),(331,16,331),(332,16,332),(333,16,333),(334,16,334),(335,16,335),(336,16,336),(337,16,337),(338,16,338),(339,16,339),(340,16,340),(341,16,341),(342,16,342),(343,16,343),(344,16,344),(345,16,345),(346,16,346),(347,16,347),(348,16,348),(349,16,349),(350,16,350),(351,16,351),(352,16,352),(353,16,353),(354,16,354),(355,16,355),(356,16,356),(357,16,357),(358,16,358),(359,16,359),(360,16,360),(361,16,361),(362,16,362),(363,16,363),(364,16,364),(365,16,365),(366,16,366),(367,16,367),(368,16,368),(369,16,369),(370,16,370),(371,16,371),(372,16,372),(373,16,373),(374,16,374),(375,16,375),(376,16,376),(377,17,377),(378,17,378),(379,17,379),(380,17,380),(381,17,381),(382,17,382),(383,17,383),(384,17,384),(385,17,385),(386,17,386),(387,17,387),(388,17,388),(389,17,389),(390,17,390),(391,17,391),(392,17,392),(393,18,393),(394,18,394),(395,18,395),(396,18,396),(397,18,397),(398,18,398),(399,18,399),(400,18,400),(401,18,401),(402,18,402),(403,18,403),(404,18,404),(405,18,405),(406,18,406),(407,18,407),(408,18,408),(409,18,409),(410,18,410),(411,18,411),(412,18,412),(413,18,413),(414,18,414),(415,18,415),(416,18,416),(417,18,417),(418,18,418),(419,18,419),(420,18,420),(421,18,421),(422,18,422),(423,18,423),(424,18,424),(425,18,425),(426,18,426),(427,18,427),(428,18,428),(429,18,429),(430,18,430),(431,18,431),(432,18,432),(433,18,433),(434,18,434),(435,18,435),(436,18,436),(437,18,437),(438,18,438),(439,18,439),(440,18,440),(441,18,441),(442,18,442),(443,18,443),(444,18,444),(445,18,445),(446,18,446),(447,18,447),(448,18,448),(449,18,449),(450,18,450),(451,18,451),(452,18,452),(453,18,453),(454,18,454),(455,18,455),(456,18,456),(457,18,457),(458,18,458),(459,18,459),(460,18,460),(461,18,461),(462,18,462),(463,18,463),(464,18,464),(465,18,465),(466,18,466),(467,18,467),(468,18,468),(469,18,469),(470,18,470),(471,18,471),(472,18,472),(473,18,473),(474,18,474),(475,18,475),(476,18,476),(477,18,477),(478,18,478),(479,18,479),(480,18,480),(481,19,481),(482,19,482),(483,19,483),(484,19,484),(485,19,485),(486,19,486),(487,19,487),(488,19,488),(489,19,489),(490,19,490),(491,19,491),(492,19,492),(493,19,493),(494,19,494),(495,19,495),(496,19,496),(497,19,497),(498,19,498),(499,19,499),(500,19,500),(501,19,501),(502,19,502),(503,19,503);
/*!40000 ALTER TABLE `articles_article_references_bibliographique` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_auteur`
--

DROP TABLE IF EXISTS `articles_auteur`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_auteur` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `nom` longtext NOT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=25 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_auteur`
--

LOCK TABLES `articles_auteur` WRITE;
/*!40000 ALTER TABLE `articles_auteur` DISABLE KEYS */;
INSERT INTO `articles_auteur` VALUES (1,'Vlado Menkovski','2024-02-03 22:08:18.785201','2024-02-03 22:08:18.785201'),(2,'Dimitrios Metafas','2024-02-03 22:08:18.805730','2024-02-03 22:08:18.805730'),(3,'Tetsuo Tamai','2024-02-03 22:08:21.998533','2024-02-03 22:08:21.998533'),(4,'Stephen Macneil','2024-02-03 22:25:48.294360','2024-02-03 22:25:48.294360'),(5,'Andrew Tran','2024-02-03 22:25:48.314575','2024-02-03 22:25:48.314575'),(6,'Dan Mogil','2024-02-03 22:25:48.334595','2024-02-03 22:25:48.334595'),(7,'Seth Bernstein','2024-02-03 22:25:48.356426','2024-02-03 22:25:48.356426'),(8,'Erin Ross','2024-02-03 22:25:48.373091','2024-02-03 22:25:48.373091'),(9,'Ziheng Huang','2024-02-03 22:25:48.391060','2024-02-03 22:25:48.391060'),(10,'Sheshera Mysore','2024-02-03 22:25:54.592772','2024-02-03 22:25:54.592772'),(11,'Andrew Mccallum','2024-02-03 22:25:54.605695','2024-02-03 22:25:54.605695'),(12,'Hamed Zamani','2024-02-03 22:25:54.618956','2024-02-03 22:25:54.618956'),(13,'Sahil Bansal','2024-02-03 22:46:06.920758','2024-02-03 22:46:06.920758'),(14,'Paul Saves','2024-02-03 22:46:19.134908','2024-02-03 22:46:19.134908'),(15,'Rémi Lafage','2024-02-03 22:46:19.246310','2024-02-03 22:46:19.246310'),(16,'Nathalie Bartoli','2024-02-03 22:46:19.361880','2024-02-03 22:46:19.361880'),(17,'Youssef Diouane','2024-02-03 22:46:19.486097','2024-02-03 22:46:19.486097'),(18,'Jasper Bussemaker','2024-02-03 22:46:19.596345','2024-02-03 22:46:19.596345'),(19,'Thierry Lefebvre','2024-02-03 22:46:19.705535','2024-02-03 22:46:19.705535'),(20,'John Hwang','2024-02-03 22:46:19.834658','2024-02-03 22:46:19.834658'),(21,'Joseph Morlier','2024-02-03 22:46:19.954424','2024-02-03 22:46:19.954424'),(22,'Joaquim Martins','2024-02-03 22:46:20.073563','2024-02-03 22:46:20.073563'),(23,'Firuz Kamalov','2024-02-03 22:46:29.363261','2024-02-03 22:46:29.363261'),(24,'Ho-Hon Leung','2024-02-03 22:46:29.507746','2024-02-03 22:46:29.507746');
/*!40000 ALTER TABLE `articles_auteur` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_auteur_institutions`
--

DROP TABLE IF EXISTS `articles_auteur_institutions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_auteur_institutions` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `auteur_id` bigint NOT NULL,
  `institution_id` bigint NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `Articles_auteur_institut_auteur_id_institution_id_4c8f9ade_uniq` (`auteur_id`,`institution_id`),
  KEY `Articles_auteur_inst_institution_id_63b7c819_fk_Articles_` (`institution_id`),
  CONSTRAINT `Articles_auteur_inst_auteur_id_2211f937_fk_Articles_` FOREIGN KEY (`auteur_id`) REFERENCES `articles_auteur` (`id`),
  CONSTRAINT `Articles_auteur_inst_institution_id_63b7c819_fk_Articles_` FOREIGN KEY (`institution_id`) REFERENCES `articles_institution` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=25 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_auteur_institutions`
--

LOCK TABLES `articles_auteur_institutions` WRITE;
/*!40000 ALTER TABLE `articles_auteur_institutions` DISABLE KEYS */;
INSERT INTO `articles_auteur_institutions` VALUES (1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5),(6,6,6),(7,7,7),(8,8,8),(9,9,9),(10,10,10),(11,11,11),(12,12,12),(13,13,13),(14,14,14),(15,15,15),(16,16,16),(17,17,17),(18,18,18),(19,19,19),(20,20,20),(21,21,21),(22,22,22),(23,23,23),(24,24,24);
/*!40000 ALTER TABLE `articles_auteur_institutions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_institution`
--

DROP TABLE IF EXISTS `articles_institution`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_institution` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `nom` longtext NOT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=25 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_institution`
--

LOCK TABLES `articles_institution` WRITE;
/*!40000 ALTER TABLE `articles_institution` DISABLE KEYS */;
INSERT INTO `articles_institution` VALUES (1,'Athens Information Technology','2024-02-03 22:08:18.791184','2024-02-03 22:08:18.791184'),(2,'Athens Information Technology','2024-02-03 22:08:18.808274','2024-02-03 22:08:18.808274'),(3,'Graduate School of Arts','2024-02-03 22:08:22.006346','2024-02-03 22:08:22.006346'),(4,'Temple University Philadelphia','2024-02-03 22:25:48.296920','2024-02-03 22:25:48.296920'),(5,'Temple University Philadelphia','2024-02-03 22:25:48.324142','2024-02-03 22:25:48.324142'),(6,'Temple University Philadelphia','2024-02-03 22:25:48.342309','2024-02-03 22:25:48.342309'),(7,'Temple University Philadelphia','2024-02-03 22:25:48.357059','2024-02-03 22:25:48.357059'),(8,'Temple University Philadelphia','2024-02-03 22:25:48.377943','2024-02-03 22:25:48.377943'),(9,'University of California-San Diego La Jolla','2024-02-03 22:25:48.395509','2024-02-03 22:25:48.395509'),(10,'University of Massachusetts Amherst','2024-02-03 22:25:54.597360','2024-02-03 22:25:54.597360'),(11,'University of Massachusetts Amherst','2024-02-03 22:25:54.608100','2024-02-03 22:25:54.608100'),(12,'University of Massachusetts Amherst','2024-02-03 22:25:54.622489','2024-02-03 22:25:54.622489'),(13,'Department of Civil Engineering','2024-02-03 22:46:06.923279','2024-02-03 22:46:06.923279'),(14,'ONERA/DTIS','2024-02-03 22:46:19.138450','2024-02-03 22:46:19.138450'),(15,'ONERA/DTIS','2024-02-03 22:46:19.250307','2024-02-03 22:46:19.250307'),(16,'ONERA/DTIS','2024-02-03 22:46:19.364874','2024-02-03 22:46:19.364874'),(17,'Polytechnique Montréal','2024-02-03 22:46:19.490112','2024-02-03 22:46:19.490112'),(18,'German Aerospace Center (DLR)','2024-02-03 22:46:19.598885','2024-02-03 22:46:19.598885'),(19,'ONERA/DTIS','2024-02-03 22:46:19.707881','2024-02-03 22:46:19.707881'),(20,'Department of Mechanical and Aerospace Engineering','2024-02-03 22:46:19.837255','2024-02-03 22:46:19.838764'),(21,'ISAE-SUPAERO','2024-02-03 22:46:19.958145','2024-02-03 22:46:19.958145'),(22,'Department of Aerospace Engineering','2024-02-03 22:46:20.076121','2024-02-03 22:46:20.076121'),(23,'Department of Electrical Engineering','2024-02-03 22:46:29.366314','2024-02-03 22:46:29.366314'),(24,'Department of Mathematical Sciences','2024-02-03 22:46:29.510758','2024-02-03 22:46:29.512129');
/*!40000 ALTER TABLE `articles_institution` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_motcle`
--

DROP TABLE IF EXISTS `articles_motcle`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_motcle` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `text` longtext NOT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=55 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_motcle`
--

LOCK TABLES `articles_motcle` WRITE;
/*!40000 ALTER TABLE `articles_motcle` DISABLE KEYS */;
INSERT INTO `articles_motcle` VALUES (1,'Software and its engineering → Software design engineering Model design','2024-02-03 22:08:16.165058','2024-02-03 22:08:16.165058'),(2,'learning model','2024-02-03 22:08:16.176468','2024-02-03 22:08:16.176468'),(3,'Gamification','2024-02-03 22:08:16.181506','2024-02-03 22:08:16.181506'),(4,'I.2.1 [Applications and Expert Systems]: Games Algorithms','2024-02-03 22:08:18.747105','2024-02-03 22:08:18.747105'),(5,'Performance Game AI','2024-02-03 22:08:18.755380','2024-02-03 22:08:18.755380'),(6,'Case Based Reasoning','2024-02-03 22:08:18.762745','2024-02-03 22:08:18.762745'),(7,'AI Planning','2024-02-03 22:08:18.770523','2024-02-03 22:08:18.770523'),(8,'Game Trees','2024-02-03 22:08:18.776847','2024-02-03 22:08:18.776847'),(9,'K.3.2 [Computers and Education]: Computer and Information Science Education-computer science education; D.2.1 [Software Engineering]: Requirements/Specification-modeling software modeling','2024-02-03 22:08:21.969224','2024-02-03 22:08:21.969224'),(10,'software engineering education','2024-02-03 22:08:21.978884','2024-02-03 22:08:21.978884'),(11,'UML','2024-02-03 22:08:21.988454','2024-02-03 22:08:21.988454'),(12,'Computer systems organization → Embedded systems','2024-02-03 22:08:24.590271','2024-02-03 22:08:24.590271'),(13,'Redundancy','2024-02-03 22:08:24.599372','2024-02-03 22:08:24.599372'),(14,'Robotics','2024-02-03 22:08:24.605205','2024-02-03 22:08:24.605205'),(15,'• Networks → Network reliability Dataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional LSTM, GRU, Word-Embeddings, CNN','2024-02-03 22:08:24.611955','2024-02-03 22:08:24.611955'),(16,'conference proceedings','2024-02-03 22:08:27.477243','2024-02-03 22:08:27.477243'),(17,'Design','2024-02-03 22:08:27.486205','2024-02-03 22:08:27.486205'),(18,'• Software and its engineering → System description languages','2024-02-03 22:08:27.493725','2024-02-03 22:08:27.493725'),(19,'Unified Modeling Language (UML)','2024-02-03 22:08:27.502486','2024-02-03 22:08:27.502486'),(20,'Software design engineering','2024-02-03 22:08:27.508954','2024-02-03 22:08:27.508954'),(21,'• Theory of computation → Quantum computation theory','2024-02-03 22:08:27.523291','2024-02-03 22:08:27.523291'),(22,'Quantum information theory quantum computing, software engineering, UML','2024-02-03 22:08:27.531116','2024-02-03 22:08:27.531116'),(23,'CCS CONCEPTS','2024-02-03 22:19:40.851987','2024-02-03 22:19:40.851987'),(24,'large language models','2024-02-03 22:19:43.049509','2024-02-03 22:19:43.049509'),(25,'CCS CONCEPTS','2024-02-03 22:19:45.351991','2024-02-03 22:19:45.351991'),(26,'CCS CONCEPTS','2024-02-03 22:19:47.595831','2024-02-03 22:19:47.595831'),(27,'D.1.7 [Programming Techniques]: Visual Programming','2024-02-03 22:19:49.623245','2024-02-03 22:19:49.623245'),(28,'CCS CONCEPTS','2024-02-03 22:25:44.961534','2024-02-03 22:25:44.961534'),(29,'Human-centered computing → HCI theory, concepts and models','2024-02-03 22:25:44.967424','2024-02-03 22:25:44.967424'),(30,'• Software and its engineering → Designing software','2024-02-03 22:25:44.973088','2024-02-03 22:25:44.973088'),(31,'• Computing methodologies → Generative large language models, foundation models, conversational interaction, human-centered AI','2024-02-03 22:25:44.979655','2024-02-03 22:25:44.979655'),(32,'large language models','2024-02-03 22:25:48.264209','2024-02-03 22:25:48.264209'),(33,'natural language processing','2024-02-03 22:25:48.272620','2024-02-03 22:25:48.272620'),(34,'code explanations','2024-02-03 22:25:48.278124','2024-02-03 22:25:48.278124'),(35,'computer science education','2024-02-03 22:25:48.285502','2024-02-03 22:25:48.285502'),(36,'CCS CONCEPTS','2024-02-03 22:25:51.634526','2024-02-03 22:25:51.634526'),(37,'Computing methodologies → Information extraction;','2024-02-03 22:25:51.641067','2024-02-03 22:25:51.641067'),(38,'Human-centered computing → Text input Covid-19 no-vax, news framing, GPT-3, prompt-engineering, transformers, large language models','2024-02-03 22:25:51.646417','2024-02-03 22:25:51.646417'),(39,'CCS CONCEPTS','2024-02-03 22:25:54.567822','2024-02-03 22:25:54.567822'),(40,'Information systems → Recommender systems','2024-02-03 22:25:54.574329','2024-02-03 22:25:54.574329'),(41,'Users and interactive retrieval','2024-02-03 22:25:54.580594','2024-02-03 22:25:54.580594'),(42,'• Computing methodologies → Natural language generation','2024-02-03 22:25:54.588791','2024-02-03 22:25:54.588791'),(43,'D.1.7 [Programming Techniques]: Visual Programming','2024-02-03 22:25:57.362750','2024-02-03 22:25:57.362750'),(44,'D.2.2 [Design Tools and Techniques]: Computer-aided software engineering (CASE)','2024-02-03 22:25:57.371617','2024-02-03 22:25:57.371617'),(45,'D.2.6 [Software Engineering]: Programming Environments-Graphical environments Orthographic Software Modeling, View-based Modeling Behavioral KobrA2::SUM::Constraint::Structural KobrA2::SUM::Constraint','2024-02-03 22:25:57.386766','2024-02-03 22:25:57.386766'),(46,'Stochastic subset optimization Voronoi tessellation Stochastic simulation','2024-02-03 22:46:06.734598','2024-02-03 22:46:06.734598'),(47,'Fuzzy logic controller (FLC)','2024-02-03 22:46:14.653066','2024-02-03 22:46:14.653066'),(48,'Maximum power point tracker (MPPT)','2024-02-03 22:46:14.749553','2024-02-03 22:46:14.749553'),(49,'Photovoltaic (PV)','2024-02-03 22:46:14.840092','2024-02-03 22:46:14.840092'),(50,'Surrogate modeling Gaussian process Kriging Hierarchical problems Hierarchical','2024-02-03 22:46:19.022213','2024-02-03 22:46:19.022213'),(51,'engineering mathematics','2024-02-03 22:46:28.988559','2024-02-03 22:46:28.988559'),(52,'numerical computing','2024-02-03 22:46:29.080222','2024-02-03 22:46:29.080222'),(53,'education','2024-02-03 22:46:29.172086','2024-02-03 22:46:29.172086'),(54,'Industry 4.0','2024-02-03 22:46:29.271622','2024-02-03 22:46:29.271622');
/*!40000 ALTER TABLE `articles_motcle` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_referencebibliographique`
--

DROP TABLE IF EXISTS `articles_referencebibliographique`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_referencebibliographique` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `nom` longtext NOT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=504 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_referencebibliographique`
--

LOCK TABLES `articles_referencebibliographique` WRITE;
/*!40000 ALTER TABLE `articles_referencebibliographique` DISABLE KEYS */;
INSERT INTO `articles_referencebibliographique` VALUES (1,'[1] Rick Adcock, Edward Alef, Bruce Amato, Mark Ardis, Larry Bernstein, Barry Boehm, Pierre Bourque, John Brackett, Murray Cantor, Lillian Cassel. \"Curriculum guidelines for graduate degree programs in software engineering\"  ACM, 2009.','2024-02-03 22:08:16.191100','2024-02-03 22:08:16.191100'),(2,'[2] Mark Ardis, David Budgen, Gregory W Hislop, Jeff Offutt, Mark Sebern, Willem Visser. \"SE 2014: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering\" Computer 48(11):106-109 Institute of Electrical and Electronics Engineers (IEEE), 2015-11. DOI: 10.1109/mc.2015.345','2024-02-03 22:08:16.199797','2024-02-03 22:08:16.199797'),(3,'[3] Sébastien Valerio Cosentino, Jordi Gérard, Sagrera Cabot. \"A modelbased approach to gamify the learning of modeling\"  .','2024-02-03 22:08:16.208408','2024-02-03 22:08:16.208408'),(4,'[4] Daniel De, Paula Porto, Gabriela Martins De, Jesus, Fabiano Cutigi Ferrari, Sandra Camargo Pinto, Ferraz Fabbri. \"Initiatives and challenges of using gamification in software engineering: A Systematic Mapping\" Journal of Systems and Software 173(None):110870 .','2024-02-03 22:08:16.233598','2024-02-03 22:08:16.233598'),(5,'[5] Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton O\'hara, Dan Dixon. \"Gamification. using game-design elements in non-gaming contexts\"  ACM, 2011-05-07. DOI: 10.1145/1979742.1979575','2024-02-03 22:08:16.240729','2024-02-03 22:08:16.240729'),(6,'[6] Ana Fernández-Saez. \"A systematic literature review on the quality of UML models\" J. Data. Manage 22(3):46-70 .','2024-02-03 22:08:16.245528','2024-02-03 22:08:16.245528'),(7,'[7] Kleinner Farias, Alessandro Garcia, Carlos Lucena. \"Evaluating the Impact of Aspects on Inconsistency Detection Effort: A Controlled Experiment\"  Springer Berlin Heidelberg, 2012. DOI: 10.1007/978-3-642-33666-9_15','2024-02-03 22:08:16.252593','2024-02-03 22:08:16.252593'),(8,'[8] Kleinner Farias, Alessandro Garcia, Carlos Lucena, Luiz Gonzaga, Cristiano André Da Costa, Rodrigo Da Rosa Righi, Fábio Basso, Toacy Oliveira. \"Towards a quality model for model composition effort\"  ACM, 2014-03-24. DOI: 10.1145/2554850.2555131','2024-02-03 22:08:16.258545','2024-02-03 22:08:16.258545'),(9,'[9] Kleinner Farias, Alessandro Garcia, Jon Whittle, Christina Von Flach garcia chavez, Carlos Lucena. \"Evaluating the effort of composing design models: a controlled experiment\" Software & Systems Modeling 14(4):1349-1365 Springer Science and Business Media LLC, 2015. DOI: 10.1007/s10270-014-0408-2','2024-02-03 22:08:16.265231','2024-02-03 22:08:16.265231'),(10,'[10] Kleinner Farias, Toacy Cavalcante, Lucian José Gonçales, Vinicius Bischoff. \"UML2Merge: a UML extension for model merging\" IET Software 13(6):575-586 Institution of Engineering and Technology (IET), 2019-12. DOI: 10.1049/iet-sen.2018.5104','2024-02-03 22:08:16.269391','2024-02-03 22:08:16.269391'),(11,'[11] Juho Hamari, Jonna Koivisto, Harri Sarsa. \"Does Gamification Work? -- A Literature Review of Empirical Studies on Gamification\"  IEEE, 2014-01. DOI: 10.1109/hicss.2014.377','2024-02-03 22:08:16.274232','2024-02-03 22:08:16.274232'),(12,'[12] Mantas Jurgelaitis, Vaidotas Drungilas, Lina Čeponienė. \"Gamified Moodle Course for Teaching UML\" Baltic Journal of Modern Computing 6(2):119-127 University of Latvia, 2018. DOI: 10.22364/bjmc.2018.6.2.03','2024-02-03 22:08:16.280384','2024-02-03 22:08:16.280384'),(13,'[13] A Barbara, Shari L Kitchenham, Pfleeger. \"Personal opinion surveys\"  Springer, 2008.','2024-02-03 22:08:16.284113','2024-02-03 22:08:16.284113'),(14,'[14] Christian Franz, Josef Lange. \"Assessing and Improving the Quality of Modeling: A series of Empirical Studies about the UML\"  .','2024-02-03 22:08:16.296975','2024-02-03 22:08:16.296975'),(15,'[15] Guttorm Odd Ivar Lindland, Arne Sindre, Solvberg. \"Understanding quality in conceptual modeling\" IEEE software 11(2):42-49 .','2024-02-03 22:08:16.301435','2024-02-03 22:08:16.301435'),(16,'[16] Nikola Marangunić, Andrina Granić. \"Technology acceptance model: a literature review from 1986 to 2013\" Universal Access in the Information Society 14(1):81-95 Springer Science and Business Media LLC, 2015. DOI: 10.1007/s10209-014-0348-1','2024-02-03 22:08:16.306860','2024-02-03 22:08:16.306860'),(17,'[17] Beatriz Marín. \"Lessons Learned About Gamification in Software Engineering Education\"  IGI Global, 2021. DOI: 10.4018/978-1-7998-7552-9.ch008','2024-02-03 22:08:16.312554','2024-02-03 22:08:16.312554'),(18,'[18] Kleinner Oliveira, Alessandro Garcia, Jon Whittle. \"On the quantitative assessment of class model compositions: An exploratory study. 1th ESMDE at MODELS\"  .','2024-02-03 22:08:16.319882','2024-02-03 22:08:16.319882'),(19,'[19] Omg. \"Information technology. Object Management Group Unified Modeling Language (OMG UML)\"  BSI British Standards, 2017. DOI: 10.3403/30193744u','2024-02-03 22:08:16.323777','2024-02-03 22:08:16.323777'),(20,'[20] Sofia Ouhbi, Nuno Pombo. \"Software Engineering Education: Challenges and Perspectives\"  IEEE, 2020-04. DOI: 10.1109/educon45650.2020.9125353','2024-02-03 22:08:16.331089','2024-02-03 22:08:16.331089'),(21,'[21] Oscar Pedreira, Félix García, Nieves Brisaboa, Mario Piattini. \"Gamification in software engineering – A systematic mapping\" Information and Software Technology 57(None):157-168 Elsevier BV, 2015-01. DOI: 10.1016/j.infsof.2014.08.007','2024-02-03 22:08:16.335239','2024-02-03 22:08:16.335239'),(22,'[22] Pedro Rodrigues, Mauricio Souza, Eduardo Figueiredo. \"Games and Gamification in Software Engineering Education: A Survey with Educators\"  IEEE, 2018-10. DOI: 10.1109/fie.2018.8658524','2024-02-03 22:08:16.338474','2024-02-03 22:08:16.338474'),(23,'[23] Kevin Werbach, Dan Hunter. \"For the Win, Revised and Updated Edition\"  Wharton School Press, 2012. DOI: 10.2307/j.ctv2hdrfsm','2024-02-03 22:08:16.348478','2024-02-03 22:08:16.348478'),(24,'[24] Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, Anders Wesslén. \"Experimentation in Software Engineering\"  Springer Berlin Heidelberg, 2012. DOI: 10.1007/978-3-642-29044-2','2024-02-03 22:08:16.353861','2024-02-03 22:08:16.353861'),(25,'[25] Alfa Yohannis, Yulius Prabowo. \"Sort Attack: Visualization and Gamification of Sorting Algorithm Learning\"  IEEE, 2016. DOI: 10.1109/vs-games.2015.7295785','2024-02-03 22:08:16.358671','2024-02-03 22:08:16.358671'),(26,'[1] Minimax, Wikipedia. \"Chronologie der Online-Enzyklopädie Wikipedia\"  DE GRUYTER, 2008-04-23. DOI: 10.1515/9783110376357-018','2024-02-03 22:08:18.818399','2024-02-03 22:08:18.818399'),(27,'[2] Von Neumann, J. \"Zur Theorie der Gesellschaftsspiele\" Mathematische Annalen 100(1):295-320 Springer Science and Business Media LLC, 1928-12. DOI: 10.1007/bf01448847','2024-02-03 22:08:18.824006','2024-02-03 22:08:18.824006'),(28,'[3] Mostofa Najmus Sakib. \"Automated Detection of Sockpuppet Accounts in Wikipedia\" Automated Planning. Wikipedia None(None):None Boise State University, Albertsons Library, 2008-04-23. DOI: 10.18122/td.1994.boisestate','2024-02-03 22:08:18.829963','2024-02-03 22:08:18.829963'),(29,'[4] Antonio Sanchez-Ruiz. \"Game AI for a Turn-based Strategy Game with Plan Adaptation and Ontology-based retrieval\"  .','2024-02-03 22:08:18.836122','2024-02-03 22:08:18.836122'),(30,'[5] K Erol, J Hendler, D Nau. \"Semantics for hierarchical task-network planning\"  .','2024-02-03 22:08:18.842192','2024-02-03 22:08:18.842192'),(31,'[6] S J J Smith, Dana S Nau, T A Throop. \"A PLANNING APPROACH TO DECLARER PLAY IN CONTRACT BRIDGE\" Computational Intelligence 12(1):106-130 Wiley, 1996-02. DOI: 10.1111/j.1467-8640.1996.tb00255.x','2024-02-03 22:08:18.849744','2024-02-03 22:08:18.849744'),(32,'[7] J Schaeffer. \"One Jump Ahead: Challenging Human Supremacy in Checkers\" ICGA Journal 20(2):93-93 IOS Press, 1997-06-01. DOI: 10.3233/icg-1997-20207','2024-02-03 22:08:18.856320','2024-02-03 22:08:18.856320'),(33,'[8] . \"Overview of the IBM Blue Gene/P project\" IBM Journal of Research and Development 52(1.2):199-220 IBM, 1997-04-23. DOI: 10.1147/rd.521.0199','2024-02-03 22:08:18.861650','2024-02-03 22:08:18.861650'),(34,'[9] Malik Ghallab, Dana Nau, Paolo Traverso. \"Temporal Planning\"  Elsevier, 2004-05. DOI: 10.1016/b978-155860856-6/50021-1','2024-02-03 22:08:18.868171','2024-02-03 22:08:18.868171'),(35,'[10] . \"Case-based reasoning: Experiences, lessons, & future directions\" Computers & Mathematics with Applications 33(4):128 Elsevier BV, 1997-02. DOI: 10.1016/s0898-1221(97)90003-1','2024-02-03 22:08:18.873604','2024-02-03 22:08:18.873604'),(36,'[11] . \"Geothermal reservoir insurance study. Final report\"  Office of Scientific and Technical Information (OSTI), 1998. DOI: 10.2172/5244901','2024-02-03 22:08:18.879318','2024-02-03 22:08:18.879318'),(37,'[12] A Plaza, E Aamodt. \"Case-based reasoning: Foundational issues, methodological\" AI Communications 7(None):None .','2024-02-03 22:08:18.885356','2024-02-03 22:08:18.885356'),(38,'[13] . \"Preface\"  Cambridge University Press, 2008-04-23. DOI: 10.1017/cbo9780511735202.001','2024-02-03 22:08:18.891094','2024-02-03 22:08:18.891094'),(39,'[14] Belén Díaz-Agudo, Pedro A González-Calero. \"An Architecture for Knowledge Intensive CBR Systems\"  Springer Berlin Heidelberg, 2000. DOI: 10.1007/3-540-44527-7_5','2024-02-03 22:08:18.898312','2024-02-03 22:08:18.898312'),(40,'[15] Okhtay Ilghami, Dana S Nau. \"A General Approach to Synthesize Problem-Specific Planners\"  Defense Technical Information Center, 2003-10-27. DOI: 10.21236/ada455023','2024-02-03 22:08:18.903510','2024-02-03 22:08:18.903510'),(41,'[1] T Tamai. \"Foundations of Software Engineering\"  Iwanami Shoten, 2004.','2024-02-03 22:08:22.019582','2024-02-03 22:08:22.019582'),(42,'[1] Rayan Salah, Hag Ali, Neamat El Gayar. \"Sentiment analysis using unlabeled email data\"  IEEE, 2019.','2024-02-03 22:08:24.617956','2024-02-03 22:08:24.617956'),(43,'[2] Ali Shafigh, Aski, Navid Khalilzadeh, Sourati. \"Proposed efficient algorithm to filter spam using machine learning techniques\" Pacific Science Review A: Natural Science and Engineering 18(2):145-149 .','2024-02-03 22:08:24.624496','2024-02-03 22:08:24.624496'),(44,'[3] T Huwaida, Esraa A Elshoush, Dinar. \"Using adaboost and stochastic gradient descent (sgd) algorithms with R and orange software for filtering e-mail spam\"  IEEE, 2019.','2024-02-03 22:08:24.631555','2024-02-03 22:08:24.631555'),(45,'[4] Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, Qing Yang. \"A support vector machine based naive Bayes algorithm for spam filtering\"  IEEE, 2016-12. DOI: 10.1109/pccc.2016.7820655','2024-02-03 22:08:24.635933','2024-02-03 22:08:24.635933'),(46,'[5] Pranjul Garg, Nancy Girdhar. \"A Systematic Review on Spam Filtering Techniques based on Natural Language Processing Framework\"  IEEE, 2021-01-28. DOI: 10.1109/confluence51648.2021.9377042','2024-02-03 22:08:24.641440','2024-02-03 22:08:24.641440'),(47,'[6] Adam Kavon, Ghazi-Tehrani, Henry N Pontell. \"Phishing evolves: Analyzing the enduring cybercrime\" Victims & Offenders 16(3):316-342 .','2024-02-03 22:08:24.647601','2024-02-03 22:08:24.647601'),(48,'[7] Radicati Group. \"Running an O.R./analytics group\"  Institute for Operations Research and the Management Sciences (INFORMS), 2015-08-13. DOI: 10.1287/orms.2015.04.08','2024-02-03 22:08:24.653767','2024-02-03 22:08:24.653767'),(49,'[8] Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali Khan, Zunera Jalil. \"SeFACED: Semantic-Based Forensic Analysis and Classification of E-Mail Data Using Deep Learning\" IEEE Access 9(None):98398-98411 Institute of Electrical and Electronics Engineers (IEEE), 2021. DOI: 10.1109/access.2021.3095730','2024-02-03 22:08:24.659908','2024-02-03 22:08:24.659908'),(50,'[9] Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali Khan, Zunera Jalil. \"SeFACED: Semantic-Based Forensic Analysis and Classification of E-Mail Data Using Deep Learning\" IEEE Access 9(None):98398-98411 Institute of Electrical and Electronics Engineers (IEEE), 2021. DOI: 10.1109/access.2021.3095730','2024-02-03 22:08:24.666054','2024-02-03 22:08:24.666054'),(51,'[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, Yuan Zhang. \"Short-Term Residential Load Forecasting Based on LSTM Recurrent Neural Network\" IEEE Transactions on Smart Grid 10(1):841-851 Institute of Electrical and Electronics Engineers (IEEE), 2017. DOI: 10.1109/tsg.2017.2753802','2024-02-03 22:08:24.671909','2024-02-03 22:08:24.671909'),(52,'[11] T Kumaresan, C Palanisamy. \"E-mail spam classification using S-cuckoo search and support vector machine\" International Journal of Bio-Inspired Computation 9(3):142-156 .','2024-02-03 22:08:24.678640','2024-02-03 22:08:24.678640'),(53,'[12] Mehdi E Nuha H Marza, Hussein A Manaa, Lafta. \"Classification of spam emails using deep learning\"  IEEE, 2021.','2024-02-03 22:08:24.684271','2024-02-03 22:08:24.684271'),(54,'[13] Tomas Mikolov, Geoffrey Zweig. \"Context dependent recurrent neural network language model\"  IEEE, 2012-12. DOI: 10.1109/slt.2012.6424228','2024-02-03 22:08:24.689545','2024-02-03 22:08:24.689545'),(55,'[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, Dong Duong Nguyen. \"Detection of fraudulent emails by employing advanced feature abundance\" Egyptian Informatics Journal 15(3):169-174 Elsevier BV, 2014-11. DOI: 10.1016/j.eij.2014.07.002','2024-02-03 22:08:24.695039','2024-02-03 22:08:24.695039'),(56,'[15] I Priya, Thippa Reddy Sumaiya Thaseen, Mohamed K Gadekallu, Emad Aboudaif, Nasr Abouel. \"Robust attack detection approach for IIoT using ensemble classifier\"  .','2024-02-03 22:08:24.700712','2024-02-03 22:08:24.700712'),(57,'[16] Justinas Rastenis, Simona Ramanauskaitė, Justinas Janulevičius, Antanas Čenys, Asta Slotkienė, Kęstutis Pakrijauskas. \"E-mail-Based Phishing Attack Taxonomy\" Applied Sciences 10(7):2363 MDPI AG, 2020-03-30. DOI: 10.3390/app10072363','2024-02-03 22:08:24.705793','2024-02-03 22:08:24.705793'),(58,'[17] D Karthika, P Renuka, Visalakshi. \"Latent semantic indexing based SVM model for email spam classification\"  .','2024-02-03 22:08:24.711582','2024-02-03 22:08:24.711582'),(59,'[18] Shuvendu Roy, Sk. Imran Hossain, M A H Akhand, N Siddique. \"Sequence Modeling for Intelligent Typing Assistant with Bangla and English Keyboard\"  IEEE, 2018-12. DOI: 10.1109/ciet.2018.8660871','2024-02-03 22:08:24.718712','2024-02-03 22:08:24.718712'),(60,'[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, Hasim Sak. \"Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks\"  IEEE, 2015-04. DOI: 10.1109/icassp.2015.7178838','2024-02-03 22:08:24.723308','2024-02-03 22:08:24.723308'),(61,'[20] Anuj Kumar Singh, Shashi Bhushan, Sonakshi Vij. \"Filtering spam messages and mails using fuzzy C means algorithm\"  IEEE, 2019.','2024-02-03 22:08:24.729961','2024-02-03 22:08:24.729961'),(62,'[21] Kristina Toutanova, Colin Cherry. \"A global model for joint lemmatization and part-of-speech prediction\"  Association for Computational Linguistics, 2009. DOI: 10.3115/1687878.1687947','2024-02-03 22:08:24.734964','2024-02-03 22:08:24.734964'),(63,'[22] Tian Xia. \"A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\" IEEE Access 8(None):82653-82661 Institute of Electrical and Electronics Engineers (IEEE), 2020. DOI: 10.1109/access.2020.2991328','2024-02-03 22:08:24.740076','2024-02-03 22:08:24.740076'),(64,'[23] Yan Zhang, Pengfei Liu, Jingtao Yao. \"Three-way Email Spam Filtering with Game-theoretic Rough Sets\"  IEEE, 2019-04-15. DOI: 10.1109/iccnc.2019.8685642','2024-02-03 22:08:24.745526','2024-02-03 22:08:24.745526'),(65,'[1] Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando G S L Brandao, David A Buell, Brian Burkett, Yu Chen, Zijun Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew Dunsworth, Edward Farhi, Brooks Foxen, Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve Habegger, Matthew P Harrigan, Michael J Hartmann, Alan Ho, Markus Hoffmann, Trent Huang, Travis S Humble, Sergei V Isakov, Evan Jeffrey, Zhang Jiang, Dvir Kafri, Kostyantyn Kechedzhi, Julian Kelly, Paul V Klimov, Sergey Knysh, Alexander Korotkov, Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero, Dmitry Lyakh, Salvatore Mandrà, Jarrod R Mcclean, Matthew Mcewen, Anthony Megrant, Xiao Mi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley, Charles Neill, Murphy Yuezhen Niu, Eric Ostby, Andre Petukhov, John C Platt, Chris Quintana, Eleanor G Rieffel, Pedram Roushan, Nicholas C Rubin, Daniel Sank, Kevin J Satzinger, Vadim Smelyanskiy, Kevin J Sung, Matthew D Trevithick, Amit Vainsencher, Benjamin Villalonga, Theodore White, Z Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut Neven, John M Martinis. \"Quantum supremacy using a programmable superconducting processor\" Nature 574(7779):505-510 Springer Science and Business Media LLC, 2019-10-23. DOI: 10.1038/s41586-019-1666-5','2024-02-03 22:08:27.538646','2024-02-03 22:08:27.538646'),(66,'[2] H Charles, Gilles Bennett, Brassard. \"Quantum cryptography: public key distribution and coin tossing\" Theor. Comput. Sci 560(None):7-11 .','2024-02-03 22:08:27.551127','2024-02-03 22:08:27.551127'),(67,'[3] Grady Booch, James Rumbaugh, Ivar Jacobson. \"Unified Modeling Language User Guide, The\"  Addison-Wesley Professional, 2005.','2024-02-03 22:08:27.560090','2024-02-03 22:08:27.560090'),(68,'[4] C Canevet, S Gilmore, J Hillston, M Prowse, P Stevens. \"Performance modelling with the unified modelling language and stochastic process algebras\" IEE Proceedings - Computers and Digital Techniques 150(2):107 Institution of Engineering and Technology (IET), 2003-03. DOI: 10.1049/ip-cdt:20030084','2024-02-03 22:08:27.572668','2024-02-03 22:08:27.572668'),(69,'[5] K Lov, Grover. \"A Fast Quantum Mechanical Algorithm for Database Search\"  ACM, 1996. DOI: 10.1145/237814.237866','2024-02-03 22:08:27.580797','2024-02-03 22:08:27.580797'),(70,'[6] Carlos A Pérez-Delgado, Donny Cheung. \"Local unitary quantum cellular automata\" Physical Review A 76(3):32320 American Physical Society (APS), 2007-09-20. DOI: 10.1103/physreva.76.032320','2024-02-03 22:08:27.588913','2024-02-03 22:08:27.588913'),(71,'[7] W Peter, Shor. \"Algorithms for quantum computation: Discrete logarithms and factoring\"  Ieee, 1994.','2024-02-03 22:08:27.594527','2024-02-03 22:08:27.594527'),(72,'[8] Liming Zhao, Carlos A Pérez-Delgado, Joseph F Fitzsimons. \"Fast graph operations in quantum computation\" Physical Review A 93(3):32314 American Physical Society (APS), 2016-03-10. DOI: 10.1103/physreva.93.032314','2024-02-03 22:08:27.603246','2024-02-03 22:08:27.603246'),(73,'[1] Rabe Abdalkareem, Emad Shihab, Juergen Rilling. \"What Do Developers Use the Crowd For? A Study Using Stack Overflow\" IEEE Software 34(2):53-60 Institute of Electrical and Electronics Engineers (IEEE), 2017-03. DOI: 10.1109/ms.2017.31','2024-02-03 22:25:44.986968','2024-02-03 22:25:44.986968'),(74,'[2] Eleni Adamopoulou, Lefteris Moussiades. \"Chatbots: History, technology, and applications\" Machine Learning with Applications 2(None):100006 Elsevier BV, 2020-12. DOI: 10.1016/j.mlwa.2020.100006','2024-02-03 22:25:44.992985','2024-02-03 22:25:44.992985'),(75,'[3] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V Le. \"Towards a Human-like Open-Domain Chatbot\"  .','2024-02-03 22:25:44.998507','2024-02-03 22:25:44.998507'),(76,'[4] Safinah Ali, Nisha Elizabeth Devasia, Cynthia Breazeal. \"Escape!Bot: Social Robots as Creative Problem-Solving Partners\"  ACM, 2022-06-20. DOI: 10.1145/3527927.3532793','2024-02-03 22:25:45.002644','2024-02-03 22:25:45.002644'),(77,'[5] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, Charles Sutton. \"A Survey of Machine Learning for Big Code and Naturalness\" ACM Computing Surveys 51(4):1-37 Association for Computing Machinery (ACM), 2018-07-31. DOI: 10.1145/3212695','2024-02-03 22:25:45.007120','2024-02-03 22:25:45.007120'),(78,'[6] Irene Alvarado, Idan Gazit, Amelia Wattenberger. \"GitHub Next | GitHub Copilot Labs\"  .','2024-02-03 22:25:45.011740','2024-02-03 22:25:45.011740'),(79,'[7] Hikari Ando, Rosanna Cousins, Carolyn Young. \"Achieving Saturation in Thematic Analysis: Development and Refinement of a Codebook\" Comprehensive Psychology 3(None):03.CP.3.4 Ammons Scientific, 2014-01. DOI: 10.2466/03.cp.3.4','2024-02-03 22:25:45.017363','2024-02-03 22:25:45.017363'),(80,'[8] Craig Anslow, Stuart Marshall, James Noble, Robert Biddle. \"SourceVis: Collaborative software visualization for co-located environments\"  IEEE, 2013-09. DOI: 10.1109/vissoft.2013.6650527','2024-02-03 22:25:45.021870','2024-02-03 22:25:45.021870'),(81,'[9] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Narendra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, Qian Pan, Christine T Wolf, Evelyn Duesterwald, Casey Dugan, Werner Geyer, Darrell Reimer. \"AI-Assisted Human Labeling\" Proceedings of the ACM on Human-Computer Interaction 5(CSCW1):1-27 Association for Computing Machinery (ACM), 2021-04-13. DOI: 10.1145/3449163','2024-02-03 22:25:45.029487','2024-02-03 22:25:45.029487'),(82,'[10] Catherine A Ashworth. \"GUI users have trouble using graphic conventions on novel tasks\"  ACM Press, 1996. DOI: 10.1145/257089.257151','2024-02-03 22:25:45.035378','2024-02-03 22:25:45.035378'),(83,'[11] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma. \"A general language assistant as a laboratory for alignment\"  .','2024-02-03 22:25:45.043916','2024-02-03 22:25:45.043916'),(84,'[12] Leif Azzopardi, Paul Thomas, Nick Craswell. \"Measuring the Utility of Search Engine Result Pages\"  ACM, 2018-06-27. DOI: 10.1145/3209978.3210027','2024-02-03 22:25:45.049239','2024-02-03 22:25:45.049239'),(85,'[13] Shraddha Barke, Michael B James, Nadia Polikarpova. \"Grounded Copilot: How Programmers Interact with Code-Generating Models\" Proceedings of the ACM on Programming Languages 7(OOPSLA1):85-111 Association for Computing Machinery (ACM), 2022. DOI: 10.1145/3586030','2024-02-03 22:25:45.052650','2024-02-03 22:25:45.052650'),(86,'[14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill. \"On the opportunities and risks of foundation models\"  .','2024-02-03 22:25:45.058079','2024-02-03 22:25:45.058079'),(87,'[15] Joel Brandt, Mira Dontcheva, Marcos Weskamp, Scott R Klemmer. \"Example-centric programming\"  ACM, 2010-04-10. DOI: 10.1145/1753326.1753402','2024-02-03 22:25:45.063960','2024-02-03 22:25:45.063960'),(88,'[16] Virginia Braun, Victoria Clarke, Nikki Hayfield. \"‘A starting point for your journey, not a map’: Nikki Hayfield in conversation with Virginia Braun and Victoria Clarke about thematic analysis\" Qualitative Research in Psychology 19(2):424-445 Informa UK Limited, 2022-08-11. DOI: 10.1080/14780887.2019.1670765','2024-02-03 22:25:45.068703','2024-02-03 22:25:45.068703'),(89,'[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei. \"Language Models are Few-Shot Learners\"  .','2024-02-03 22:25:45.074606','2024-02-03 22:25:45.074606'),(90,'[18] Sallyann Bryant, Pablo Romero, Benedict Du Boulay. \"The Collaborative Nature of Pair Programming\"  Springer Berlin Heidelberg, 2006. DOI: 10.1007/11774129_6','2024-02-03 22:25:45.080152','2024-02-03 22:25:45.080152'),(91,'[19] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, Abdullah Almaatouq, Thomas W Malone. \"A Test for Evaluating Performance in Human-Computer Systems\"  .','2024-02-03 22:25:45.084984','2024-02-03 22:25:45.084984'),(92,'[20] Gaetano Cascini, Yukari Nagai, G V Georgiev, J Zelaya, N Becattini, J F Boujut, H Casakin, N Crilly, E Dekoninck, J Gero, A Goel, G Goldschmidt, M Gonçalves, K Grace, L Hay, P Le Masson, M L Maher, D Marjanović, D Motte, P Papalambros, R Sosa, M Štorga, B Tversky, B Yannou, A Wodehouse. \"Perspectives on design creativity and innovation research: 10 years later\" International Journal of Design Creativity and Innovation 10(1):1-30 Informa UK Limited, 2022-01-02. DOI: 10.1080/21650349.2022.2021480','2024-02-03 22:25:45.090995','2024-02-03 22:25:45.090995'),(93,'[21] Stephen Cass. \"The top programming languages: Our latest rankings put Python on top-again - [Careers]\" IEEE Spectrum 57(8):22-22 Institute of Electrical and Electronics Engineers (IEEE), 2022-08-23. DOI: 10.1109/mspec.2020.9150550','2024-02-03 22:25:45.096153','2024-02-03 22:25:45.096153'),(94,'[22] Cristina Catalan Aguirre, Nuria Gonzalez Castro, Carlos Delgado Kloos, Carlos Alario-Hoyos, Pedro José, Muñoz Merino. \"Conversational agent for supporting learners on a MOOC on programming with Java\"  .','2024-02-03 22:25:45.100950','2024-02-03 22:25:45.100950'),(95,'[23] Ana, Paula Chaves, Marco Aurelio, Gerosa. \"How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design\" International Journal of Human-Computer Interaction 37(8):729-758 .','2024-02-03 22:25:45.105304','2024-02-03 22:25:45.105304'),(96,'[24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H Guss, Alex Nichol, Igor Babuschkin, S Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mcgrew, Dario Amodei, Sam Mccandlish, Ilya Sutskever, Wojciech Zaremba. \"Evaluating a Large Language Models Trained on Code\"  .','2024-02-03 22:25:45.110311','2024-02-03 22:25:45.110311'),(97,'[25] Li-Te Cheng, Cleidson R B De Souza, Susanne Hupfer, John Patterson, Steven Ross. \"Building Collaboration into IDEs\" Queue 1(9):40-50 Association for Computing Machinery (ACM), 2003-12. DOI: 10.1145/966789.966803','2024-02-03 22:25:45.115177','2024-02-03 22:25:45.115177'),(98,'[26] Carl Cook, Warwick Irwin, Neville Churcher. \"A user evaluation of synchronous collaborative software engineering tools\"  IEEE, 2005. DOI: 10.1109/apsec.2005.22','2024-02-03 22:25:45.120176','2024-02-03 22:25:45.120176'),(99,'[27] Claudio León De La Barra, Broderick Crawford, Ricardo Soto, Sanjay Misra, Eric Monfroy. \"Agile Software Development: It Is about Knowledge Management and Creativity\"  Springer Berlin Heidelberg, 2013. DOI: 10.1007/978-3-642-39646-5_8','2024-02-03 22:25:45.125962','2024-02-03 22:25:45.125962'),(100,'[28] Uri Dekel, Steven Ross. \"Eclipse as a platform for research on interruption management in software development\"  ACM Press, 2004. DOI: 10.1145/1066129.1066132','2024-02-03 22:25:45.130789','2024-02-03 22:25:45.130789'),(101,'[29] Bobbie Eicher, Kathryn Cunningham, Sydni Peterson, Marissa Gonzales, Ashok Goel. \"Toward mutual theory of mind as a foundation for co-creation\"  Co-Creation Workshop, 2017.','2024-02-03 22:25:45.135399','2024-02-03 22:25:45.135399'),(102,'[30] Eduardo Stephen M Fiore, Janis A Salas, Cannon-Bowers. \"Group Dynamics and Shared Mental Model Development\"  Psychology Press, 2001. DOI: 10.4324/9781410600608-25','2024-02-03 22:25:45.140408','2024-02-03 22:25:45.140408'),(103,'[31] Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides. \"Design Patterns: Abstraction and Reuse of Object-Oriented Design\"  Springer Berlin Heidelberg, 1995. DOI: 10.1007/978-3-642-59412-0_40','2024-02-03 22:25:45.144981','2024-02-03 22:25:45.144981'),(104,'[32] Dominik Sobania, Martin Briesch, Franz Rothlauf. \"Choose your programming copilot\"  ACM, 2022-08-05. DOI: 10.1145/3512290.3528700','2024-02-03 22:25:45.149993','2024-02-03 22:25:45.149993'),(105,'[33] Amelia Glaese, Nat Mcaleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving. \"Improving alignment of dialogue agents via targeted human judgements\"  .','2024-02-03 22:25:45.153993','2024-02-03 22:25:45.155103'),(106,'[34] Stephanie Glen. \"ChatGPT writes code, but won\'t replace developers\" TechTarget None(None):None .','2024-02-03 22:25:45.159098','2024-02-03 22:25:45.159098'),(107,'[35] Samuel Holmes, Anne Moorhead, Raymond Bond, Huiru Zheng, Vivien Coates, Mike Mctear. \"WeightMentor: A New Automated Chatbot for Weight Loss Maintenance\"  BCS Learning & Development, 2018. DOI: 10.14236/ewic/hci2018.103','2024-02-03 22:25:45.164039','2024-02-03 22:25:45.164039'),(108,'[36] Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin. \"Deep code comment generation with hybrid lexical and syntactical information\" Empirical Software Engineering 25(3):2179-2217 Springer Science and Business Media LLC, 2020. DOI: 10.1007/s10664-019-09730-9','2024-02-03 22:25:45.170232','2024-02-03 22:25:45.170232'),(109,'[37] L Edwin, James D Hutchins, Donald A Hollan, Norman. \"Direct manipulation interfaces\" Human-computer interaction 1(None):311-338 .','2024-02-03 22:25:45.174840','2024-02-03 22:25:45.174840'),(110,'[38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer. \"Summarizing Source Code using a Neural Attention Model\"  Association for Computational Linguistics, 2016. DOI: 10.18653/v1/p16-1195','2024-02-03 22:25:45.179917','2024-02-03 22:25:45.179917'),(111,'[39] Andreas Jedlitschka, Markus Nick. \"Software Engineering Knowledge Repositories\"  Springer Berlin Heidelberg, 2003. DOI: 10.1007/978-3-540-45143-3_5','2024-02-03 22:25:45.184945','2024-02-03 22:25:45.184945'),(112,'[40] Eirini Kalliamvakou. \"Research: Quantifying github copilot\'s impact on developer productivity and happiness\"  .','2024-02-03 22:25:45.188943','2024-02-03 22:25:45.188943'),(113,'[41] Anna Kantosalo, Sirpa Riihiaho. \"Quantifying co-creative writing experiences\" Digital Creativity 30(1):23-38 Informa UK Limited, 2019-01-02. DOI: 10.1080/14626268.2019.1575243','2024-02-03 22:25:45.193838','2024-02-03 22:25:45.193838'),(114,'[42] Bali Sandeep Kaur Kuttal, Kate Ong, Peter Kwasny, Robe. \"Trade-Offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly\"  Association for Computing Machinery, 2021.','2024-02-03 22:25:45.198951','2024-02-03 22:25:45.198951'),(115,'[43] Lauramaria Laine. \"Exploring Advertising Creatives\' Attitudes Towards Human-AI Collaboration\"  .','2024-02-03 22:25:45.202898','2024-02-03 22:25:45.202898'),(116,'[44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D’autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando De Freitas, Koray Kavukcuoglu, Oriol Vinyals. \"Competition-level code generation with AlphaCode\" Science 378(6624):1092-1097 American Association for the Advancement of Science (AAAS), 2022-12-09. DOI: 10.1126/science.abq1158','2024-02-03 22:25:45.207409','2024-02-03 22:25:45.207409'),(117,'[45] Yaosheng Lou, Qi Sun. \"Over-reliance on database: A case study of using web of science\" Human Behavior and Emerging Technologies 3(3):454-459 .','2024-02-03 22:25:45.214002','2024-02-03 22:25:45.214002'),(118,'[46] David Lyell, Enrico Coiera. \"Automation bias and verification complexity: a systematic review\" Journal of the American Medical Informatics Association 24(2):423-431 Oxford University Press (OUP), 2017. DOI: 10.1093/jamia/ocw105','2024-02-03 22:25:45.219559','2024-02-03 22:25:45.219559'),(119,'[47] Wendy E Mackay, Anne-Laure Fayard. \"HCI, natural science and design\"  ACM Press, 1997. DOI: 10.1145/263552.263612','2024-02-03 22:25:45.224281','2024-02-03 22:25:45.224281'),(120,'[48] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, Janis A Cannon-Bowers. \"The influence of shared mental models on team process and performance.\" Journal of Applied Psychology 85(2):273-283 American Psychological Association (APA), 2000. DOI: 10.1037//0021-9010.85.2.273','2024-02-03 22:25:45.228292','2024-02-03 22:25:45.228292'),(121,'[49] Cade Metz. \"Meet GPT-3. It Has Learned to Code (and Blog and Argue)\"  .','2024-02-03 22:25:45.234291','2024-02-03 22:25:45.234291'),(122,'[50] Robert J Moore, Raphael Arar. \"Conversational UX design process\"  Association for Computing Machinery, 2019-05-01. DOI: 10.1145/3304087.3304096','2024-02-03 22:25:45.238022','2024-02-03 22:25:45.238022'),(123,'[51] Ekaterina A Moroz, Vladimir O Grizkevich, Igor M Novozhilov. \"The Potential of Artificial Intelligence as a Method of Software Developer\'s Productivity Improvement\"  IEEE, 2022-01-25. DOI: 10.1109/elconrus54750.2022.9755659','2024-02-03 22:25:45.244106','2024-02-03 22:25:45.244106'),(124,'[52] Michael Muller, Stevean Ross, Stephanie Houde, Mayank Agarwal, Fernando Martinez, John Richards, Kartik Talamadupula, Justin D Weisz. \"Drinking Chai with Your (AI) Programming Partner: A Design Fiction about Generative AI for Software Engineering\"  .','2024-02-03 22:25:45.248444','2024-02-03 22:25:45.248444'),(125,'[53] R Sandra, Alfredo Murillo, Sánchez. \"Empowering interfaces for system administrators: Keeping the command line in mind when designing GUIs\"  .','2024-02-03 22:25:45.252446','2024-02-03 22:25:45.252446'),(126,'[54] D Elizabeth, Gerhard Mynatt, Weber. \"Nonvisual presentation of graphical user interfaces: contrasting two approaches\"  .','2024-02-03 22:25:45.257657','2024-02-03 22:25:45.257657'),(127,'[55] Alok Mysore, Philip J Guo. \"Torta\"  ACM, 2017-10-20. DOI: 10.1145/3126594.3126628','2024-02-03 22:25:45.262754','2024-02-03 22:25:45.262754'),(128,'[56] Clifford Nass, Youngme Moon. \"Machines and Mindlessness: Social Responses to Computers\" Journal of Social Issues 56(1):81-103 Wiley, 2000-01. DOI: 10.1111/0022-4537.00153','2024-02-03 22:25:45.269287','2024-02-03 22:25:45.269287'),(129,'[57] Nhan Nguyen, Sarah Nadi. \"An empirical evaluation of GitHub copilot\'s code suggestions\"  ACM, 2022-05-23. DOI: 10.1145/3524842.3528470','2024-02-03 22:25:45.273287','2024-02-03 22:25:45.273287'),(130,'[58] Martin Nordio, H Estler, Carlo A Furia, Bertrand Meyer. \"Collaborative software development on the web\"  .','2024-02-03 22:25:45.279055','2024-02-03 22:25:45.279055'),(131,'[59] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Witold Henryk, Jacob Michalewski, David Austin, David Bieber, Aitor Martin Dohan, Maarten Lewkowycz, David Paul Bosma, Charles Luan, Augustus Sutton, Odena. \"Show Your Work: Scratchpads for Intermediate Computation with Language Models\"  .','2024-02-03 22:25:45.285276','2024-02-03 22:25:45.285276'),(132,'[60] Openai. \"ChatGPT: Optimizing Language Models for Dialogue\" OpenAI Blog None(None):None .','2024-02-03 22:25:45.290171','2024-02-03 22:25:45.290171'),(133,'[61] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. \"Training language models to follow instructions with human feedback\"  .','2024-02-03 22:25:45.294457','2024-02-03 22:25:45.294457'),(134,'[62] Peter Pirolli, Stuart Card. \"Information foraging.\" Psychological Review 106(4):643-675 American Psychological Association (APA), 1999. DOI: 10.1037//0033-295x.106.4.643','2024-02-03 22:25:45.299394','2024-02-03 22:25:45.299394'),(135,'[63] Larry Press. \"Personal computing: Windows, DOS and the MAC\" Communications of the ACM 33(11):19-26 Association for Computing Machinery (ACM), 1990-11. DOI: 10.1145/92755.92783','2024-02-03 22:25:45.304227','2024-02-03 22:25:45.304227'),(136,'[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. \"Language Models are Unsupervised Multitask Learners\"  .','2024-02-03 22:25:45.310520','2024-02-03 22:25:45.310520'),(137,'[65] Alvin Rajkomar, Jeffrey Dean, Isaac Kohane. \"Machine Learning in Medicine\" New England Journal of Medicine 380(14):1347-1358 Massachusetts Medical Society, 2019-04-04. DOI: 10.1056/nejmra1814259','2024-02-03 22:25:45.315727','2024-02-03 22:25:45.315727'),(138,'[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. \"Hierarchical text-conditional image generation with clip latents\"  .','2024-02-03 22:25:45.320698','2024-02-03 22:25:45.320698'),(139,'[67] B Reeves, C I Nass. \"The media equation: How people treat computers, television, & new media like real people & places\" Computers & Mathematics with Applications 33(5):128 Elsevier BV, 1996. DOI: 10.1016/s0898-1221(97)82929-x','2024-02-03 22:25:45.326304','2024-02-03 22:25:45.326304'),(140,'[68] Mawarny Md Rejab, James Noble, George Allan. \"Distributing Expertise in Agile Software Development Projects\"  .','2024-02-03 22:25:45.331305','2024-02-03 22:25:45.331305'),(141,'[69] Jeba Rezwana, Mary Lou Maher. \"Designing Creative AI Partners with COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems\" ACM Transactions on Computer-Human Interaction 30(5):1-28 Association for Computing Machinery (ACM), 2021. DOI: 10.1145/3519026','2024-02-03 22:25:45.335907','2024-02-03 22:25:45.335907'),(142,'[70] Charles H Rich, Richard C Waters. \"The Programmer\'s Apprentice\"  Addison-Wesley Publishing Company, 1990.','2024-02-03 22:25:45.340973','2024-02-03 22:25:45.340973'),(143,'[71] Peter Robe, Sandeep Kaur Kuttal. \"Designing PairBuddy—A Conversational Agent for Pair Programming\" ACM Transactions on Computer-Human Interaction 29(4):1-44 Association for Computing Machinery (ACM), 2022-05-05. DOI: 10.1145/3498326','2024-02-03 22:25:45.344007','2024-02-03 22:25:45.344007'),(144,'[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjorn Ommer. \"High-Resolution Image Synthesis with Latent Diffusion Models\"  IEEE, 2022-06. DOI: 10.1109/cvpr52688.2022.01042','2024-02-03 22:25:45.349094','2024-02-03 22:25:45.349094'),(145,'[73] Steven Ross, Elizabeth Brownholtz, Robert Armes. \"A multiple-application conversational agent\"  ACM, 2004-01-13. DOI: 10.1145/964442.964518','2024-02-03 22:25:45.354098','2024-02-03 22:25:45.354098'),(146,'[74] Steven Ross, Elizabeth Brownholtz, Robert Armes. \"Voice user interface principles for a conversational agent\"  ACM, 2004-01-13. DOI: 10.1145/964442.964536','2024-02-03 22:25:45.359273','2024-02-03 22:25:45.359769'),(147,'[75] Marie-Anne Baptiste Roziere, Lowik Lachaux, Guillaume Chanussot, Lample. \"Unsupervised Translation of Programming Languages\"  Curran Associates, Inc, 2020.','2024-02-03 22:25:45.363960','2024-02-03 22:25:45.363960'),(148,'[76] Harvey Sacks. \"Notes on methodology\"  Cambridge University Press, 1984.','2024-02-03 22:25:45.368446','2024-02-03 22:25:45.368446'),(149,'[77] Nithya Sambasivan, Rajesh Veeraraghavan. \"The Deskilling of Domain Expertise in AI Development\"  ACM, 2022-04-29. DOI: 10.1145/3491102.3517578','2024-02-03 22:25:45.372442','2024-02-03 22:25:45.372442'),(150,'[78] Harini Sampath, Alice Merrick, Andrew Macvean. \"Accessibility of Command Line Interfaces\"  ACM, 2021-05-06. DOI: 10.1145/3411764.3445544','2024-02-03 22:25:45.377315','2024-02-03 22:25:45.377315'),(151,'[79] Matthias Scheutz, Scott A Deloach, Julie A Adams. \"A Framework for Developing and Using Shared Mental Models in Human-Agent Teams\" Journal of Cognitive Engineering and Decision Making 11(3):203-224 SAGE Publications, 2017-01-17. DOI: 10.1177/1555343416682891','2024-02-03 22:25:45.382011','2024-02-03 22:25:45.382011'),(152,'[80] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-Jan De Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah Oeste-Reiß, Nils Randrup, Gerhard Schwabe, Matthias Söllner. \"Machines as teammates: A research agenda on AI in team collaboration\" Information & Management 57(2):103174 Elsevier BV, 2020-03. DOI: 10.1016/j.im.2019.103174','2024-02-03 22:25:45.386867','2024-02-03 22:25:45.386867'),(153,'[81] Shilad Sen, Werner Geyer, Michael Muller, Marty Moore, Beth Brownholtz, Eric Wilcox, David R Millen. \"FeedMe\"  ACM, 2006-11-04. DOI: 10.1145/1180875.1180890','2024-02-03 22:25:45.392203','2024-02-03 22:25:45.392203'),(154,'[82] Ben Shneiderman. \"Human-Centered Artificial Intelligence: Three Fresh Ideas\" AIS Transactions on Human-Computer Interaction 12(3):109-124 Association for Information Systems, 2020. DOI: 10.17705/1thci.00131','2024-02-03 22:25:45.397302','2024-02-03 22:25:45.397302'),(155,'[83] Ben Shneiderman. \"Human-Centered AI\"  Oxford University Press, 2022.','2024-02-03 22:25:45.402404','2024-02-03 22:25:45.402404'),(156,'[84] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane. \"BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage\"  .','2024-02-03 22:25:45.406928','2024-02-03 22:25:45.406928'),(157,'[85] Michael Skirpan, Casey Fiesler. \"Ad Empathy\"  ACM, 2018-01-07. DOI: 10.1145/3148330.3149407','2024-02-03 22:25:45.411336','2024-02-03 22:25:45.411336'),(158,'[86] Diomidis Spinellis. \"Git\" IEEE Software 29(3):100-101 Institute of Electrical and Electronics Engineers (IEEE), 2012-05. DOI: 10.1109/ms.2012.61','2024-02-03 22:25:45.417183','2024-02-03 22:25:45.417183'),(159,'[87] Angie Spoto, Natalia Oleynik. \"Library of Mixed-Initiative Creative Interfaces\"  .','2024-02-03 22:25:45.421617','2024-02-03 22:25:45.421617'),(160,'[88] Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, Pushpendra Singh. \"Actionable UI Design Guidelines for Smartphone Applications Inclusive of Low-Literate Users\" Proceedings of the ACM on Human-Computer Interaction 5(CSCW1):1-30 Association for Computing Machinery (ACM), 2021-04-13. DOI: 10.1145/3449210','2024-02-03 22:25:45.426231','2024-02-03 22:25:45.426231'),(161,'[89] Margaret-Anne Storey, Alexey Zagalsky. \"Disrupting developer productivity one bot at a time\"  ACM, 2016-11. DOI: 10.1145/2950290.2983989','2024-02-03 22:25:45.430567','2024-02-03 22:25:45.430567'),(162,'[90] Kartik Talamadupula. \"Applied AI matters: AI4Code\" AI Matters 7(1):18-20 Association for Computing Machinery (ACM), 2021-03. DOI: 10.1145/3465074.3465080','2024-02-03 22:25:45.436088','2024-02-03 22:25:45.436088'),(163,'[91] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du. \"LAMDA: Language models for dialog applications\"  .','2024-02-03 22:25:45.441552','2024-02-03 22:25:45.442097'),(164,'[92] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Sundaresan. \"Generating accurate assert statements for unit test cases using pretrained transformers\"  ACM, 2020. DOI: 10.1145/3524481.3527220','2024-02-03 22:25:45.447657','2024-02-03 22:25:45.447657'),(165,'[93] Severi Uusitalo, Anna Kantosalo, Antti Salovaara, Tapio Takala, Christian Guckelsberger. \"Co-creative Product Design with Interactive Evolutionary Algorithms: A Practice-Based Reflection\"  Springer International Publishing, 2022. DOI: 10.1007/978-3-031-03789-4_19','2024-02-03 22:25:45.452882','2024-02-03 22:25:45.452882'),(166,'[94] Priyan Vaithilingam, Philip J Guo. \"Bespoke: Interactively Synthesizing Custom GUIs from Command-Line Applications By Demonstration\"  ACM, 2019-10-17. DOI: 10.1145/3332165.3347944','2024-02-03 22:25:45.457149','2024-02-03 22:25:45.457149'),(167,'[95] Priyan Vaithilingam, Tianyi Zhang, Elena L Glassman. \"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models\"  ACM, 2022-04-27. DOI: 10.1145/3491101.3519665','2024-02-03 22:25:45.462563','2024-02-03 22:25:45.462563'),(168,'[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Ukasz Kaiser, Illia Polosukhin. \"Attention is All you Need\"  .','2024-02-03 22:25:45.468099','2024-02-03 22:25:45.468099'),(169,'[97] U Guyon, S Von Luxburg, H Bengio, R Wallach, Fergus.   Curran Associates, Inc, None.','2024-02-03 22:25:45.473117','2024-02-03 22:25:45.473117'),(170,'[98] Zhou Yao Wan, Min Zhao, Guandong Yang, Haochao Xu, Jian Ying, Philip S Wu, Yu. \"Improving automatic source code summarization via deep reinforcement learning\"  .','2024-02-03 22:25:45.477438','2024-02-03 22:25:45.477438'),(171,'[99] April Yi, Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D Weisz, Xuye Liu, Lingfei Wu, Casey Dugan. \"Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks\" ACM Transactions on Computer-Human Interaction 29(2):1-33 .','2024-02-03 22:25:45.483047','2024-02-03 22:25:45.483047'),(172,'[100] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, Alexander Gray. \"Human-AI Collaboration in Data Science\" Proceedings of the ACM on Human-Computer Interaction 3(CSCW):1-24 Association for Computing Machinery (ACM), 2019-11-07. DOI: 10.1145/3359313','2024-02-03 22:25:45.487485','2024-02-03 22:25:45.487485'),(173,'[101] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, Ashok Goel. \"Towards Mutual Theory of Mind in Human-AI Interaction: How Language Reflects What Students Perceive About a Virtual Teaching Assistant\"  ACM, 2021-05-06. DOI: 10.1145/3411764.3445645','2024-02-03 22:25:45.493902','2024-02-03 22:25:45.493902'),(174,'[102] Jeremy Warner, Philip J Guo. \"CodePilot\"  ACM, 2017-05-02. DOI: 10.1145/3025453.3025876','2024-02-03 22:25:45.498666','2024-02-03 22:25:45.498666'),(175,'[103] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, Kartik Talamadupula. \"Perfection Not Required? Human-AI Partnerships in Code Translation\"  ACM, 2021-04-14. DOI: 10.1145/3397481.3450656','2024-02-03 22:25:45.503922','2024-02-03 22:25:45.503922'),(176,'[104] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, John T Richards. \"Better Together? An Evaluation of AI-Supported Code Translation\"  ACM, 2022-03-22. DOI: 10.1145/3490099.3511157','2024-02-03 22:25:45.509120','2024-02-03 22:25:45.509120'),(177,'[105] Joseph Weizenbaum. \"ELIZA—a computer program for the study of natural language communication between man and machine\" Communications of the ACM 9(1):36-45 Association for Computing Machinery (ACM), 1966-01. DOI: 10.1145/365153.365168','2024-02-03 22:25:45.515433','2024-02-03 22:25:45.515433'),(178,'[106] Bogdan Frank F Xu, Graham Vasilescu, Neubig. \"In-ide code generation from natural language: Promise and challenges\" ACM Transactions on Software Engineering and Methodology (TOSEM) 31(2):1-47 .','2024-02-03 22:25:45.520377','2024-02-03 22:25:45.520377'),(179,'[107] Aditya Ankur Yadav, Ishan Garg, Dr Pratistha Mathur. \"PACT -Programming Assistant ChaTbot\"  .','2024-02-03 22:25:45.526130','2024-02-03 22:25:45.526130'),(180,'[108] Munazza Zaib, Quan Z Sheng, Wei Emma Zhang. \"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\"  ACM, 2020-02-04. DOI: 10.1145/3373017.3373028','2024-02-03 22:25:45.530355','2024-02-03 22:25:45.530355'),(181,'[109] Elaine Zibrowski, Lisa Shepherd, Kamran Sedig, Richard Booth, Candace Gibson. \"Easier and Faster Is Not Always Better: Grounded Theory of the Impact of Large-Scale System Transformation on the Clinical Work of Emergency Medicine Nurses and Physicians\" JMIR Human Factors 5(4):e11013 JMIR Publications Inc., 2018-12-13. DOI: 10.2196/11013','2024-02-03 22:25:45.535457','2024-02-03 22:25:45.535457'),(182,'[110] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, Edward Aftandilian. \"Productivity assessment of neural code completion\"  ACM, 2022-06-13. DOI: 10.1145/3520312.3534864','2024-02-03 22:25:45.541097','2024-02-03 22:25:45.541097'),(183,'[1] Amjad Altadmri, Neil C C Brown. \"37 Million Compilations\"  ACM, 2015-02-24. DOI: 10.1145/2676723.2677258','2024-02-03 22:25:48.406645','2024-02-03 22:25:48.406645'),(184,'[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell. \"Language models are few-shot learners\" Advances in Neural Information Processing Systems 33(None):1877-1901 .','2024-02-03 22:25:48.414297','2024-02-03 22:25:48.414297'),(185,'[3] Adam Carberry, Stephen Krause, Casey Ankeny, Cynthia Waters. \"&#x201C;Unmuddying&#x201D; course content using muddiest point reflections\"  IEEE, 2013-10. DOI: 10.1109/fie.2013.6684966','2024-02-03 22:25:48.426165','2024-02-03 22:25:48.426165'),(186,'[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman. \"Evaluating large language models trained on code\"  .','2024-02-03 22:25:48.435847','2024-02-03 22:25:48.435847'),(187,'[5] Kathryn Cunningham, Yike Qiao, Alex Feng, Eleanor O\'rourke. \"Bringing \"High-level\" Down to Earth\"  ACM, 2022-02-22. DOI: 10.1145/3478431.3499370','2024-02-03 22:25:48.439750','2024-02-03 22:25:48.439750'),(188,'[6] Elvina Elvina, Oscar Karnalim. \"Complexitor: An Educational Tool for Learning Algorithm Time Complexity in Practical Manner\" ComTech: Computer, Mathematics and Engineering Applications 8(1):21 Universitas Bina Nusantara, 2017-03-31. DOI: 10.21512/comtech.v8i1.3783','2024-02-03 22:25:48.444267','2024-02-03 22:25:48.444267'),(189,'[7] James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, James Prather. \"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming\"  ACM, 2022-02-14. DOI: 10.1145/3511861.3511863','2024-02-03 22:25:48.458060','2024-02-03 22:25:48.458060'),(190,'[8] J Philip, Guo. \"Online python tutor: embeddable web-based program visualization for cs education\"  .','2024-02-03 22:25:48.464171','2024-02-03 22:25:48.464171'),(191,'[9] Andrew Head, Codanda Appachu, Marti A Hearst, Bjorn Hartmann. \"Tutorons: Generating context-relevant, on-demand explanations and demonstrations of online code\"  IEEE, 2015-10. DOI: 10.1109/vlhcc.2015.7356972','2024-02-03 22:25:48.471206','2024-02-03 22:25:48.471206'),(192,'[10] Ge Samiha Marwan, Susan Gao, Thomas W Fisk, Tiffany Price, Barnes. \"Adaptive Immediate Feedback Can Improve Novice Programming Engagement and Intention to Persist in Computer Science\"  Association for Computing Machinery, 2020. DOI: 10.1145/3372782.3406264','2024-02-03 22:25:48.474283','2024-02-03 22:25:48.474283'),(193,'[11] Davin Mccall, Michael Kolling. \"Meaningful categorisation of novice programmer errors\"  IEEE, 2014-10. DOI: 10.1109/fie.2014.7044420','2024-02-03 22:25:48.482311','2024-02-03 22:25:48.482311'),(194,'[12] Benjamin Greg L Nelson, Amy J Xie, Ko. \"Comprehension first: evaluating a novel pedagogy and tutoring system for program tracing in CS1\"  .','2024-02-03 22:25:48.486503','2024-02-03 22:25:48.486503'),(195,'[13] Miranda Parker, Colleen Lewis. \"What makes big-O analysis difficult: understanding how students understand runtime analysis\" Journal of Computing Sciences in Colleges 29(4):164-174 .','2024-02-03 22:25:48.494254','2024-02-03 22:25:48.494254'),(196,'[14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany Vinci-Cannava, Laird Kramer, Maria Charters. \"WIP: An exploration into the muddiest points and self-efficacy of students in introductory computer science courses\"  IEEE, 2020-10-21. DOI: 10.1109/fie44824.2020.9273932','2024-02-03 22:25:48.494254','2024-02-03 22:25:48.494254'),(197,'[15] Chris Piech, Mehran Sahami, Jonathan Huang, Leonidas Guibas. \"Autonomously Generating Hints by Inferring Problem Solving Policies\"  ACM, 2015-03-14. DOI: 10.1145/2724660.2724668','2024-02-03 22:25:48.505706','2024-02-03 22:25:48.505706'),(198,'[16] Yihuan Thomas W Price, Dragan Dong, Lipovac. \"iSnap: towards intelligent tutoring in novice programming environments\"  .','2024-02-03 22:25:48.515568','2024-02-03 22:25:48.515568'),(199,'[17] Carsten Schulte, Jens Bennedsen. \"What do teachers teach in introductory programming?\"  ACM, 2006-09-09. DOI: 10.1145/1151588.1151593','2024-02-03 22:25:48.520860','2024-02-03 22:25:48.520860'),(200,'[1] Justito Adiprasetio, Annissa Winda Larasati. \"Pandemic Crisis in Online Media: Quantitative Framing Analysis on detik.com’s Coverage of Covid-19\" Jurnal Ilmu Sosial dan Ilmu Politik 24(2):153 Universitas Gadjah Mada, 2020. DOI: 10.22146/jsp.56457','2024-02-03 22:25:51.653147','2024-02-03 22:25:51.653147'),(201,'[2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier. \"RAFT: A real-world few-shot text classification benchmark\"  .','2024-02-03 22:25:51.661053','2024-02-03 22:25:51.661053'),(202,'[3] David Alonso, Del Barrio, Daniel Gatica-Perez. \"How Did Europe\'s Press Cover Covid-19 Vaccination News? A Five-Country Analysis\"  . DOI: 10.1145/3512732.3533588','2024-02-03 22:25:51.667657','2024-02-03 22:25:51.667657'),(203,'[4] Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell. \"On the Dangers of Stochastic Parrots\"  ACM, 2021-03. DOI: 10.1145/3442188.3445922','2024-02-03 22:25:51.674231','2024-02-03 22:25:51.674231'),(204,'[5] Santosh Kumar, Biswal, Nikhil Kumar, Gouda. \"Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications\"  Springer, 2020.','2024-02-03 22:25:51.680263','2024-02-03 22:25:51.680263'),(205,'[6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, Rana Abdelhamid. \"Media Portrayals of Minorities: Muslims in British Newspaper Headlines, 2001–2012\" Journal of Ethnic and Migration Studies 41(6):942-962 Informa UK Limited, 2015-02-09. DOI: 10.1080/1369183x.2014.1002200','2024-02-03 22:25:51.685912','2024-02-03 22:25:51.685912'),(206,'[7] Michael Bommarito, Daniel Martin Katz. \"GPT Takes the Bar Exam\"  . DOI: 10.48550/ARXIV.2212.14402','2024-02-03 22:25:51.691744','2024-02-03 22:25:51.691744'),(207,'[8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, Ching-Hua Chuan. \"Artificial Intelligence and Journalism\" Journalism & Mass Communication Quarterly 96(3):673-695 SAGE Publications, 2019-07-31. DOI: 10.1177/1077699019859901','2024-02-03 22:25:51.697247','2024-02-03 22:25:51.697247'),(208,'[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell. \"Language models are few-shot learners\" Advances in neural information processing systems 33(None):1877-1901 .','2024-02-03 22:25:51.703439','2024-02-03 22:25:51.703439'),(209,'[10] Björn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, Claes H De Vreese. \"Teaching the Computer to Code Frames in News: Comparing Two Supervised Machine Learning Approaches to Frame Analysis\" Communication Methods and Measures 8(3):190-206 Informa UK Limited, 2014-07-03. DOI: 10.1080/19312458.2014.937527','2024-02-03 22:25:51.710127','2024-02-03 22:25:51.710127'),(210,'[11] Bjorn Burscher, Rens Vliegenthart, Claes H De Vreese. \"Frames Beyond Words\" Social Science Computer Review 34(5):530-545 SAGE Publications, 2016-08-03. DOI: 10.1177/0894439315596385','2024-02-03 22:25:51.716292','2024-02-03 22:25:51.716292'),(211,'[12] Dallas Card, Amber E Boydstun, Justin H Gross, Philip Resnik, Noah A Smith. \"The Media Frames Corpus: Annotations of Frames Across Issues\"  Association for Computational Linguistics, 2015. DOI: 10.3115/v1/p15-2072','2024-02-03 22:25:51.722499','2024-02-03 22:25:51.722499'),(212,'[13] Daniel Catalan, -Matamoros, Carlos Elías. \"Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press\" International Journal of Environmental Research and Public Health 17(None):8136 .','2024-02-03 22:25:51.728123','2024-02-03 22:25:51.728123'),(213,'[14] Daniel Catalán, -Matamoros, Carmen Peñafiel-Saiz. \"Media and mistrust of vaccines: a content analysis of press headlines\" Revista latina de comunicación social 74(None):786-802 .','2024-02-03 22:25:51.734539','2024-02-03 22:25:51.734539'),(214,'[15] Mark Coddington. \"Clarifying Journalism’s Quantitative Turn\" Digital Journalism 3(3):331-348 Informa UK Limited, 2015. DOI: 10.1080/21670811.2014.976400','2024-02-03 22:25:51.741892','2024-02-03 22:25:51.741892'),(215,'[16] D Stephen, Cooper. \"Doing News Framing Analysis\"  Routledge, 2010-02-26. DOI: 10.4324/9780203864463','2024-02-03 22:25:51.748042','2024-02-03 22:25:51.748042'),(216,'[17] Robert Dale. \"GPT-3: What’s it good for?\" Natural Language Engineering 27(1):113-118 Cambridge University Press (CUP), 2021. DOI: 10.1017/s1351324920000601','2024-02-03 22:25:51.755118','2024-02-03 22:25:51.755118'),(217,'[18] Astrid Dirikx, Dave Gelders. \"To frame is to explain: A deductive frame-analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties\" Public Understanding of Science 19(6):732-742 SAGE Publications, 2010-01-29. DOI: 10.1177/0963662509352044','2024-02-03 22:25:51.762691','2024-02-03 22:25:51.762691'),(218,'[19] Astrid Dirikx, Dave Gelders. \"To frame is to explain: A deductive frame-analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties\" Public Understanding of Science 19(6):732-742 SAGE Publications, 2010-01-29. DOI: 10.1177/0963662509352044','2024-02-03 22:25:51.767309','2024-02-03 22:25:51.767309'),(219,'[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig. \"GSum: A General Framework for Guided Neural Abstractive Summarization\"  Association for Computational Linguistics, 2020. DOI: 10.18653/v1/2021.naacl-main.384','2024-02-03 22:25:51.772315','2024-02-03 22:25:51.772315'),(220,'[21] Sumayya Ebrahim. \"The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa\" Health SA Gesondheid 27(None):1-8 AOSIS, 2022-02-21. DOI: 10.4102/hsag.v27i0.1683','2024-02-03 22:25:51.777434','2024-02-03 22:25:51.777434'),(221,'[22] Hend Abdelgaber, Ahmed El-Behary. \"A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt\"  .','2024-02-03 22:25:51.782130','2024-02-03 22:25:51.782130'),(222,'[23] Robert M Entman. \"Framing: Towards clarification of a fractured paradigm\" McQuail\'s reader in mass communication theory 390(None):397 .','2024-02-03 22:25:51.787342','2024-02-03 22:25:51.787342'),(223,'[24] Tianyu Gao, Adam Fisch, Danqi Chen. \"Making Pre-trained Language Models Better Few-shot Learners\"  Association for Computational Linguistics, 2020. DOI: 10.18653/v1/2021.acl-long.295','2024-02-03 22:25:51.792659','2024-02-03 22:25:51.792659'),(224,'[25] Piyush Ghasiya, Koji Okamura. \"Investigating COVID-19 News Across Four Nations: A Topic Modeling and Sentiment Analysis Approach\" IEEE Access 9(None):36645-36656 Institute of Electrical and Electronics Engineers (IEEE), 2021. DOI: 10.1109/access.2021.3062875','2024-02-03 22:25:51.799864','2024-02-03 22:25:51.799864'),(225,'[26] Robert Gifford. \"A lens-mapping framework for understanding the encoding and decoding of interpersonal dispositions in nonverbal behavior.\" Journal of Personality and Social Psychology 66(2):398-412 American Psychological Association (APA), 1994. DOI: 10.1037//0022-3514.66.2.398','2024-02-03 22:25:51.805121','2024-02-03 22:25:51.805121'),(226,'[27] Quentin Grail, Julien Perez, Eric Gaussier. \"Globalizing BERT-based Transformer Architectures for Long Document Summarization\"  Association for Computational Linguistics, 2021. DOI: 10.18653/v1/2021.eacl-main.154','2024-02-03 22:25:51.811233','2024-02-03 22:25:51.811233'),(227,'[28] Anushka Gupta, Diksha Chugh, Anjum, Rahul Katarya. \"Automated News Summarization Using Transformers\"  Springer Singapore, 2022. DOI: 10.1007/978-981-16-9012-9_21','2024-02-03 22:25:51.816538','2024-02-03 22:25:51.816538'),(228,'[29] Alfred Hermida, Mary Lynn Young. \"Finding the Data Unicorn\" Digital Journalism 5(2):159-176 Informa UK Limited, 2017. DOI: 10.1080/21670811.2016.1162663','2024-02-03 22:25:51.820784','2024-02-03 22:25:51.820784'),(229,'[30] Karoliina Isoaho, Daria Gritsenko, Eetu Mäkelä. \"Topic Modeling and Text Analysis for Qualitative Policy Research\" Policy Studies Journal 49(1):300-324 Wiley, 2021. DOI: 10.1111/psj.12343','2024-02-03 22:25:51.826583','2024-02-03 22:25:51.826583'),(230,'[31] Carina Jacobi, Wouter Van Atteveldt, Kasper Welbers. \"Quantitative analysis of large amounts of journalistic texts using topic modelling\" Digital Journalism 4(1):89-106 Informa UK Limited, 2016. DOI: 10.1080/21670811.2015.1093271','2024-02-03 22:25:51.831576','2024-02-03 22:25:51.831576'),(231,'[32] Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig. \"How Can We Know What Language Models Know?\" Transactions of the Association for Computational Linguistics 8(None):423-438 MIT Press - Journals, 2020-12. DOI: 10.1162/tacl_a_00324','2024-02-03 22:25:51.837248','2024-02-03 22:25:51.837248'),(232,'[33] Shima Khanehzar, Trevor Cohn, Gosia Mikolajczak, Andrew Turpin, Lea Frermann. \"Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames\"  Association for Computational Linguistics, 2019. DOI: 10.18653/v1/2021.naacl-main.174','2024-02-03 22:25:51.844247','2024-02-03 22:25:51.844247'),(233,'[34] Jeesun Kim, Wayne Wanta. \"News framing of the U.S. immigration debate during election years: Focus on generic frames\" The Communication Review 21(2):89-115 Informa UK Limited, 2018-04-03. DOI: 10.1080/10714421.2018.1479616','2024-02-03 22:25:51.849314','2024-02-03 22:25:51.849314'),(234,'[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar. \"Holistic evaluation of language models\"  .','2024-02-03 22:25:51.854315','2024-02-03 22:25:51.854315'),(235,'[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\"  . DOI: 10.48550/ARXIV.2107.13586','2024-02-03 22:25:51.859448','2024-02-03 22:25:51.859448'),(236,'[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, Derry Tanti Wijaya. \"Detecting Frames in News Headlines and Its Application to Analyzing News Framing Trends Surrounding U.S. Gun Violence\"  Association for Computational Linguistics, 2019. DOI: 10.18653/v1/k19-1047','2024-02-03 22:25:51.865295','2024-02-03 22:25:51.865295'),(237,'[38] Jrg Matthes, Matthias Kohring. \"The Content Analysis of Media Frames: Toward Improving Reliability and Validity\" Journal of Communication 58(2):258-279 Oxford University Press (OUP), 2008-06. DOI: 10.1111/j.1460-2466.2008.00384.x','2024-02-03 22:25:51.869557','2024-02-03 22:25:51.869557'),(238,'[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, David E Losada. \"Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI\"  ACM, 2022-07-26. DOI: 10.1145/3543829.3544529','2024-02-03 22:25:51.876119','2024-02-03 22:25:51.876119'),(239,'[40] Symeon Stuart E Middleton, Yiannis Papadopoulos, Kompatsiaris. \"Social computing for verifying social media content in breaking news\" IEEE Internet Computing 22(2):83-89 .','2024-02-03 22:25:51.881287','2024-02-03 22:25:51.881287'),(240,'[41] Marko Milosavljević, Igor Vobič. \"‘Our task is to demystify fears’: Analysing newsroom management of automation in journalism\" Journalism 22(9):2203-2221 SAGE Publications, 2021. DOI: 10.1177/1464884919861598','2024-02-03 22:25:51.885729','2024-02-03 22:25:51.885729'),(241,'[42] R Monarch. \"Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI\"  Manning, 2021.','2024-02-03 22:25:51.891696','2024-02-03 22:25:51.891696'),(242,'[43] Tom Nicholls, Pepper D Culpepper. \"Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities\" Political Communication 38(1-2):159-181 Informa UK Limited, 2021. DOI: 10.1080/10584609.2020.1812777','2024-02-03 22:25:51.897309','2024-02-03 22:25:51.897309'),(243,'[44] Zhongdang Pan, Gerald M Kosicki. \"Framing analysis: An approach to news discourse\" Political Communication 10(1):55-75 Informa UK Limited, 1993. DOI: 10.1080/10584609.1993.9962963','2024-02-03 22:25:51.902562','2024-02-03 22:25:51.902562'),(244,'[45] Raul Puri, Bryan Catanzaro. \"Preprint repository arXiv achieves milestone million uploads\" Physics Today None(None):None AIP Publishing, 2019. DOI: 10.1063/pt.5.028530','2024-02-03 22:25:51.907771','2024-02-03 22:25:51.907771'),(245,'[46] Guanghui Qin, Jason Eisner. \"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\"  Association for Computational Linguistics, 2021. DOI: 10.18653/v1/2021.naacl-main.410','2024-02-03 22:25:51.912223','2024-02-03 22:25:51.912223'),(246,'[47] Rabindra Lamsal. \"A Sentiment Analysis Dashboard of COVID-19 Tweets\"  Institute for Operations Research and the Management Sciences (INFORMS), 2021-04-12. DOI: 10.1287/lytx.2021.02.20n','2024-02-03 22:25:51.917645','2024-02-03 22:25:51.917645'),(247,'[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. \"Language models are unsupervised multitask learners\" OpenAI blog 1(8):9 .','2024-02-03 22:25:51.922340','2024-02-03 22:25:51.922340'),(248,'[49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, Ahad Ali. \"Fake News Classification using transformer based enhanced LSTM and BERT\" International Journal of Cognitive Computing in Engineering 3(None):98-105 Elsevier BV, 2022-06. DOI: 10.1016/j.ijcce.2022.03.003','2024-02-03 22:25:51.929171','2024-02-03 22:25:51.929171'),(249,'[50] Frida V Rodelo. \"El framing sobre la pandemia de Covid-19 y sus factores indicadores organizacionales\" Cuadernos.info 50(50):91-112 Pontificia Universidad Catolica de Chile, 2021. DOI: 10.7764/cdi.50.37525','2024-02-03 22:25:51.934393','2024-02-03 22:25:51.934393'),(250,'[51] Le Teven, Angela Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ilić, Roman Hesslow, Alexandra Castagné, François Sasha Luccioni, Matthias Yvon, Gallé. \"Bloom: A 176b-parameter open-access multilingual language model\"  .','2024-02-03 22:25:51.939642','2024-02-03 22:25:51.939642'),(251,'[52] Holli A Semetko, Patti M Valkenburg Valkenburg. \"Framing European politics: A Content Analysis of Press and Television News\" Journal of Communication 50(2):93-109 Oxford University Press (OUP), 2000-06-01. DOI: 10.1111/j.1460-2466.2000.tb02843.x','2024-02-03 22:25:51.946308','2024-02-03 22:25:51.946308'),(252,'[53] Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme. \"Constrained Language Models Yield Few-Shot Semantic Parsers\"  Association for Computational Linguistics, 2021. DOI: 10.18653/v1/2021.emnlp-main.608','2024-02-03 22:25:51.951569','2024-02-03 22:25:51.951569'),(253,'[54] Efstathios A Sidiropoulos, Andreas A Veglis. \"Computer Supported Collaborative Work trends on Media Organizations: Mixing Qualitative and Quantitative Approaches\" Studies in Media and Communication 5(1):63 Redfame Publishing, 2017-04-23. DOI: 10.11114/smc.v5i1.2279','2024-02-03 22:25:51.957351','2024-02-03 22:25:51.957351'),(254,'[55] Emma Strubell, Ananya Ganesh, Andrew Mccallum. \"Energy and Policy Considerations for Deep Learning in NLP\"  Association for Computational Linguistics, 2019. DOI: 10.18653/v1/p19-1355','2024-02-03 22:25:51.963484','2024-02-03 22:25:51.963484'),(255,'[56] Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli. \"Understanding the capabilities, limitations, and societal impact of large language models\"  .','2024-02-03 22:25:51.968097','2024-02-03 22:25:51.968097'),(256,'[57] H Trieu, Quoc V Trinh, Le. \"Preprint repository arXiv achieves milestone million uploads\" Physics Today None(None):None AIP Publishing, 2018. DOI: 10.1063/pt.5.028530','2024-02-03 22:25:51.973201','2024-02-03 22:25:51.973201'),(257,'[58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, Oriol Eslami, Felix Vinyals, Hill. \"Multimodal few-shot learning with frozen language models\" Advances in Neural Information Processing Systems 34(None):200-212 .','2024-02-03 22:25:51.978177','2024-02-03 22:25:51.979422'),(258,'[59] A Sandra, Prashant Vannoy, Palvia. \"The social influence model of technology adoption\" Commun. ACM 53(6):149-153 .','2024-02-03 22:25:51.984500','2024-02-03 22:25:51.984500'),(259,'[60] Tuukka Ylä-Anttila, Veikko Eranti, Anna Kukkonen. \"Topic modeling for frame analysis: A study of media debates on climate change in India and USA\" Global Media and Communication 18(1):91-112 .','2024-02-03 22:25:51.990228','2024-02-03 22:25:51.990228'),(260,'[1] Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog. \"POINTREC: A Test Collection for Narrative-driven Point of Interest Recommendation\"  ACM, 2021-07-11. DOI: 10.1145/3404835.3463243','2024-02-03 22:25:54.632456','2024-02-03 22:25:54.632456'),(261,'[2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, Fernando Diaz. \"Tip of the Tongue Known-Item Retrieval\"  ACM, 2021-03-14. DOI: 10.1145/3406522.3446021','2024-02-03 22:25:54.638850','2024-02-03 22:25:54.638850'),(262,'[3] Toine Bogers, Maria Gäde, Marijn Koolen, Vivien Petras, Mette Skov. \"“What was this Movie About this Chick?”\"  Springer International Publishing, 2018-03-25. DOI: 10.1007/978-3-319-78105-1_36','2024-02-03 22:25:54.644394','2024-02-03 22:25:54.644394'),(263,'[4] Toine Bogers, Maria Gäde, Marijn Koolen, Vivien Petras, Mette Skov. \"“Looking for an Amazing Game I Can Relax and Sink Hours into...”: A Study of Relevance Aspects in Video Game Discovery\"  Springer International Publishing, 2019-03-31. DOI: 10.1007/978-3-030-15742-5_48','2024-02-03 22:25:54.649478','2024-02-03 22:25:54.650625'),(264,'[5] Toine Bogers, Marijn Koolen. \"Defining and Supporting Narrative-driven Recommendation\"  ACM, 2017-08-27. DOI: 10.1145/3109859.3109893','2024-02-03 22:25:54.655114','2024-02-03 22:25:54.655114'),(265,'[6] Toine Bogers, Marijn Koolen. \"Defining and Supporting Narrative-driven Recommendation\"  ACM, 2018. DOI: 10.1145/3109859.3109893','2024-02-03 22:25:54.660711','2024-02-03 22:25:54.660711'),(266,'[7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira. \"InPars: Unsupervised Dataset Generation for Information Retrieval\"  ACM, 2022-07-06. DOI: 10.1145/3477495.3531863','2024-02-03 22:25:54.666958','2024-02-03 22:25:54.666958'),(267,'[8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg. \"InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers\"  .','2024-02-03 22:25:54.671990','2024-02-03 22:25:54.671990'),(268,'[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei. \"Language Models are Few-Shot Learners\"  Curran Associates, Inc, 2020.','2024-02-03 22:25:54.678710','2024-02-03 22:25:54.678710'),(269,'[10] Chris Buckley, Ellen M Voorhees. \"Retrieval evaluation with incomplete information\"  ACM, 2004-07-25. DOI: 10.1145/1008992.1009000','2024-02-03 22:25:54.684671','2024-02-03 22:25:54.684671'),(270,'[11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, Sang-Wook Kim. \"AR-CF\"  ACM, 2020-07-25. DOI: 10.1145/3397271.3401038','2024-02-03 22:25:54.690241','2024-02-03 22:25:54.690241'),(271,'[12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, Meng Wang. \"Improving Recommendation Fairness via Data Augmentation\"  ACM, 2023-04-30. DOI: 10.1145/3543507.3583341','2024-02-03 22:25:54.696043','2024-02-03 22:25:54.696043'),(272,'[13] Li Chen, Zhirun Zhang, Xinzhi Zhang, Lehong Zhao. \"A Pilot Study for Understanding Users’ Attitudes Towards a Conversational Agent for News Recommendation\"  ACM, 2022-07-26. DOI: 10.1145/3543829.3544530','2024-02-03 22:25:54.700465','2024-02-03 22:25:54.700465'),(273,'[14] Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma. \"Scaling instruction-finetuned language models\"  .','2024-02-03 22:25:54.706991','2024-02-03 22:25:54.706991'),(274,'[15] Zhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith Guu, Ming-Wei Hall, Chang. \"Promptagator: Few-shot Dense Retrieval From 8 Examples\"  .','2024-02-03 22:25:54.712996','2024-02-03 22:25:54.712996'),(275,'[16] Abhinandan S Das, Mayur Datar, Ashutosh Garg, Shyam Rajaram. \"Google news personalization\"  ACM, 2007-05-08. DOI: 10.1145/1242572.1242610','2024-02-03 22:25:54.718288','2024-02-03 22:25:54.718288'),(276,'[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, Dasarathi Sampath. \"The YouTube video recommendation system\"  ACM, 2010-09-26. DOI: 10.1145/1864708.1864770','2024-02-03 22:25:54.724269','2024-02-03 22:25:54.724269'),(277,'[18] Lukas Eberhard, Simon Walk, Lisa Posch, Denis Helic. \"Evaluating narrative-driven movie recommendations on Reddit\"  ACM, 2019-03-17. DOI: 10.1145/3301275.3302287','2024-02-03 22:25:54.731764','2024-02-03 22:25:54.731764'),(278,'[19] Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan. \"Precise Zero-Shot Dense Retrieval without Relevance Labels\"  Association for Computational Linguistics, 2022. DOI: 10.18653/v1/2023.acl-long.99','2024-02-03 22:25:54.736789','2024-02-03 22:25:54.736789'),(279,'[20] Negar Hariri, Bamshad Mobasher, Robin Burke. \"Query-driven context aware recommendation\"  ACM, 2013-10-12. DOI: 10.1145/2507157.2507187','2024-02-03 22:25:54.741214','2024-02-03 22:25:54.741214'),(280,'[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, L A Charles, Ellen M Clarke, Voorhees. \"Overview of the TREC 2016 Contextual Suggestion Track\"  .','2024-02-03 22:25:54.747011','2024-02-03 22:25:54.747011'),(281,'[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave. \"Unsupervised Dense Information Retrieval with Contrastive Learning\" Transactions on Machine Learning Research None(None):None .','2024-02-03 22:25:54.752806','2024-02-03 22:25:54.752806'),(282,'[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, Rodrigo Nogueira. \"InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval\"  .','2024-02-03 22:25:54.757129','2024-02-03 22:25:54.757129'),(283,'[24] Marijn Koolen, Toine Bogers, Maria Gäde, Mark Hall, Iris Hendrickx, Hugo Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, David Walsh. \"Overview of the CLEF 2016 Social Book Search Lab\"  Springer International Publishing, 2016. DOI: 10.1007/978-3-319-44564-9_29','2024-02-03 22:25:54.763273','2024-02-03 22:25:54.763273'),(284,'[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski, Fernando Pereira, Arun Tejasvi, Chaganty. \"Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models\"  .','2024-02-03 22:25:54.770182','2024-02-03 22:25:54.770182'),(285,'[26] Xin Liu, Yong Liu, Karl Aberer, Chunyan Miao. \"Personalized point-of-interest recommendation by mining users\' preference transition\"  ACM Press, 2013. DOI: 10.1145/2505515.2505639','2024-02-03 22:25:54.775478','2024-02-03 22:25:54.775478'),(286,'[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, Quan Yuan. \"An experimental evaluation of point-of-interest recommendation in location-based social networks\" Proceedings of the VLDB Endowment 10(10):1010-1021 Association for Computing Machinery (ACM), 2017-06. DOI: 10.14778/3115404.3115407','2024-02-03 22:25:54.780922','2024-02-03 22:25:54.780922'),(287,'[28] Federico López, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, Lucas Dixon. \"Augmenting the user-item graph with textual similarity models\"  .','2024-02-03 22:25:54.787031','2024-02-03 22:25:54.787031'),(288,'[29] Xing Han, Lu, Siva Reddy, Harm De Vries. \"The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents\"  .','2024-02-03 22:25:54.791429','2024-02-03 22:25:54.791429'),(289,'[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, Hojin Yang. \"Latent Linear Critiquing for Conversational Recommender Systems\"  ACM, 2020-04-20. DOI: 10.1145/3366423.3380003','2024-02-03 22:25:54.796324','2024-02-03 22:25:54.796324'),(290,'[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, Ryan Mcdonald. \"Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation\"  Association for Computational Linguistics, 2021. DOI: 10.18653/v1/2021.eacl-main.92','2024-02-03 22:25:54.802982','2024-02-03 22:25:54.802982'),(291,'[32] Sheshera Mysore, O\' Tim, Andrew Gorman, Hamed Mccallum, Zamani. \"CSFCube -A Test Collection of Computer Science Research Articles for Faceted Query by Example\"  . DOI: 10.48550/arXiv.2103.12906','2024-02-03 22:25:54.807498','2024-02-03 22:25:54.807498'),(292,'[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Jan Paul F Christiano, Ryan Leike, Lowe. \"Training language models to follow instructions with human feedback\"  Curran Associates, Inc, 2022.','2024-02-03 22:25:54.815680','2024-02-03 22:25:54.815680'),(293,'[34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker, Norbert Fuhr. \"Starting Conversations with Search Engines - Interfaces that Elicit Natural Language Queries\"  ACM, 2021-03-14. DOI: 10.1145/3406522.3446035','2024-02-03 22:25:54.822341','2024-02-03 22:25:54.822341'),(294,'[35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, Hugues Bouchard. \"Improving Content Retrievability in Search with Controllable Query Generation\"  ACM, 2023-04-30. DOI: 10.1145/3543507.3583261','2024-02-03 22:25:54.826736','2024-02-03 22:25:54.826736'),(295,'[36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, Ben Wedin. \"On Natural Language User Profiles for Transparent and Scrutable Recommendation\"  ACM, 2022-07-06. DOI: 10.1145/3477495.3531873','2024-02-03 22:25:54.833045','2024-02-03 22:25:54.833045'),(296,'[37] Nils Reimers, Iryna Gurevych. \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"  Association for Computational Linguistics, 2019. DOI: 10.18653/v1/d19-1410','2024-02-03 22:25:54.837930','2024-02-03 22:25:54.837930'),(297,'[38] Stephen Robertson, Hugo Zaragoza. \"The Probabilistic Relevance Framework: BM25 and Beyond\" Foundations and Trends® in Information Retrieval 3(4):333-389 Now Publishers, 2009-04. DOI: 10.1561/1500000019','2024-02-03 22:25:54.844534','2024-02-03 22:25:54.844534'),(298,'[39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Sultan, Christopher Potts. \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\"  Association for Computational Linguistics, 2023. DOI: 10.18653/v1/2023.emnlp-main.693','2024-02-03 22:25:54.850110','2024-02-03 22:25:54.850110'),(299,'[40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-Tau Yih, Joelle Pineau, Luke Zettlemoyer. \"Improving Passage Retrieval with Zero-Shot Question Generation\"  Association for Computational Linguistics, 2022. DOI: 10.18653/v1/2022.emnlp-main.249','2024-02-03 22:25:54.854541','2024-02-03 22:25:54.854541'),(300,'[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. \"MPNet: Masked and Permuted Pre-training for Language Understanding\"  .','2024-02-03 22:25:54.859564','2024-02-03 22:25:54.860566'),(301,'[42] Jaime Teevan, Susan T Dumais, Eric Horvitz. \"Personalizing search via automated analysis of interests and activities\"  ACM, 2005-08-15. DOI: 10.1145/1076034.1076111','2024-02-03 22:25:54.865634','2024-02-03 22:25:54.865634'),(302,'[43] Mengting Wan, Julian Mcauley. \"Item recommendation on monotonic behavior chains\"  ACM, 2018-09-27. DOI: 10.1145/3240323.3240369','2024-02-03 22:25:54.871160','2024-02-03 22:25:54.871160'),(303,'[44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, Jingrui He. \"Controllable Gradient Item Retrieval\"  ACM, 2021-04-19. DOI: 10.1145/3442381.3449963','2024-02-03 22:25:54.877022','2024-02-03 22:25:54.877022'),(304,'[45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang, Lizhen Cui. \"Enhancing Collaborative Filtering with Generative Augmentation\"  ACM, 2019-07-25. DOI: 10.1145/3292500.3330873','2024-02-03 22:25:54.882467','2024-02-03 22:25:54.883007'),(305,'[46] Jiajing Xu, Andrew Zhai, Charles Rosenberg. \"Rethinking Personalized Ranking at Pinterest: An End-to-End Approach\"  ACM, 2022-09-13. DOI: 10.1145/3523227.3547394','2024-02-03 22:25:54.887840','2024-02-03 22:25:54.887840'),(306,'[47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, Hongwei Zheng. \"CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation\"  ACM, 2023-04-30. DOI: 10.1145/3543507.3583538','2024-02-03 22:25:54.893524','2024-02-03 22:25:54.893524'),(307,'[48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, Filip Radlinski. \"Conversational Information Seeking\"  Now Publishers, 2022. DOI: 10.1561/9781638282013','2024-02-03 22:25:54.899653','2024-02-03 22:25:54.899653'),(308,'[49] Jie Zou, Yifan Chen, Evangelos Kanoulas. \"Towards Question-based Recommender Systems\"  ACM, 2020-07-25. DOI: 10.1145/3397271.3401180','2024-02-03 22:25:54.905556','2024-02-03 22:25:54.905556'),(309,'[1] Colin Atkinson, Joachim Bayer, Dirk Muthig. \"Component-Based Product Line Development: The KobrA Approach\"  Springer US, 2001-11. DOI: 10.1007/978-1-4615-4339-8_16','2024-02-03 22:25:57.394456','2024-02-03 22:25:57.394456'),(310,'[2] Colin Atkinson, Dietmar Stoll, Philipp Bostan. \"Orthographic Software Modeling: A Practical Approach to View-Based Development\"  Springer Berlin Heidelberg, 2010. DOI: 10.1007/978-3-642-14819-4_15','2024-02-03 22:25:57.404337','2024-02-03 22:25:57.404337'),(311,'[3] Colin Atkinson, Dietmar Stoll, Christian Tunjic. \"Orthographic Service Modeling\"  IEEE, 2011-08. DOI: 10.1109/edocw.2011.20','2024-02-03 22:25:57.409679','2024-02-03 22:25:57.409679'),(312,'[4] Stephen Lombardi. \"Interactions between eclipse foundation members and eclipse projects\"  Carleton University, 2013. DOI: 10.22215/etd/2009-09151','2024-02-03 22:25:57.416955','2024-02-03 22:25:57.416955'),(313,'[5] / Iso, Itu-T Iec. \"The Reference Model of Open Distributed Processing\"  RM-ODP, 1998.','2024-02-03 22:25:57.428672','2024-02-03 22:25:57.428672'),(314,'[6] J I J Jose, Raul Romero, A Vallecillo. \"Realizing Correspondences in MultiViewpoint Specifications\"  .','2024-02-03 22:25:57.437161','2024-02-03 22:25:57.437161'),(315,'[7] Marc Lankhorst. \"Beyond Enterprise Architecture\"  Springer Berlin Heidelberg, 2009. DOI: 10.1007/978-3-642-01310-2_12','2024-02-03 22:25:57.443768','2024-02-03 22:25:57.443768'),(316,'[8] . \"Information technology. Object Management Group Unified Modeling Language (OMG UML)\" Superstructure None(2):None BSI British Standards, 2007-11. DOI: 10.3403/bsisoiec19505','2024-02-03 22:25:57.449905','2024-02-03 22:25:57.449905'),(317,'[9] Oliver Sims. \"The OMG Business Object Facility and the OMG Business Object\"  Springer London, 2008-04. DOI: 10.1007/978-1-4471-0947-1_4','2024-02-03 22:25:57.456877','2024-02-03 22:25:57.456877'),(318,'[10] C Seybold, M Glinz, S Meier, N Merlo-Schett. \"An effective layout adaptation technique for a graphical modeling tool\"  IEEE, 2003. DOI: 10.1109/icse.2003.1201306','2024-02-03 22:25:57.462882','2024-02-03 22:25:57.462882'),(319,'[11] S Chekanov, J Boomsma. \"ATLAS note ATL-COM-PHYS-2009.\"  Office of Scientific and Technical Information (OSTI), 2013. DOI: 10.2172/970380','2024-02-03 22:25:57.472504','2024-02-03 22:25:57.472504'),(320,'[12] Sofiansyah Fadli, Yuan Sa\'adati. \"Perencanaan Strategis SITI Menggunakan Model The Open Group Architecture Framework (TOGAF)\" Jurnal Teknologi dan Ilmu Komputer Prima (JUTIKOMP) 2(1):36-41 Universitas Prima Indonesia, 2009-02. DOI: 10.34012/jutikomp.v2i1.423','2024-02-03 22:25:57.479476','2024-02-03 22:25:57.479476'),(321,'[13] Special Collections, University Libraries University Of Southern Mississippi. \"Naomi Buchheimer Papers\"  University of Southern Mississippi, 2015-05-11. DOI: 10.18785/fa.dg0129','2024-02-03 22:25:57.486232','2024-02-03 22:25:57.486232'),(322,'[14] J A Zachman. \"The Zachman Framework: A Primer for Enterprise Engineering and Manufacturing\"  .','2024-02-03 22:25:57.492265','2024-02-03 22:25:57.492265'),(323,'[1] Kurt Marti. \"Stochastic Optimization Methods\"  Springer Berlin Heidelberg, 2008. DOI: 10.1007/978-3-540-79458-5','2024-02-03 22:46:07.052495','2024-02-03 22:46:07.052495'),(324,'[2] Y Tsompanakis, N D Lagaros, M Papadrakakis. \"Structural Design Optimization Considering Uncertainties\"  CRC Press, 2008-02-07. DOI: 10.1201/b10995','2024-02-03 22:46:07.177746','2024-02-03 22:46:07.178259'),(325,'[3] Mohd Aman Khalid, Sahil Bansal, Varun Ramamohan. \"An augmented formulation for robust design optimization of structures using stochastic simulation method\" Research in Engineering Design 34(2):179-200 Springer Science and Business Media LLC, 2023. DOI: 10.1007/s00163-022-00405-z','2024-02-03 22:46:07.256936','2024-02-03 22:46:07.256936'),(326,'[4] Zeng Meng, Gang Li, Xuan Wang, Sadiq M Sait, Ali Rıza Yıldız. \"A Comparative Study of Metaheuristic Algorithms for Reliability-Based Design Optimization Problems\" Archives of Computational Methods in Engineering 28(3):1853-1869 Springer Science and Business Media LLC, 2021. DOI: 10.1007/s11831-020-09443-z','2024-02-03 22:46:07.326971','2024-02-03 22:46:07.326971'),(327,'[5] Laith Abualigah, Mohamed Abd Elaziz, Ahmad M Khasawneh, Mohammad Alshinwan, Rehab Ali Ibrahim, Mohammed A A Al-Qaness, Seyedali Mirjalili, Putra Sumari, Amir H Gandomi. \"Meta-heuristic optimization algorithms for solving real-world mechanical engineering design problems: a comprehensive survey, applications, comparative analysis, and results\" Neural Computing and Applications 34(6):4081-4110 Springer Science and Business Media LLC, 2022-01-16. DOI: 10.1007/s00521-021-06747-4','2024-02-03 22:46:07.407388','2024-02-03 22:46:07.407388'),(328,'[6] Javad Katebi, Mona Shoaei-Parchin, Mahdi Shariati, Nguyen Thoi Trung, Majid Khorami. \"Developed comparative analysis of metaheuristic optimization algorithms for optimal active control of structures\" Engineering with Computers 36(4):1539-1558 Springer Science and Business Media LLC, 2020. DOI: 10.1007/s00366-019-00780-7','2024-02-03 22:46:07.488094','2024-02-03 22:46:07.488094'),(329,'[7] Abdulaziz Alorf. \"A survey of recently developed metaheuristics and their comparative analysis\" Engineering Applications of Artificial Intelligence 117(None):105622 Elsevier BV, 2023-01. DOI: 10.1016/j.engappai.2022.105622','2024-02-03 22:46:07.561391','2024-02-03 22:46:07.561391'),(330,'[8] U Kirsch. \"Structural optimization: fundamentals and applications\"  Springer-Verlag, 2012.','2024-02-03 22:46:07.638687','2024-02-03 22:46:07.638687'),(331,'[9] C A Floudas, P A Pardalos. \"Encyclopedia of Optimization\"  Springer US, 2008. DOI: 10.1007/0-306-48332-7','2024-02-03 22:46:07.755883','2024-02-03 22:46:07.755883'),(332,'[10] Armen Der Kiureghian, Ove Ditlevsen. \"Aleatory or epistemic? Does it matter?\" Structural Safety 31(2):105-112 Elsevier BV, 2009-03. DOI: 10.1016/j.strusafe.2008.06.020','2024-02-03 22:46:07.877933','2024-02-03 22:46:07.877933'),(333,'[11] G I Schuëller, H A Jensen. \"Computational methods in optimization considering uncertainties – An overview\" Computer Methods in Applied Mechanics and Engineering 198(1):2-13 Elsevier BV, 2008-11. DOI: 10.1016/j.cma.2008.05.004','2024-02-03 22:46:07.994308','2024-02-03 22:46:07.994308'),(334,'[12] J Schneider, S Kirkpatrick. \"Stochastic Global Optimization\"  Springer US, 2007. DOI: 10.1007/978-0-387-74740-8','2024-02-03 22:46:08.120064','2024-02-03 22:46:08.120064'),(335,'[13] Bach Do, Makoto Ohsaki. \"A random search for discrete robust design optimization of linear-elastic steel frames under interval parametric uncertainty\" Computers & Structures 249(None):106506 Elsevier BV, 2021-06. DOI: 10.1016/j.compstruc.2021.106506','2024-02-03 22:46:08.239812','2024-02-03 22:46:08.239812'),(336,'[14] Alireza Asadpoure, Mazdak Tootkaboni, James K Guest. \"Robust topology optimization of structures with uncertainties in stiffness – Application to truss structures\" Computers & Structures 89(11-12):1131-1141 Elsevier BV, 2011-06. DOI: 10.1016/j.compstruc.2010.11.004','2024-02-03 22:46:08.342568','2024-02-03 22:46:08.342568'),(337,'[15] Ioannis Doltsinis, Zhan Kang. \"Robust design of structures using optimization methods\" Computer Methods in Applied Mechanics and Engineering 193(23-26):2221-2237 Elsevier BV, 2004-06. DOI: 10.1016/j.cma.2003.12.055','2024-02-03 22:46:08.405945','2024-02-03 22:46:08.405945'),(338,'[16] G Carneiro, N Das, C C António. \"Dimensional reduction applied to the reliabilitybased robust design optimization of composite structures\" Compos Struct 255(None):None . DOI: 10.1016/j.compstruct.2020.112937','2024-02-03 22:46:08.465683','2024-02-03 22:46:08.465683'),(339,'[17] Haichao An, Byeng D Youn, Heung Soo Kim. \"Reliability-based Design Optimization of Laminated Composite Structures under Delamination and Material Property Uncertainties\" International Journal of Mechanical Sciences 205(None):106561 Elsevier BV, 2021-09. DOI: 10.1016/j.ijmecsci.2021.106561','2024-02-03 22:46:08.569041','2024-02-03 22:46:08.569041'),(340,'[18] Z Li, L B Duan, A G Cheng, Z P Yao, T Chen, W Yao. \"Lightweight and crashworthiness design of an electric vehicle using a six-sigma robust design optimization method\" Engineering Optimization 51(8):1393-1411 Informa UK Limited, 2019. DOI: 10.1080/0305215x.2018.1521396','2024-02-03 22:46:08.627935','2024-02-03 22:46:08.627935'),(341,'[19] Hadi Gholinezhad, Seyed Hosein Torabi. \"Reliability-based multidisciplinary design optimization of an underwater vehicle including cost analysis\" Journal of Marine Science and Technology 27(1):11-26 Springer Science and Business Media LLC, 2021-06-29. DOI: 10.1007/s00773-021-00804-2','2024-02-03 22:46:08.749478','2024-02-03 22:46:08.749478'),(342,'[20] Kwon-Hee Lee, Gyung-Jin Park. \"Robust optimization considering tolerances of design variables\" Computers & Structures 79(1):77-86 Elsevier BV, 2001-01. DOI: 10.1016/s0045-7949(00)00117-6','2024-02-03 22:46:08.848599','2024-02-03 22:46:08.848599'),(343,'[21] Travis V Anderson, Christopher A Mattson. \"Propagating Skewness and Kurtosis Through Engineering Models for Low-Cost, Meaningful, Nondeterministic Design\" Journal of Mechanical Design 134(10):None ASME International, 2012-09-28. DOI: 10.1115/1.4007389','2024-02-03 22:46:08.907984','2024-02-03 22:46:08.907984'),(344,'[22] Qi Zhou, Yan Wang, Seung-Kyum Choi, Ping Jiang, Xinyu Shao, Jiexiang Hu, Leshi Shu. \"A robust optimization approach based on multi-fidelity metamodel\" Structural and Multidisciplinary Optimization 57(2):775-797 Springer Science and Business Media LLC, 2018. DOI: 10.1007/s00158-017-1783-4','2024-02-03 22:46:09.015519','2024-02-03 22:46:09.015519'),(345,'[23] G Gary Wang, S Shan. \"Review of Metamodeling Techniques in Support of Engineering Design Optimization\" Journal of Mechanical Design 129(4):370-380 ASME International, 2007. DOI: 10.1115/1.2429697','2024-02-03 22:46:09.109676','2024-02-03 22:46:09.109676'),(346,'[24] Tanmoy Chatterjee, Souvik Chakraborty, Rajib Chowdhury. \"A Critical Review of Surrogate Assisted Robust Design Optimization\" Archives of Computational Methods in Engineering 26(1):245-274 Springer Science and Business Media LLC, 2019. DOI: 10.1007/s11831-017-9240-5','2024-02-03 22:46:09.214621','2024-02-03 22:46:09.214621'),(347,'[25] Tanmoy Chatterjee, Michael I Friswell, Sondipon Adhikari, Rajib Chowdhury. \"A global two-layer meta-model for response statistics in robust design optimization\" Engineering Optimization 54(1):153-169 Informa UK Limited, 2021-01-11. DOI: 10.1080/0305215x.2020.1861262','2024-02-03 22:46:09.311225','2024-02-03 22:46:09.311225'),(348,'[26] Xu Guo, Xiaofang Zhao, Weisheng Zhang, Jun Yan, Guomin Sun. \"Multi-scale robust design and optimization considering load uncertainties\" Computer Methods in Applied Mechanics and Engineering 283(None):994-1009 Elsevier BV, 2015-01. DOI: 10.1016/j.cma.2014.10.014','2024-02-03 22:46:09.403544','2024-02-03 22:46:09.403544'),(349,'[27] D J Jerez, H A Jensen, M Beer. \"Reliability-based design optimization of structural systems under stochastic excitation: An overview\" Mechanical Systems and Signal Processing 166(None):108397 Elsevier BV, 2022-03. DOI: 10.1016/j.ymssp.2021.108397','2024-02-03 22:46:09.502308','2024-02-03 22:46:09.502308'),(350,'[28] Wei Li, Liang Gao, Mi Xiao. \"Multidisciplinary robust design optimization under parameter and model uncertainties\" Engineering Optimization 52(3):426-445 Informa UK Limited, 2020. DOI: 10.1080/0305215x.2019.1590564','2024-02-03 22:46:09.607898','2024-02-03 22:46:09.607898'),(351,'[29] Hans-Georg Beyer, Bernhard Sendhoff. \"Robust optimization – A comprehensive survey\" Computer Methods in Applied Mechanics and Engineering 196(33-34):3190-3218 Elsevier BV, 2007-07. DOI: 10.1016/j.cma.2007.03.003','2024-02-03 22:46:09.660768','2024-02-03 22:46:09.660768'),(352,'[30] Renato De S Motta, Silvana M B Afonso. \"An efficient procedure for structural reliability-based robust design optimization\" Structural and Multidisciplinary Optimization 54(3):511-530 Springer Science and Business Media LLC, 2016-03-09. DOI: 10.1007/s00158-016-1418-1','2024-02-03 22:46:09.711357','2024-02-03 22:46:09.711357'),(353,'[31] Ali R Yildiz. \"Comparison of evolutionary-based optimization algorithms for structural design optimization\" Engineering Applications of Artificial Intelligence 26(1):327-333 Elsevier BV, 2013-01. DOI: 10.1016/j.engappai.2012.05.014','2024-02-03 22:46:09.812256','2024-02-03 22:46:09.812256'),(354,'[32] André Teófilo Beck, Wellison José De Santana Gomes. \"A comparison of deterministic, reliability-based and risk-based structural optimization under uncertainty\" Probabilistic Engineering Mechanics 28(None):18-29 Elsevier BV, 2012-04. DOI: 10.1016/j.probengmech.2011.08.007','2024-02-03 22:46:09.905696','2024-02-03 22:46:09.905696'),(355,'[33] Erdem Acar, Gamze Bayrak, Yongsu Jung, Ikjin Lee, Palaniappan Ramu, Suja Shree Ravichandran. \"Modeling, analysis, and optimization under uncertainties: a review\" Structural and Multidisciplinary Optimization 64(5):2909-2945 Springer Science and Business Media LLC, 2021-08-21. DOI: 10.1007/s00158-021-03026-7','2024-02-03 22:46:10.026592','2024-02-03 22:46:10.026592'),(356,'[34] Angelos Georghiou, Daniel Kuhn, Wolfram Wiesemann. \"The decision rule approach to optimization under uncertainty: methodology and applications\" Computational Management Science 16(4):545-576 Springer Science and Business Media LLC, 2019. DOI: 10.1007/s10287-018-0338-5','2024-02-03 22:46:10.124842','2024-02-03 22:46:10.124842'),(357,'[35] Oussama Braydi, Pascal Lafon, Rafic Younes. \"Study of Uncertainties and Objective Function Modeling Effects on Probabilistic Optimization Results\" ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part B: Mechanical Engineering 5(4):None ASME International, 2019-09-25. DOI: 10.1115/1.4044152','2024-02-03 22:46:10.223540','2024-02-03 22:46:10.223540'),(358,'[36] Wang-Sheng Liu, Sai Hung Cheung. \"Reliability based design optimization with approximate failure probability function in partitioned design space\" Reliability Engineering & System Safety 167(None):602-611 Elsevier BV, 2017-11. DOI: 10.1016/j.ress.2017.07.007','2024-02-03 22:46:10.324360','2024-02-03 22:46:10.324360'),(359,'[37] Anukal Chiralaksanakul, Sankaran Mahadevan. \"First-Order Approximation Methods in Reliability-Based Design Optimization\" Journal of Mechanical Design 127(5):851-857 ASME International, 2005. DOI: 10.1115/1.1899691','2024-02-03 22:46:10.429760','2024-02-03 22:46:10.429760'),(360,'[38] Ioannis Doltsinis, Zhan Kang, Gengdong Cheng. \"Robust design of non-linear structures using optimization methods\" Computer Methods in Applied Mechanics and Engineering 194(12-16):1779-1795 Elsevier BV, 2005-04. DOI: 10.1016/j.cma.2004.02.027','2024-02-03 22:46:10.482898','2024-02-03 22:46:10.482898'),(361,'[39] A A Taflanidis, J L Beck. \"Stochastic Subset Optimization for optimal reliability problems\" Probabilistic Engineering Mechanics 23(2-3):324-338 Elsevier BV, 2008-04. DOI: 10.1016/j.probengmech.2007.12.011','2024-02-03 22:46:10.581362','2024-02-03 22:46:10.581362'),(362,'[40] Siu-Kui Au, James L Beck. \"Estimation of small failure probabilities in high dimensions by subset simulation\" Probabilistic Engineering Mechanics 16(4):263-277 Elsevier BV, 2001-10. DOI: 10.1016/s0266-8920(01)00019-4','2024-02-03 22:46:10.681784','2024-02-03 22:46:10.681784'),(363,'[41] Gaofeng Jia, Alexandros A Taflanidis. \"Non-parametric stochastic subset optimization for optimal-reliability design problems\" Computers & Structures 126(None):86-99 Elsevier BV, 2013-09. DOI: 10.1016/j.compstruc.2012.12.009','2024-02-03 22:46:10.771703','2024-02-03 22:46:10.771703'),(364,'[42] Alexandros A Taflanidis. \"Stochastic subset optimization incorporating moving least squares response surface methodologies for stochastic sampling\" Advances in Engineering Software 44(1):3-14 Elsevier BV, 2012-02. DOI: 10.1016/j.advengsoft.2011.07.009','2024-02-03 22:46:10.870706','2024-02-03 22:46:10.870706'),(365,'[43] Mohd Aman Khalid, Sahil Bansal. \"Framework for Robust Design Optimization of Tuned Mass Dampers by Stochastic Subset Optimization\" International Journal of Structural Stability and Dynamics 23(13):None World Scientific Pub Co Pte Ltd, 2023-02-24. DOI: 10.1142/s0219455423501559','2024-02-03 22:46:10.959134','2024-02-03 22:46:10.959134'),(366,'[44] S K Au. \"Reliability-based design sensitivity by efficient simulation\" Computers & Structures 83(14):1048-1061 Elsevier BV, 2005-05. DOI: 10.1016/j.compstruc.2004.11.015','2024-02-03 22:46:11.045683','2024-02-03 22:46:11.045683'),(367,'[45] Alexandros A Taflanidis, James L Beck. \"An efficient framework for optimal robust stochastic system design using stochastic simulation\" Computer Methods in Applied Mechanics and Engineering 198(1):88-101 Elsevier BV, 2008-11. DOI: 10.1016/j.cma.2008.03.029','2024-02-03 22:46:11.142304','2024-02-03 22:46:11.142304'),(368,'[46] Christian P Robert, George Casella. \"Monte Carlo Optimization\"  Springer New York, 2004. DOI: 10.1007/978-1-4757-4145-2_5','2024-02-03 22:46:11.246910','2024-02-03 22:46:11.246910'),(369,'[47] Hong-Shuang S Li. \"Subset simulation for unconstrained global optimization\" Applied Mathematical Modelling 35(10):5108-5120 Elsevier BV, 2011-10. DOI: 10.1016/j.apm.2011.04.023','2024-02-03 22:46:11.294285','2024-02-03 22:46:11.294285'),(370,'[48] Mohd Aman Khalid, Sahil Bansal, Varun Ramamohan. \"An augmented formulation for robust design optimization of structures using stochastic simulation method\" Research in Engineering Design 34(2):179-200 Springer Science and Business Media LLC, 2022-12-05. DOI: 10.1007/s00163-022-00405-z','2024-02-03 22:46:11.390348','2024-02-03 22:46:11.390348'),(371,'[49] Alexandros A Taflanidis, James L Beck. \"An efficient framework for optimal robust stochastic system design using stochastic simulation\" Computer Methods in Applied Mechanics and Engineering 198(1):88-101 Elsevier BV, 2008-11. DOI: 10.1016/j.cma.2008.03.029','2024-02-03 22:46:11.479980','2024-02-03 22:46:11.479980'),(372,'[50] Elif Cagda Kandemir, Ali Mortazavi. \"Optimization of Seismic Base Isolation System Using a Fuzzy Reinforced Swarm Intelligence\" Advances in Engineering Software 174(None):103323 Elsevier BV, 2022-12. DOI: 10.1016/j.advengsoft.2022.103323','2024-02-03 22:46:11.567443','2024-02-03 22:46:11.568444'),(373,'[51] S Rebay. \"Efficient Unstructured Mesh Generation by Means of Delaunay Triangulation and Bowyer-Watson Algorithm\" Journal of Computational Physics 106(1):125-138 Elsevier BV, 1993-05. DOI: 10.1006/jcph.1993.1097','2024-02-03 22:46:11.659880','2024-02-03 22:46:11.659880'),(374,'[52] Noah Wade, Lori Graham‐brady. \"Estimating microstructural feature distributions from image data using a Bayesian framework\" Journal of Microscopy 290(3):137-152 Wiley, 2023-03-23. DOI: 10.1111/jmi.13184','2024-02-03 22:46:11.760461','2024-02-03 22:46:11.760461'),(375,'[53] Xinqiao Duan, Lin Li, Yong Ge, Bo Liu. \"Exact Voronoi diagram for topographic spatial analysis\" GIScience & Remote Sensing 60(1):None Informa UK Limited, 2023-02. DOI: 10.1080/15481603.2023.2171703','2024-02-03 22:46:11.850151','2024-02-03 22:46:11.850151'),(376,'[54] Susan Eitelman. \"Matlab Version 6.5 Release 13. The MathWorks, Inc., 3 Apple Hill Dr., Natick, MA 01760-2098; 508/647-7000, Fax 508/647-7001, www.mathworks.com\" Ergonomics in Design: The Quarterly of Human Factors Applications 11(3):27-29 SAGE Publications, 2021. DOI: 10.1177/106480460301100306','2024-02-03 22:46:11.949289','2024-02-03 22:46:11.949289'),(377,'[1] G Mahendran, K V Kandaswamy. \"Ant Colony Optimized Tuned DC-DC converter\" International Journal of Computer Applications None(11):45-50 .','2024-02-03 22:46:14.929212','2024-02-03 22:46:14.929212'),(378,'[2] J Dunia, Mwinyiwiwa. \"Performance Comparison between ĆUK and SEPIC Converters for Maximum Power Point Tracking Using Incremental Conductance Technique in Solar Power Applications\" International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering 7(12):2510-2517 .','2024-02-03 22:46:15.018813','2024-02-03 22:46:15.018813'),(379,'[3] Maurizio Cirrincione, Marcello Pucci, Gianpaolo Vitale. \"Growing Neural Gas (GNG)-Based Maximum Power Point Tracking for High-Performance Wind Generator With an Induction Machine\" IEEE Transactions on Industry Applications 47(2):861-872 Institute of Electrical and Electronics Engineers (IEEE), 2011-03. DOI: 10.1109/tia.2010.2102994','2024-02-03 22:46:15.118085','2024-02-03 22:46:15.118085'),(380,'[4] Musa Abdulkadir, A S Samosir, Ahn H M Yatim. \"Modeling and Simulation of a Solar Photovoltaic System, Its Dynamics and Transient Characteristics in LABVIEW\" International Journal of Power Electronics and Drive Systems (IJPEDS) 3(2):185-192 Institute of Advanced Engineering and Science, 2013-06-22. DOI: 10.11591/ijpeds.v3i2.2422','2024-02-03 22:46:15.216244','2024-02-03 22:46:15.216244'),(381,'[5] H Bouzeria, C Fetha, T Bahi, I Abadlia, Z Layate, S Lekhchine. \"Fuzzy Logic Space Vector Direct Torque Control of PMSM for Photovoltaic Water Pumping System\" Energy Procedia 74(None):760-771 Elsevier BV, 2015-08. DOI: 10.1016/j.egypro.2015.07.812','2024-02-03 22:46:15.313934','2024-02-03 22:46:15.314446'),(382,'[6] Yaow-Ming Chen, Yuan-Chuan Liu, Shih-Chieh Hung, Chung-Sheng Cheng. \"Multi-Input Inverter for Grid-Connected Hybrid PV/Wind Power System\" IEEE Transactions on Power Electronics 22(3):1070-1077 Institute of Electrical and Electronics Engineers (IEEE), 2007-05. DOI: 10.1109/tpel.2007.897117','2024-02-03 22:46:15.423043','2024-02-03 22:46:15.424043'),(383,'[7] S Ganesh, J Janani, G B Angel. \"A Maximum Power Point Tracker for PV Panels Using SEPIC Converter\" International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering 8(2):637-642 .','2024-02-03 22:46:15.477656','2024-02-03 22:46:15.477656'),(384,'[8] R Vijayabalan, S Ravivarman. \"Z Source Inverter for Photovoltaic System with Fuzzy Logic Controller\" International Journal of Power Electronics and Drive System (IJPEDS) 2(4):371-379 .','2024-02-03 22:46:15.537976','2024-02-03 22:46:15.537976'),(385,'[9] A Ramkumar, Svs Florence. \"Analysis of Single Phase AC-DC SEPIC Converter using Closed Loop Techniques\" International Journal of Advanced Research in Electrical, Electronics and Instrumentation Engineering 4(2):193-201 .','2024-02-03 22:46:15.599818','2024-02-03 22:46:15.599818'),(386,'[10] G Tadi, P Ramamurthyraju. \"Analysis of SEPIC for PV-Applications using PI Controller and Current Mode Control\" International Journal for Scientific Research & Development 1(9):175-180 .','2024-02-03 22:46:15.658068','2024-02-03 22:46:15.658068'),(387,'[11] J Li, H Wang. \"A novel stand-alone PV generation system based on variable step size INC MPPT and SVPWM control\"  .','2024-02-03 22:46:15.708562','2024-02-03 22:46:15.708562'),(388,'[12] C Wang. \"A Study of Membership Functions on Mamdani-Type Fuzzy Inference System for Industrial Decision-Making\"  .','2024-02-03 22:46:15.759304','2024-02-03 22:46:15.759304'),(389,'[13] Ma, Ö Usta, İ H Akyazi, Altaş. \"Design and performance of solar tracking system with fuzzy logic controller used different membership functions\"  .','2024-02-03 22:46:15.814426','2024-02-03 22:46:15.814426'),(390,'[14] R K Mudi, N R Pal. \"A robust self-tuning scheme for PI- and PD-type fuzzy controllers\" IEEE Transactions on Fuzzy Systems 7(1):2-16 Institute of Electrical and Electronics Engineers (IEEE), 1999. DOI: 10.1109/91.746295','2024-02-03 22:46:15.897752','2024-02-03 22:46:15.897752'),(391,'[15] Rk, N R Mudi, Pal. \"A robust self-tuning scheme for PI-and PDtype fuzzy controllers\" IEEE Transactions on Fuzzy Systems 7(1):2-16 .','2024-02-03 22:46:15.955885','2024-02-03 22:46:15.955885'),(392,'[16] A Shehata, H Metered, Walid A H Oraby. \"Vibration Control of Active Vehicle Suspension System Using Fuzzy Logic Controller\"  Springer International Publishing, 2015. DOI: 10.1007/978-3-319-09918-7_35','2024-02-03 22:46:16.051308','2024-02-03 22:46:16.051308'),(393,'[1] Charles A Mader, Joaquim R R A Martins, Juan J Alonso, Edwin Van Der Weide. \"ADjoint: An Approach for the Rapid Development of Discrete Adjoint Solvers\" AIAA Journal 46(4):863-873 American Institute of Aeronautics and Astronautics (AIAA), 2008-04. DOI: 10.2514/1.29123','2024-02-03 22:46:20.196295','2024-02-03 22:46:20.196295'),(394,'[2] Marc C Kennedy, Anthony O\'hagan. \"Bayesian Calibration of Computer Models\" Journal of the Royal Statistical Society Series B: Statistical Methodology 63(3):425-464 Oxford University Press (OUP), 2001-09-01. DOI: 10.1111/1467-9868.00294','2024-02-03 22:46:20.309831','2024-02-03 22:46:20.309831'),(395,'[3] John T Hwang, Joaquim R R A Martins. \"A fast-prediction surrogate model for large datasets\" Aerospace Science and Technology 75(None):74-87 Elsevier BV, 2018-04. DOI: 10.1016/j.ast.2017.12.030','2024-02-03 22:46:20.413890','2024-02-03 22:46:20.413890'),(396,'[4] Joaquim R R A Martins, Andrew Ning. \"Engineering Design Optimization\"  Cambridge University Press, 2021-11-18. DOI: 10.1017/9781108980647','2024-02-03 22:46:20.496081','2024-02-03 22:46:20.496081'),(397,'[5] Mohamed Amine Bouhlel, John T Hwang, Nathalie Bartoli, Rémi Lafage, Joseph Morlier, Joaquim R R A Martins. \"A Python surrogate modeling framework with derivatives\" Advances in Engineering Software 135(None):102662 Elsevier BV, 2019-09. DOI: 10.1016/j.advengsoft.2019.03.005','2024-02-03 22:46:20.583925','2024-02-03 22:46:20.583925'),(398,'[6] M A Bouhlel, J Martins. \"Gradient-enhanced kriging for high-dimensional problems\" Eng Comput 35(None):157-173 .','2024-02-03 22:46:20.694619','2024-02-03 22:46:20.694619'),(399,'[7] F Pedregosa, G Varoquaux, A Gramfort, Vmb Thirion, O Grisel. \"Scikit-learn: Machine learning in Python\" J Mach Learn Res 12(None):2825-2830 .','2024-02-03 22:46:20.808050','2024-02-03 22:46:20.808050'),(400,'[8] P Saves.   .','2024-02-03 22:46:20.873390','2024-02-03 22:46:20.873390'),(401,'[9] Christos Lataniotis, Stefano Marelli, Bruno Sudret. \"UNCERTAINTY QUANTIFICATION IN THE CLOUD WITH UQCLOUD\"  Institute of Research and Development for Computational Methods in Engineering Sciences (ICMES), 2022. DOI: 10.7712/120221.8033.18990','2024-02-03 22:46:20.946721','2024-02-03 22:46:20.946721'),(402,'[10] Alessio Faraci, Pierre Beaurepaire, Nicolas Gayton. \"Review on Python Toolboxes for Kriging Surrogate Modelling\"  Research Publishing Services, 2022. DOI: 10.3850/978-981-18-5183-4_r16-08-037-cd','2024-02-03 22:46:21.013297','2024-02-03 22:46:21.013297'),(403,'[11] M Krügener, J F Zapata Usandivaras, M Bauerheim, A Urbano. \"Coaxial-Injector Surrogate Modeling Based on Reynolds-Averaged Navier–Stokes Simulations Using Deep Learning\" Journal of Propulsion and Power 38(5):783-798 American Institute of Aeronautics and Astronautics (AIAA), 2022-09. DOI: 10.2514/1.b38696','2024-02-03 22:46:21.087237','2024-02-03 22:46:21.087237'),(404,'[12] Deyu Ming, Daniel Williamson, Serge Guillas. \"Deep Gaussian Process Emulation using Stochastic Imputation\" Technometrics 65(2):150-161 Informa UK Limited, 2022-10-12. DOI: 10.1080/00401706.2022.2124311','2024-02-03 22:46:21.154871','2024-02-03 22:46:21.154871'),(405,'[13] Jan Eliáš, Miroslav Vořechovský, Václav Sadílek. \"Periodic version of the minimax distance criterion for Monte Carlo integration\" Advances in Engineering Software 149(None):102900 Elsevier BV, 2020-11. DOI: 10.1016/j.advengsoft.2020.102900','2024-02-03 22:46:21.229862','2024-02-03 22:46:21.229862'),(406,'[14] Vincent Drouet, Mathieu Balesdent, Loïc Brevault, Sylvain Dubreuil, Jérôme Morio. \"Multifidelity Algorithm for the Sensitivity Analysis of Multidisciplinary Problems\" Journal of Mechanical Design 145(7):1-22 ASME International, 2023-05-17. DOI: 10.1115/1.4062332','2024-02-03 22:46:21.305992','2024-02-03 22:46:21.305992'),(407,'[15] Pavel Karban, David Pánek, Tamás Orosz, Iveta Petrášová, Ivo Doležel. \"FEM based robust design optimization with Agros and Ārtap\" Computers & Mathematics with Applications 81(None):618-633 Elsevier BV, 2021-01. DOI: 10.1016/j.camwa.2020.02.010','2024-02-03 22:46:21.369022','2024-02-03 22:46:21.369022'),(408,'[16] Jakub Kudela, Radomil Matousek. \"Recent advances and applications of surrogate models for finite element method computations: a review\" Soft Computing 26(24):13709-13733 Springer Science and Business Media LLC, 2022-07-17. DOI: 10.1007/s00500-022-07362-8','2024-02-03 22:46:21.432927','2024-02-03 22:46:21.432927'),(409,'[17] Yang Chen, Fadwa Dababneh, Bei Zhang, Saiid Kassaee, Brennan T Smith, Xiaobing Liu, Ayyoub M Momen. \"Surrogate Modeling for Capacity Planning of Charging Station Equipped With Photovoltaic Panel and Hydropneumatic Energy Storage\" Journal of Energy Resources Technology 142(5):50907 ASME International, 2020. DOI: 10.1115/1.4045733','2024-02-03 22:46:21.497735','2024-02-03 22:46:21.497735'),(410,'[18] John Jasa, Pietro Bortolotti, Daniel Zalkind, Garrett Barter. \"Effectively using multifidelity optimization for wind turbine design\" Wind Energy Science 7(3):991-1006 Copernicus GmbH, 2022-05-11. DOI: 10.5194/wes-7-991-2022','2024-02-03 22:46:21.561974','2024-02-03 22:46:21.561974'),(411,'[19] Wen Wang, Guocheng Tao, Dandan Ke, Jiaqi Luo, Jiahuan Cui. \"Transpiration cooling of high pressure turbine vane with optimized porosity distribution\" Applied Thermal Engineering 223(None):119831 Elsevier BV, 2023-03. DOI: 10.1016/j.applthermaleng.2022.119831','2024-02-03 22:46:21.631501','2024-02-03 22:46:21.632505'),(412,'[20] Thomas Savage, Hector Fernando Almeida-Trasvina, Ehecatl Antonio Del Río-Chanona, Robin Smith, Dongda Zhang. \"An adaptive data-driven modelling and optimization framework for complex chemical process design\"  Elsevier, 2020. DOI: 10.1016/b978-0-12-823377-1.50013-6','2024-02-03 22:46:21.696918','2024-02-03 22:46:21.696918'),(413,'[21] Anouck Chan, Anthony Fernandes Pires, Thomas Polacsek. \"Trying to Elicit and Assign Goals to the Right Actors\"  Springer International Publishing, 2022. DOI: 10.1007/978-3-031-17995-2_29','2024-02-03 22:46:21.766549','2024-02-03 22:46:21.766549'),(414,'[22] F Hutter, M A Osborne. \"A kernel for hierarchical parameter spaces\"  .','2024-02-03 22:46:21.834812','2024-02-03 22:46:21.834812'),(415,'[23] Jasper H Bussemaker, Pier Davide Ciampa, Bjoern Nagel. \"System Architecture Design Space Exploration: An Approach to Modeling and Optimization\"  American Institute of Aeronautics and Astronautics, 2020-06-08. DOI: 10.2514/6.2020-3172','2024-02-03 22:46:21.919207','2024-02-03 22:46:21.919207'),(416,'[24] Mea Fouda, E J Adler, J Bussemaker, Jrra Martins, D F Kurtulus, L Boggero. \"Automated hybrid propulsion model construction for conceptual aircraft design and optimization\"  .','2024-02-03 22:46:22.022785','2024-02-03 22:46:22.022785'),(417,'[25] Jasper H Bussemaker, Nathalie Bartoli, Thierry Lefebvre, Pier Davide Ciampa, Björn Nagel. \"Effectiveness of Surrogate-Based Optimization Algorithms for System Architecture Optimization\"  American Institute of Aeronautics and Astronautics, 2021-07-28. DOI: 10.2514/6.2021-3095','2024-02-03 22:46:22.133352','2024-02-03 22:46:22.133352'),(418,'[26] M Balandat, B Karrer, D Jiang, S Daulton, B Letham, A Wilson. \"BoTorch: A framework for efficient Monte-Carlo Bayesian optimization\" Adv Neural Inf Process Syst 33(None):21524-21538 .','2024-02-03 22:46:22.199537','2024-02-03 22:46:22.199537'),(419,'[27] B Adams, W Bohnhoff, K Dalbey, M Ebeida, J Eddy, M Eldred. \"Dakota, a multilevel parallel object-oriented framework for design optimization, parameter estimation\"  Sandia National Lab.(SNL-NM, 2020.','2024-02-03 22:46:22.261242','2024-02-03 22:46:22.261242'),(420,'[28] Olivier Roustant, David Ginsbourger, Yves Deville. \"<b>DiceKriging</b>,<b>DiceOptim</b>: Two<i>R</i>Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization\" Journal of Statistical Software 51(1):1-55 Foundation for Open Access Statistic, 2012. DOI: 10.18637/jss.v051.i01','2024-02-03 22:46:22.329708','2024-02-03 22:46:22.330715'),(421,'[29] Yichi Zhang, Siyu Tao, Wei Chen, Daniel W Apley. \"A Latent Variable Approach to Gaussian Process Modeling with Qualitative and Quantitative Factors\" Technometrics 62(3):291-302 Informa UK Limited, 2020. DOI: 10.1080/00401706.2019.1638834','2024-02-03 22:46:22.393374','2024-02-03 22:46:22.393374'),(422,'[30] Tyler H Chang, Stefan M Wild. \"ParMOO: A Python library for parallel multiobjective simulation optimization\" Journal of Open Source Software 8(82):4468 The Open Journal, 2023-02-03. DOI: 10.21105/joss.04468','2024-02-03 22:46:22.454876','2024-02-03 22:46:22.454876'),(423,'[31] Eduardo C Garrido-Merchán, Daniel Hernández-Lobato. \"Dealing with categorical and integer-valued variables in Bayesian Optimization with Gaussian processes\" Neurocomputing 380(None):20-35 Elsevier BV, 2020-03. DOI: 10.1016/j.neucom.2019.11.004','2024-02-03 22:46:22.530771','2024-02-03 22:46:22.530771'),(424,'[32] M Halstrup. \"Black-box optimization of mixed discrete-continuous optimization problems\"  .','2024-02-03 22:46:22.626316','2024-02-03 22:46:22.626316'),(425,'[33] Olivier Roustant, Espéran Padonou, Yves Deville, Aloïs Clément, Guillaume Perrin, Jean Giorla, Henry Wynn. \"Group Kernels for Gaussian Process Metamodels with Categorical Inputs\" SIAM/ASA Journal on Uncertainty Quantification 8(2):775-806 Society for Industrial & Applied Mathematics (SIAM), 2020-01. DOI: 10.1137/18m1209386','2024-02-03 22:46:22.786159','2024-02-03 22:46:22.787084'),(426,'[34] Qiang Zhou, Peter Z G Qian, Shiyu Zhou. \"A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors\" Technometrics 53(3):266-273 Informa UK Limited, 2011-08. DOI: 10.1198/tech.2011.10025','2024-02-03 22:46:22.857388','2024-02-03 22:46:22.857388'),(427,'[35] P Saves, Y Diouane, N Bartoli, T Lefebvre, J Morlier. \"A mixed-categorical correlation kernel for Gaussian process\" Neurocomputing 550(None):126472 Elsevier BV, 2023-09. DOI: 10.1016/j.neucom.2023.126472','2024-02-03 22:46:22.923430','2024-02-03 22:46:22.923430'),(428,'[36] Julien Pelamatti, Loïc Brevault, Mathieu Balesdent, El-Ghazali Talbi, Yannick Guerin. \"Efficient global optimization of constrained mixed variable problems\" Journal of Global Optimization 73(3):583-613 Springer Science and Business Media LLC, 2019. DOI: 10.1007/s10898-018-0715-1','2024-02-03 22:46:22.994324','2024-02-03 22:46:22.994324'),(429,'[37] Daniel Horn, Jörg Stork, Nils-Jannik Schüßler, Martin Zaefferer. \"Surrogates for hierarchical search spaces\"  ACM, 2019-07-13. DOI: 10.1145/3321707.3321765','2024-02-03 22:46:23.054817','2024-02-03 22:46:23.054817'),(430,'[38] Ying Hung, V Roshan Joseph, Shreyes N Melkote. \"Design and Analysis of Computer Experiments With Branching and Nested Factors\" Technometrics 51(4):354-365 Informa UK Limited, 2009-11. DOI: 10.1198/tech.2009.07097','2024-02-03 22:46:23.122385','2024-02-03 22:46:23.122385'),(431,'[39] Charles Audet, Edward Hallé-Hannan, Sébastien Le digabel. \"A General Mathematical Framework for Constrained Mixed-variable Blackbox Optimization Problems with Meta and Categorical Variables\" Operations Research Forum 4(1):1-37 Springer Science and Business Media LLC, 2023-02-23. DOI: 10.1007/s43069-022-00180-6','2024-02-03 22:46:23.192648','2024-02-03 22:46:23.192648'),(432,'[40] Paul Saves, Nathalie Bartoli, Youssef Diouane, Thierry Lefebvre, Joseph Morlier, Christophe David, Eric Nguyen Van, Sébastien Defoort. \"Multidisciplinary design optimization with mixed categorical variables for aircraft design\"  American Institute of Aeronautics and Astronautics, 2022-01-03. DOI: 10.2514/6.2022-0082','2024-02-03 22:46:23.260558','2024-02-03 22:46:23.260558'),(433,'[41] R Conde Arenzana, A López-Lopera, S Mouton, N Bartoli, T Lefebvre. \"Assessing the Performance of an Adaptive Multi-Fidelity Gaussian Process with Noisy Training Data...\"  American Institute of Aeronautics and Astronautics (AIAA), 2021-08-02. DOI: 10.2514/6.2021-3098.vid','2024-02-03 22:46:23.323492','2024-02-03 22:46:23.323492'),(434,'[42] R C Rufato, Y Diouane, J Henry, R Ahlfeld, J Morlier. \"A mixed-categorical data-driven approach for prediction and optimization of hybrid discontinuous composites performance\"  .','2024-02-03 22:46:23.387770','2024-02-03 22:46:23.387770'),(435,'[43] D Gorissen, K Crombecq, I Couckuyt, T Dhaene, P Demeester. \"A surrogate modeling and adaptive sampling toolbox for computer based design\" J Mach Learn Res 11(None):2051-2055 .','2024-02-03 22:46:23.463588','2024-02-03 22:46:23.464602'),(436,'[44] C K Williams, C E Rasmussen. \"Gaussian processes for machine learning\"  MIT press Cambridge, 2006.','2024-02-03 22:46:23.523971','2024-02-03 22:46:23.523971'),(437,'[45] M A Bouhlel, N Bartoli, R Regis, A Otsmane, J Morlier. \"Efficient Global Optimization for high-dimensional constrained problems by using the Kriging models combined with the Partial Least Squares method\" Eng Optim 50(None):2038-2053 .','2024-02-03 22:46:23.583444','2024-02-03 22:46:23.583444'),(438,'[46] Mohamed Amine Bouhlel, Sicheng He, Joaquim R R A Martins. \"Scalable gradient–enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes\" Structural and Multidisciplinary Optimization 61(4):1363-1376 Springer Science and Business Media LLC, 2020-03-09. DOI: 10.1007/s00158-020-02488-5','2024-02-03 22:46:23.640696','2024-02-03 22:46:23.640696'),(439,'[47] L S Kwan, A Pitrou, S Seibert. \"Numba: A LLVM-based python JIT compiler\"  .','2024-02-03 22:46:23.701660','2024-02-03 22:46:23.701660'),(440,'[48] Martin Zaefferer, Daniel Horn. \"A First Analysis of Kernels for Kriging-Based Optimization in Hierarchical Search Spaces\"  Springer International Publishing, 2018. DOI: 10.1007/978-3-319-99259-4_32','2024-02-03 22:46:23.754984','2024-02-03 22:46:23.754984'),(441,'[49] Ruichen Jin, Wei Chen, Agus Sudjianto. \"An efficient algorithm for constructing optimal design of computer experiments\" Journal of Statistical Planning and Inference 134(1):268-287 Elsevier BV, 2005-09. DOI: 10.1016/j.jspi.2004.02.014','2024-02-03 22:46:23.822678','2024-02-03 22:46:23.822678'),(442,'[50] R Garnett, M Osborne, P Hennig. \"Active learning of linear embeddings for Gaussian processes\"  .','2024-02-03 22:46:23.883133','2024-02-03 22:46:23.883133'),(443,'[51] Donald R Jones. \"A taxonomy of global optimization methods based on response surfaces\" Journal of Global Optimization 21(4):345-383 Springer Science and Business Media LLC, 2001. DOI: 10.1023/a:1012771025575','2024-02-03 22:46:23.945931','2024-02-03 22:46:23.945931'),(444,'[52] Rémi Lafage, Egobox. \"egobox, a Rust toolbox for efficient global optimization\" Journal of Open Source Software 7(78):4737 The Open Journal, 2022-10-09. DOI: 10.21105/joss.04737','2024-02-03 22:46:24.012845','2024-02-03 22:46:24.012845'),(445,'[53] Donald R Jones, Matthias Schonlau, William J Welch. \"Efficient global optimization of expensive black-box functions\" Journal of Global Optimization 13(4):455-492 Springer Science and Business Media LLC, 1998. DOI: 10.1023/a:1008306431147','2024-02-03 22:46:24.075490','2024-02-03 22:46:24.075490'),(446,'[54] X Deng, C Devon Lin, K-W Liu, R K Rowe. \"Additive Gaussian Process for Computer Models With Qualitative and Quantitative Factors\" Technometrics 59(3):283-292 Informa UK Limited, 2017-04-13. DOI: 10.1080/00401706.2016.1211554','2024-02-03 22:46:24.136381','2024-02-03 22:46:24.136381'),(447,'[55] J Cuesta-Ramirez, Le Riche, R Roustant, O Perrin, G Durantin, C Gliere, A. \"A comparison of mixed-variables Bayesian optimization approaches\" Adv Model Simul Eng Sci 9(None):1-29 .','2024-02-03 22:46:24.196040','2024-02-03 22:46:24.196040'),(448,'[56] Riccardo Rebonato, Peter Jaeckel. \"The Most General Methodology to Create a Valid Correlation Matrix for Risk Management and Option Pricing Purposes\" SSRN Electronic Journal 2(None):17-27 Elsevier BV, 2001. DOI: 10.2139/ssrn.1969689','2024-02-03 22:46:24.253484','2024-02-03 22:46:24.253484'),(449,'[57] Francesco Rapisarda, Damiano Brigo, Fabio Mercurio. \"Parameterizing correlations: a geometric interpretation\" IMA Journal of Management Mathematics 18(1):55-73 Oxford University Press (OUP), 2007-01-01. DOI: 10.1093/imaman/dpl010','2024-02-03 22:46:24.331714','2024-02-03 22:46:24.331714'),(450,'[58] Mohamed Amine Bouhlel, Nathalie Bartoli, Abdelkader Otsmane, Joseph Morlier. \"An Improved Approach for Estimating the Hyperparameters of the Kriging Model for High-Dimensional Problems through the Partial Least Squares Method\" Mathematical Problems in Engineering 2016(None):1-11 Hindawi Limited, 2016. DOI: 10.1155/2016/6723410','2024-02-03 22:46:24.410725','2024-02-03 22:46:24.410725'),(451,'[59] George H Cheng, Adel Younis, Kambiz Haji Hajikolaei, G Gary Wang. \"Trust Region Based Mode Pursuing Sampling Method for Global Optimization of High Dimensional Design Problems\" Journal of Mechanical Design 137(2):21407 ASME International, 2015-02-01. DOI: 10.1115/1.4029219','2024-02-03 22:46:24.472298','2024-02-03 22:46:24.472298'),(452,'[60] Rickard Karlsson, Laurens Bliek, Sicco Verwer, Mathijs De Weerdt. \"Continuous Surrogate-Based Optimization Algorithms Are Well-Suited for Expensive Discrete Problems\"  Springer International Publishing, 2021. DOI: 10.1007/978-3-030-76640-5_4','2024-02-03 22:46:24.532353','2024-02-03 22:46:24.532353'),(453,'[61] Julien Pelamatti, Loïc Brevault, Mathieu Balesdent, El-Ghazali Talbi, Yannick Guerin. \"Bayesian optimization of variable-size design space problems\" Optimization and Engineering 22(1):387-447 Springer Science and Business Media LLC, 2021. DOI: 10.1007/s11081-020-09520-z','2024-02-03 22:46:24.590448','2024-02-03 22:46:24.590448'),(454,'[62] Ali Hebbal, Loïc Brevault, Mathieu Balesdent, El-Ghazali Talbi, Nouredine Melab. \"Bayesian optimization using deep Gaussian processes with applications to aerospace system design\" Optimization and Engineering 22(1):321-361 Springer Science and Business Media LLC, 2021. DOI: 10.1007/s11081-020-09517-8','2024-02-03 22:46:24.646984','2024-02-03 22:46:24.646984'),(455,'[63] N J Wildberger. \"A Rational Approach to Trigonometry\" Math Horizons 15(2):16-20 Informa UK Limited, 2007-11. DOI: 10.1080/10724117.2007.11974738','2024-02-03 22:46:24.703186','2024-02-03 22:46:24.703186'),(456,'[64] Hyunghun Cho, Yongjin Kim, Eunjung Lee, Daeyoung Choi, Yongjae Lee, Wonjong Rhee. \"Basic Enhancement Strategies When Using Bayesian Optimization for Hyperparameter Tuning of Deep Neural Networks\" IEEE Access 8(None):52588-52608 Institute of Electrical and Electronics Engineers (IEEE), 2020. DOI: 10.1109/access.2020.2981072','2024-02-03 22:46:24.762688','2024-02-03 22:46:24.762688'),(457,'[65] M M Zuniga, D Sinoquet. \"Global optimization for mixed categorical-continuous variables based on Gaussian process models with a randomized categorical space exploration step\" INFOR Inf Syst Oper Res 58(None):310-341 .','2024-02-03 22:46:24.817327','2024-02-03 22:46:24.817327'),(458,'[66] M Lindauer, K Eggensperger, M Feurer, A B Deng, D Benjamins, C. \"SMAC3: A versatile Bayesian optimization package for hyperparameter optimization\" J Mach Learn Res 23(None):1-9 .','2024-02-03 22:46:24.869254','2024-02-03 22:46:24.869254'),(459,'[67] Victor Picheny, Tatiana Labopin-Richard. \"Sequential design of experiments for estimating percentiles of black-box functions\" Statistica Sinica None(None):None Statistica Sinica (Institute of Statistical Science), 2023. DOI: 10.5705/ss.202016.0160','2024-02-03 22:46:24.926192','2024-02-03 22:46:24.926192'),(460,'[68] Alexander I Cowen-Rivers, Wenlong Lyu, Rasul Tutunov, Zhi Wang, Antoine Grosnit, Ryan Rhys Griffiths, Alexandre Max Maraval, Hao Jianye, Jun Wang, Jan Peters, Haitham Bou-Ammar. \"HEBO: An Empirical Study of Assumptions in Bayesian Optimisation\" Journal of Artificial Intelligence Research 74(None):1269-1349 AI Access Foundation, 2020. DOI: 10.1613/jair.1.13643','2024-02-03 22:46:24.996330','2024-02-03 22:46:24.996330'),(461,'[69] H Jiang, Y Shen, Y Li, W Zhang, C Zhang, B Cui. \"OpenBox: A Python toolkit for generalized black-box optimization\"  .','2024-02-03 22:46:25.072644','2024-02-03 22:46:25.072644'),(462,'[70] P Saves.   .','2024-02-03 22:46:25.131573','2024-02-03 22:46:25.131573'),(463,'[71] K Kandasamy, K R Vysyaraju, W Neiswanger, B Paria, C Collins, J Schneider. \"Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly\" J Mach Learn Res 21(None):3098-3124 .','2024-02-03 22:46:25.184461','2024-02-03 22:46:25.184461'),(464,'[72] Satadru Roy, William A Crossley, Bret Stanford, Kenneth T Moore, Justin S Gray. \"A Mixed Integer Efficient Global Optimization Algorithm with Multiple Infill Strategy - Applied to a Wing Topology Optimization Problem\"  American Institute of Aeronautics and Astronautics, 2019-01-06. DOI: 10.2514/6.2019-2356','2024-02-03 22:46:25.290497','2024-02-03 22:46:25.290497'),(465,'[73] Juliane Müller, Christine A Shoemaker, Robert Piché. \"SO-MI: A surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems\" Computers & Operations Research 40(5):1383-1400 Elsevier BV, 2013-05. DOI: 10.1016/j.cor.2012.08.022','2024-02-03 22:46:25.355214','2024-02-03 22:46:25.355214'),(466,'[74] Thi Thoi Tran, Delphine Sinoquet, Sébastien Da Veiga, Marcel Mongeau. \"Derivative-free mixed binary necklace optimization for cyclic-symmetry optimal design problems\" Optimization and Engineering None(None):None Springer Science and Business Media LLC, 2021-09-28. DOI: 10.1007/s11081-021-09685-1','2024-02-03 22:46:25.420664','2024-02-03 22:46:25.420664'),(467,'[75] Mostafa Meliani, Nathalie Bartoli, Thierry Lefebvre, Mohamed-Amine Bouhlel, Joaquim R R A Martins, Joseph Morlier. \"Multi-fidelity efficient global optimization: Methodology and application to airfoil shape design\"  American Institute of Aeronautics and Astronautics, 2019-06-14. DOI: 10.2514/6.2019-3236','2024-02-03 22:46:25.480507','2024-02-03 22:46:25.480507'),(468,'[76] Herbert K H Lee. \"Gaussian Processes\"  Springer Berlin Heidelberg, 2011. DOI: 10.1007/978-3-642-04898-2_271','2024-02-03 22:46:25.545523','2024-02-03 22:46:25.546052'),(469,'[77] Andrés F López-Lopera, Déborah Idier, Jérémy Rohmer, François Bachoc. \"Multioutput Gaussian processes with functional data: A study on coastal flood hazard assessment\" Reliability Engineering & System Safety 218(None):108139 Elsevier BV, 2022-02. DOI: 10.1016/j.ress.2021.108139','2024-02-03 22:46:25.619115','2024-02-03 22:46:25.619115'),(470,'[78] Gaspard Berthelin, Sylvain Dubreuil, Michel Salaün, Nathalie Bartoli, Christian Gogu. \"Disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis\" International Journal for Numerical Methods in Engineering 123(15):3594-3626 Wiley, 2022-05-12. DOI: 10.1002/nme.6981','2024-02-03 22:46:25.685554','2024-02-03 22:46:25.685554'),(471,'[79] Inês Cardoso, Sylvain Dubreuil, Nathalie Bartoli, Christian Gogu, Michel Salaün. \"Model order reduction for parameterized multidisciplinary analysis using disciplinary surrogates: application to non-linear solvers\"  American Institute of Aeronautics and Astronautics, 2023. DOI: 10.2514/6.2024-1407','2024-02-03 22:46:25.746242','2024-02-03 22:46:25.746242'),(472,'[80] Jason A Platt, Stephen G Penny, Timothy A Smith, Tse-Chun Chen, Henry D I Abarbanel. \"A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics\" Neural Networks 153(None):530-552 Elsevier BV, 2022-09. DOI: 10.1016/j.neunet.2022.06.025','2024-02-03 22:46:25.813186','2024-02-03 22:46:25.813186'),(473,'[81] Rémy Charayron, Thierry Lefebvre, Nathalie Bartoli, Joseph Morlier. \"Multi-fidelity Bayesian optimization strategy applied to Overall Drone Design\"  American Institute of Aeronautics and Astronautics, 2023-01-19. DOI: 10.2514/6.2023-2366','2024-02-03 22:46:25.878821','2024-02-03 22:46:25.878821'),(474,'[82] Rémy Charayron, Thierry Lefebvre, Nathalie Bartoli, Joseph Morlier. \"Towards a multi-fidelity & multi-objective Bayesian optimization efficient algorithm\" Aerospace Science and Technology 142(None):108673 Elsevier BV, 2023-11. DOI: 10.1016/j.ast.2023.108673','2024-02-03 22:46:25.940670','2024-02-03 22:46:25.940670'),(475,'[83] Herman Wold. \"Soft Modelling by Latent Variables: The Non-Linear Iterative Partial Least Squares (NIPALS) Approach\" Journal of Applied Probability 12(S1):117-142 Cambridge University Press (CUP), 1975. DOI: 10.1017/s0021900200047604','2024-02-03 22:46:26.002068','2024-02-03 22:46:26.002068'),(476,'[84] Remy Priem, Nathalie Bartoli, Youssef Diouane, Sylvain Dubreuil, Paul Saves. \"High-dimensional efficient global optimization using both random and supervised embeddings\"  American Institute of Aeronautics and Astronautics, 2023-06-08. DOI: 10.2514/6.2023-4448','2024-02-03 22:46:26.058761','2024-02-03 22:46:26.058761'),(477,'[85] Wolfgang Betz, Iason Papaioannou, Daniel Straub. \"Numerical methods for the discretization of random fields by means of the Karhunen–Loève expansion\" Computer Methods in Applied Mechanics and Engineering 271(None):109-129 Elsevier BV, 2014-04. DOI: 10.1016/j.cma.2013.12.010','2024-02-03 22:46:26.159177','2024-02-03 22:46:26.159177'),(478,'[86] Morgane Menz, Sylvain Dubreuil, Jérôme Morio, Christian Gogu, Nathalie Bartoli, Marie Chiron. \"Variance based sensitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes\" Structural Safety 93(None):102116 Elsevier BV, 2021-11. DOI: 10.1016/j.strusafe.2021.102116','2024-02-03 22:46:26.224481','2024-02-03 22:46:26.224481'),(479,'[87] David Ginsbourger, Rodolphe Le Riche, Laurent Carraro. \"Kriging Is Well-Suited to Parallelize Optimization\"  Springer Berlin Heidelberg, 2010. DOI: 10.1007/978-3-642-10701-6_6','2024-02-03 22:46:26.286401','2024-02-03 22:46:26.286401'),(480,'[88] Emile Roux, Yannick Tillier, Salim Kraria, Pierre-Olivier Bouchard. \"An efficient parallel global optimization strategy based on Kriging properties suitable for material parameters identification\" Archive of Mechanical Engineering 67(None):169-195 Polish Academy of Sciences Chancellery, 2020-01-03. DOI: 10.24425/ame.2020.131689','2024-02-03 22:46:26.348765','2024-02-03 22:46:26.348765'),(481,'[1] M Abdul Majid, Z A Huneiti, M A Al-Naafa, W Balachandran. \"A study of the effects of using MATLAB as a pedagogical tool for engineering mathematics students\"  IEEE, 2012-09. DOI: 10.1109/icl.2012.6402183','2024-02-03 22:46:29.595060','2024-02-03 22:46:29.595060'),(482,'[2] M Abdul Majid, Z A Huneiti, M A Al-Naafa, W Balachandran. \"A study of the effects of using MATLAB as a pedagogical tool for engineering mathematics students\"  IEEE, 2013. DOI: 10.1109/icl.2012.6402183','2024-02-03 22:46:29.683330','2024-02-03 22:46:29.683330'),(483,'[3] M L Brake. \"MATLAB as a Tool to Increase the Math Self-Confidence and the Math Ability of First-Year Engineering Technology Students\"  .','2024-02-03 22:46:29.771039','2024-02-03 22:46:29.771039'),(484,'[4] Juebei Chen, Anette Kolmos, Xiangyun Du. \"Forms of implementation and challenges of PBL in engineering education: a review of literature\" European Journal of Engineering Education 46(1):90-115 Informa UK Limited, 2021. DOI: 10.1080/03043797.2020.1718615','2024-02-03 22:46:29.850948','2024-02-03 22:46:29.850948'),(485,'[5] Patricia Cretchley, Chris Harman, Nerida Ellerton, Gerard Fogarty. \"MATLAB in early undergraduate mathematics: An investigation into the effects of scientific software on learning\" Mathematics Education Research Journal 12(3):219-233 Springer Science and Business Media LLC, 2000-12. DOI: 10.1007/bf03217086','2024-02-03 22:46:29.919651','2024-02-03 22:46:29.919651'),(486,'[6] Roger G Hadgraft, Anette Kolmos. \"Emerging learning environments in engineering education\" Australasian Journal of Engineering Education 25(1):3-16 Informa UK Limited, 2020-01-02. DOI: 10.1080/22054952.2020.1713522','2024-02-03 22:46:29.998361','2024-02-03 22:46:29.998361'),(487,'[7] Marcela Hernandez-De-Menendez, Ruben Morales-Menendez. \"Technological innovations and practices in engineering education: a review\" International Journal on Interactive Design and Manufacturing (IJIDeM) 13(2):713-728 Springer Science and Business Media LLC, 2019-02-08. DOI: 10.1007/s12008-019-00550-1','2024-02-03 22:46:30.080379','2024-02-03 22:46:30.080641'),(488,'[8] Marcela Hernández-De-Menéndez, Antonio Vallejo Guevara, Juan Carlos Tudón Martínez, Diana Hernández Alcántara, Ruben Morales-Menendez. \"Active learning in engineering education. A review of fundamentals, best practices and experiences\" International Journal on Interactive Design and Manufacturing (IJIDeM) 13(3):909-922 Springer Science and Business Media LLC, 2019-02-13. DOI: 10.1007/s12008-019-00557-8','2024-02-03 22:46:30.136612','2024-02-03 22:46:30.136612'),(489,'[9] Firuz Kamalov, Sherif Moussa, Rita Zgheib, Omar Mashaal. \"Feature selection for intrusion detection systems\"  IEEE, 2020-12. DOI: 10.1109/iscid51228.2020.00065','2024-02-03 22:46:30.200126','2024-02-03 22:46:30.200126'),(490,'[10] Firuz Kamalov, Hana Sulieman, David Santandreu Calonge. \"Machine learning based approach to exam cheating detection\" PLOS ONE 16(8):e0254340 Public Library of Science (PLoS), 2021-08-04. DOI: 10.1371/journal.pone.0254340','2024-02-03 22:46:30.262562','2024-02-03 22:46:30.262641'),(491,'[11] Adem Kilicman, Munther A Hassan, S K Said Husain. \"Teaching and Learning using Mathematics Software “The New Challenge”\" Procedia - Social and Behavioral Sciences 8(None):613-619 Elsevier BV, 2010. DOI: 10.1016/j.sbspro.2010.12.085','2024-02-03 22:46:30.322125','2024-02-03 22:46:30.322125'),(492,'[12] M Lorenz, M Rüßmann, R Strack, K L Lueth, M Bolle. \"The Business Behind Consulting\"  Productivity Press, 2015-05-14. DOI: 10.1201/b18483-9','2024-02-03 22:46:30.379191','2024-02-03 22:46:30.379191'),(493,'[13] R D A Maurício, L Veado, R T Moreira, E Figueiredo, H Costa. \"A systematic mapping study on game-related methods for software engineering education\" Information and software technology 95(None):201-218 .','2024-02-03 22:46:30.434687','2024-02-03 22:46:30.434687'),(494,'[14] Natalia M Mezhennaya, Oleg V Pugachev. \"ON PERCEPTION OF COMPUTER ALGEBRA SYSTEMS AND MICROSOFT EXCEL BY ENGINEERING STUDENTS\" Problems of Education in the 21st Century 77(3):379-395 Scientia Socialis Ltd, 2019-06-18. DOI: 10.33225/pec/19.77.379','2024-02-03 22:46:30.544238','2024-02-03 22:46:30.544238'),(495,'[15] Jhonattan Miranda, Christelle Navarrete, Julieta Noguez, José-Martin Molina-Espinosa, María-Soledad Ramírez-Montoya, Sergio A Navarro-Tuch, Martín-Rogelio Bustamante-Bello, José-Bernardo Rosas-Fernández, Arturo Molina. \"The core components of education 4.0 in higher education: Three case studies in engineering education\" Computers & Electrical Engineering 93(None):107278 Elsevier BV, 2021-07. DOI: 10.1016/j.compeleceng.2021.107278','2024-02-03 22:46:30.697278','2024-02-03 22:46:30.697278'),(496,'[16] Kevin Moore, Carol Jones, Robert Scott Frazier. \"Engineering Education For Generation Z\" American Journal of Engineering Education (AJEE) 8(2):111-126 Clute Institute, 2017-12-01. DOI: 10.19030/ajee.v8i2.10067','2024-02-03 22:46:30.759545','2024-02-03 22:46:30.759545'),(497,'[17] Margarita Ortiz‐rojas, Katherine Chiluiza, Martin Valcke. \"Gamification through leaderboards: An empirical study in engineering education\" Computer Applications in Engineering Education 27(4):777-788 Wiley, 2019-05-09. DOI: 10.1002/cae.12116','2024-02-03 22:46:30.808597','2024-02-03 22:46:30.808597'),(498,'[18] R I Puhak. \"Twenty-Third Annual Computer Security Applications Conference - Copyright\"  IEEE, 2011. DOI: 10.1109/acsac.2007.2','2024-02-03 22:46:30.897984','2024-02-03 22:46:30.897984'),(499,'[19] Ricardo A Ramirez-Mendoza, Ruben Morales-Menendez, Hafiz Iqbal, Roberto Parra-Saldivar. \"Engineering Education 4.0: — proposal for a new Curricula\"  IEEE, 2018-04. DOI: 10.1109/educon.2018.8363376','2024-02-03 22:46:30.993150','2024-02-03 22:46:30.993150'),(500,'[20] Bashir Salah, Mustufa Abidi, Syed Mian, Mohammed Krid, Hisham Alkhalefah, Ali Abdo. \"Virtual Reality-Based Engineering Education to Enhance Manufacturing Sustainability in Industry 4.0\" Sustainability 11(5):1477 MDPI AG, 2019-03-11. DOI: 10.3390/su11051477','2024-02-03 22:46:31.096300','2024-02-03 22:46:31.096300'),(501,'[21] T L Strayhorn. \"College in the information age: Gains associated with students\' use of technology\" Journal of Interactive Online Learning 5(2):143-155 .','2024-02-03 22:46:31.156633','2024-02-03 22:46:31.156633'),(502,'[22] Fadi Thabtah, Firuz Kamalov, Khairan Rajab. \"A new computational intelligence approach to detect autistic features for autism screening\" International Journal of Medical Informatics 117(None):112-124 Elsevier BV, 2018-09. DOI: 10.1016/j.ijmedinf.2018.06.009','2024-02-03 22:46:31.257064','2024-02-03 22:46:31.257064'),(503,'[23] Antoine Van Den Beemt, Miles Macleod, Jan Van Der Veen, Anne Van De Ven, Sophie Van Baalen, Renate Klaassen, Mieke Boon. \"Interdisciplinary engineering education: A review of vision, teaching, and support\" Journal of Engineering Education 109(3):508-555 Wiley, 2020-06-30. DOI: 10.1002/jee.20347','2024-02-03 22:46:31.359779','2024-02-03 22:46:31.359779');
/*!40000 ALTER TABLE `articles_referencebibliographique` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articles_uploadedarticle`
--

DROP TABLE IF EXISTS `articles_uploadedarticle`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articles_uploadedarticle` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `file` varchar(100) NOT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articles_uploadedarticle`
--

LOCK TABLES `articles_uploadedarticle` WRITE;
/*!40000 ALTER TABLE `articles_uploadedarticle` DISABLE KEYS */;
/*!40000 ALTER TABLE `articles_uploadedarticle` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `articlesfavoris_articlefavoris`
--

DROP TABLE IF EXISTS `articlesfavoris_articlefavoris`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `articlesfavoris_articlefavoris` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  `article_id` bigint NOT NULL,
  `user_id` bigint NOT NULL,
  PRIMARY KEY (`id`),
  KEY `ArticlesFavoris_arti_article_id_d1bb7f57_fk_Articles_` (`article_id`),
  KEY `ArticlesFavoris_arti_user_id_123c477e_fk_Authentic` (`user_id`),
  CONSTRAINT `ArticlesFavoris_arti_article_id_d1bb7f57_fk_Articles_` FOREIGN KEY (`article_id`) REFERENCES `articles_article` (`id`),
  CONSTRAINT `ArticlesFavoris_arti_user_id_123c477e_fk_Authentic` FOREIGN KEY (`user_id`) REFERENCES `authentication_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `articlesfavoris_articlefavoris`
--

LOCK TABLES `articlesfavoris_articlefavoris` WRITE;
/*!40000 ALTER TABLE `articlesfavoris_articlefavoris` DISABLE KEYS */;
/*!40000 ALTER TABLE `articlesfavoris_articlefavoris` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_group`
--

DROP TABLE IF EXISTS `auth_group`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `auth_group` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(150) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_group`
--

LOCK TABLES `auth_group` WRITE;
/*!40000 ALTER TABLE `auth_group` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_group` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_group_permissions`
--

DROP TABLE IF EXISTS `auth_group_permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `auth_group_permissions` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `group_id` int NOT NULL,
  `permission_id` int NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_group_permissions_group_id_permission_id_0cd325b0_uniq` (`group_id`,`permission_id`),
  KEY `auth_group_permissio_permission_id_84c5c92e_fk_auth_perm` (`permission_id`),
  CONSTRAINT `auth_group_permissio_permission_id_84c5c92e_fk_auth_perm` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
  CONSTRAINT `auth_group_permissions_group_id_b120cbf9_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_group_permissions`
--

LOCK TABLES `auth_group_permissions` WRITE;
/*!40000 ALTER TABLE `auth_group_permissions` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_group_permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_permission`
--

DROP TABLE IF EXISTS `auth_permission`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `auth_permission` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL,
  `content_type_id` int NOT NULL,
  `codename` varchar(100) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_permission_content_type_id_codename_01ab375a_uniq` (`content_type_id`,`codename`),
  CONSTRAINT `auth_permission_content_type_id_2f476e4b_fk_django_co` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=61 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_permission`
--

LOCK TABLES `auth_permission` WRITE;
/*!40000 ALTER TABLE `auth_permission` DISABLE KEYS */;
INSERT INTO `auth_permission` VALUES (1,'Can add log entry',1,'add_logentry'),(2,'Can change log entry',1,'change_logentry'),(3,'Can delete log entry',1,'delete_logentry'),(4,'Can view log entry',1,'view_logentry'),(5,'Can add permission',2,'add_permission'),(6,'Can change permission',2,'change_permission'),(7,'Can delete permission',2,'delete_permission'),(8,'Can view permission',2,'view_permission'),(9,'Can add group',3,'add_group'),(10,'Can change group',3,'change_group'),(11,'Can delete group',3,'delete_group'),(12,'Can view group',3,'view_group'),(13,'Can add content type',4,'add_contenttype'),(14,'Can change content type',4,'change_contenttype'),(15,'Can delete content type',4,'delete_contenttype'),(16,'Can view content type',4,'view_contenttype'),(17,'Can add session',5,'add_session'),(18,'Can change session',5,'change_session'),(19,'Can delete session',5,'delete_session'),(20,'Can view session',5,'view_session'),(21,'Can add Token',6,'add_token'),(22,'Can change Token',6,'change_token'),(23,'Can delete Token',6,'delete_token'),(24,'Can view Token',6,'view_token'),(25,'Can add token',7,'add_tokenproxy'),(26,'Can change token',7,'change_tokenproxy'),(27,'Can delete token',7,'delete_tokenproxy'),(28,'Can view token',7,'view_tokenproxy'),(29,'Can add institution',8,'add_institution'),(30,'Can change institution',8,'change_institution'),(31,'Can delete institution',8,'delete_institution'),(32,'Can view institution',8,'view_institution'),(33,'Can add mot cle',9,'add_motcle'),(34,'Can change mot cle',9,'change_motcle'),(35,'Can delete mot cle',9,'delete_motcle'),(36,'Can view mot cle',9,'view_motcle'),(37,'Can add reference bibliographique',10,'add_referencebibliographique'),(38,'Can change reference bibliographique',10,'change_referencebibliographique'),(39,'Can delete reference bibliographique',10,'delete_referencebibliographique'),(40,'Can view reference bibliographique',10,'view_referencebibliographique'),(41,'Can add uploaded article',11,'add_uploadedarticle'),(42,'Can change uploaded article',11,'change_uploadedarticle'),(43,'Can delete uploaded article',11,'delete_uploadedarticle'),(44,'Can view uploaded article',11,'view_uploadedarticle'),(45,'Can add auteur',12,'add_auteur'),(46,'Can change auteur',12,'change_auteur'),(47,'Can delete auteur',12,'delete_auteur'),(48,'Can view auteur',12,'view_auteur'),(49,'Can add article',13,'add_article'),(50,'Can change article',13,'change_article'),(51,'Can delete article',13,'delete_article'),(52,'Can view article',13,'view_article'),(53,'Can add article favoris',14,'add_articlefavoris'),(54,'Can change article favoris',14,'change_articlefavoris'),(55,'Can delete article favoris',14,'delete_articlefavoris'),(56,'Can view article favoris',14,'view_articlefavoris'),(57,'Can add user',15,'add_user'),(58,'Can change user',15,'change_user'),(59,'Can delete user',15,'delete_user'),(60,'Can view user',15,'view_user');
/*!40000 ALTER TABLE `auth_permission` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `authentication_user`
--

DROP TABLE IF EXISTS `authentication_user`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `authentication_user` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `password` varchar(128) NOT NULL,
  `last_login` datetime(6) DEFAULT NULL,
  `is_superuser` tinyint(1) NOT NULL,
  `username` varchar(150) NOT NULL,
  `first_name` varchar(150) NOT NULL,
  `last_name` varchar(150) NOT NULL,
  `email` varchar(254) NOT NULL,
  `is_staff` tinyint(1) NOT NULL,
  `is_active` tinyint(1) NOT NULL,
  `date_joined` datetime(6) NOT NULL,
  `user_type` varchar(10) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `username` (`username`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `authentication_user`
--

LOCK TABLES `authentication_user` WRITE;
/*!40000 ALTER TABLE `authentication_user` DISABLE KEYS */;
INSERT INTO `authentication_user` VALUES (1,'pbkdf2_sha256$720000$Zvvam5mrtOwFveHdUZ7U7h$bqt8uy1wWNt+qNRyhVHfrj4u6I3FVJQ3FXsrmYJvpSg=',NULL,0,'amine26','string','string','string@esi.dz',0,1,'2024-02-03 22:03:51.363667','Admin'),(2,'pbkdf2_sha256$720000$1DRFw3GSgk5JLE8AteFalo$NiVqN2AstLAH0VHbnVFkekyZnBVAeAxJgPago+3pX44=',NULL,0,'aminetech26','string','string','string@esi.dz',0,1,'2024-02-03 22:59:48.125899','User');
/*!40000 ALTER TABLE `authentication_user` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `authentication_user_groups`
--

DROP TABLE IF EXISTS `authentication_user_groups`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `authentication_user_groups` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `user_id` bigint NOT NULL,
  `group_id` int NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `Authentication_user_groups_user_id_group_id_7b0e1b22_uniq` (`user_id`,`group_id`),
  KEY `Authentication_user_groups_group_id_9e1163a5_fk_auth_group_id` (`group_id`),
  CONSTRAINT `Authentication_user__user_id_56879edf_fk_Authentic` FOREIGN KEY (`user_id`) REFERENCES `authentication_user` (`id`),
  CONSTRAINT `Authentication_user_groups_group_id_9e1163a5_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `authentication_user_groups`
--

LOCK TABLES `authentication_user_groups` WRITE;
/*!40000 ALTER TABLE `authentication_user_groups` DISABLE KEYS */;
/*!40000 ALTER TABLE `authentication_user_groups` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `authentication_user_user_permissions`
--

DROP TABLE IF EXISTS `authentication_user_user_permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `authentication_user_user_permissions` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `user_id` bigint NOT NULL,
  `permission_id` int NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `Authentication_user_user_user_id_permission_id_ad8bf3db_uniq` (`user_id`,`permission_id`),
  KEY `Authentication_user__permission_id_c2ed820d_fk_auth_perm` (`permission_id`),
  CONSTRAINT `Authentication_user__permission_id_c2ed820d_fk_auth_perm` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
  CONSTRAINT `Authentication_user__user_id_45a3bcfc_fk_Authentic` FOREIGN KEY (`user_id`) REFERENCES `authentication_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `authentication_user_user_permissions`
--

LOCK TABLES `authentication_user_user_permissions` WRITE;
/*!40000 ALTER TABLE `authentication_user_user_permissions` DISABLE KEYS */;
/*!40000 ALTER TABLE `authentication_user_user_permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `authtoken_token`
--

DROP TABLE IF EXISTS `authtoken_token`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `authtoken_token` (
  `key` varchar(40) NOT NULL,
  `created` datetime(6) NOT NULL,
  `user_id` bigint NOT NULL,
  PRIMARY KEY (`key`),
  UNIQUE KEY `user_id` (`user_id`),
  CONSTRAINT `authtoken_token_user_id_35299eff_fk_Authentication_user_id` FOREIGN KEY (`user_id`) REFERENCES `authentication_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `authtoken_token`
--

LOCK TABLES `authtoken_token` WRITE;
/*!40000 ALTER TABLE `authtoken_token` DISABLE KEYS */;
/*!40000 ALTER TABLE `authtoken_token` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_admin_log`
--

DROP TABLE IF EXISTS `django_admin_log`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `django_admin_log` (
  `id` int NOT NULL AUTO_INCREMENT,
  `action_time` datetime(6) NOT NULL,
  `object_id` longtext,
  `object_repr` varchar(200) NOT NULL,
  `action_flag` smallint unsigned NOT NULL,
  `change_message` longtext NOT NULL,
  `content_type_id` int DEFAULT NULL,
  `user_id` bigint NOT NULL,
  PRIMARY KEY (`id`),
  KEY `django_admin_log_content_type_id_c4bce8eb_fk_django_co` (`content_type_id`),
  KEY `django_admin_log_user_id_c564eba6_fk_Authentication_user_id` (`user_id`),
  CONSTRAINT `django_admin_log_content_type_id_c4bce8eb_fk_django_co` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`),
  CONSTRAINT `django_admin_log_user_id_c564eba6_fk_Authentication_user_id` FOREIGN KEY (`user_id`) REFERENCES `authentication_user` (`id`),
  CONSTRAINT `django_admin_log_chk_1` CHECK ((`action_flag` >= 0))
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_admin_log`
--

LOCK TABLES `django_admin_log` WRITE;
/*!40000 ALTER TABLE `django_admin_log` DISABLE KEYS */;
/*!40000 ALTER TABLE `django_admin_log` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_content_type`
--

DROP TABLE IF EXISTS `django_content_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `django_content_type` (
  `id` int NOT NULL AUTO_INCREMENT,
  `app_label` varchar(100) NOT NULL,
  `model` varchar(100) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `django_content_type_app_label_model_76bd3d3b_uniq` (`app_label`,`model`)
) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_content_type`
--

LOCK TABLES `django_content_type` WRITE;
/*!40000 ALTER TABLE `django_content_type` DISABLE KEYS */;
INSERT INTO `django_content_type` VALUES (1,'admin','logentry'),(13,'Articles','article'),(12,'Articles','auteur'),(8,'Articles','institution'),(9,'Articles','motcle'),(10,'Articles','referencebibliographique'),(11,'Articles','uploadedarticle'),(14,'ArticlesFavoris','articlefavoris'),(3,'auth','group'),(2,'auth','permission'),(15,'Authentication','user'),(6,'authtoken','token'),(7,'authtoken','tokenproxy'),(4,'contenttypes','contenttype'),(5,'sessions','session');
/*!40000 ALTER TABLE `django_content_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_migrations`
--

DROP TABLE IF EXISTS `django_migrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `django_migrations` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `app` varchar(255) NOT NULL,
  `name` varchar(255) NOT NULL,
  `applied` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_migrations`
--

LOCK TABLES `django_migrations` WRITE;
/*!40000 ALTER TABLE `django_migrations` DISABLE KEYS */;
INSERT INTO `django_migrations` VALUES (1,'Articles','0001_initial','2024-02-03 22:00:49.527142'),(2,'contenttypes','0001_initial','2024-02-03 22:00:49.571572'),(3,'contenttypes','0002_remove_content_type_name','2024-02-03 22:00:49.658823'),(4,'auth','0001_initial','2024-02-03 22:00:49.954138'),(5,'auth','0002_alter_permission_name_max_length','2024-02-03 22:00:50.024383'),(6,'auth','0003_alter_user_email_max_length','2024-02-03 22:00:50.031263'),(7,'auth','0004_alter_user_username_opts','2024-02-03 22:00:50.037049'),(8,'auth','0005_alter_user_last_login_null','2024-02-03 22:00:50.043958'),(9,'auth','0006_require_contenttypes_0002','2024-02-03 22:00:50.071134'),(10,'auth','0007_alter_validators_add_error_messages','2024-02-03 22:00:50.078551'),(11,'auth','0008_alter_user_username_max_length','2024-02-03 22:00:50.084138'),(12,'auth','0009_alter_user_last_name_max_length','2024-02-03 22:00:50.090808'),(13,'auth','0010_alter_group_name_max_length','2024-02-03 22:00:50.107779'),(14,'auth','0011_update_proxy_permissions','2024-02-03 22:00:50.120337'),(15,'auth','0012_alter_user_first_name_max_length','2024-02-03 22:00:50.128000'),(16,'Authentication','0001_initial','2024-02-03 22:00:50.534793'),(17,'ArticlesFavoris','0001_initial','2024-02-03 22:00:50.613241'),(18,'ArticlesFavoris','0002_initial','2024-02-03 22:00:50.693791'),(19,'admin','0001_initial','2024-02-03 22:00:50.856029'),(20,'admin','0002_logentry_remove_auto_add','2024-02-03 22:00:50.865036'),(21,'admin','0003_logentry_add_action_flag_choices','2024-02-03 22:00:50.874239'),(22,'authtoken','0001_initial','2024-02-03 22:00:50.978797'),(23,'authtoken','0002_auto_20160226_1747','2024-02-03 22:00:51.009254'),(24,'authtoken','0003_tokenproxy','2024-02-03 22:00:51.014142'),(25,'sessions','0001_initial','2024-02-03 22:00:51.053239');
/*!40000 ALTER TABLE `django_migrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_session`
--

DROP TABLE IF EXISTS `django_session`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `django_session` (
  `session_key` varchar(40) NOT NULL,
  `session_data` longtext NOT NULL,
  `expire_date` datetime(6) NOT NULL,
  PRIMARY KEY (`session_key`),
  KEY `django_session_expire_date_a5c62663` (`expire_date`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_session`
--

LOCK TABLES `django_session` WRITE;
/*!40000 ALTER TABLE `django_session` DISABLE KEYS */;
/*!40000 ALTER TABLE `django_session` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2024-02-04  0:46:33
